<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 10]
- [cs.CR](#cs.CR) [Total: 39]
- [cs.AI](#cs.AI) [Total: 60]
- [cs.SE](#cs.SE) [Total: 27]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [Heimdall++: Optimizing GPU Utilization and Pipeline Parallelism for Efficient Single-Pulse Detection](https://arxiv.org/abs/2512.00398)
*Bingzheng Xia,Zujie Ren,Kuang Ma,Xiaoqian Li,Wenda Li,Shuibing He*

Main category: cs.DC

TL;DR: Heimdall++是一种优化的GPU加速单脉冲搜索工具，通过细粒度GPU并行化、改进的内存管理和多线程框架来提高效率。实验结果显示，Heimdall++在单文件处理上达到2.66倍加速，多文件批处理上达到2.05倍加速，与原Heimdall结果保持一致。


<details>
  <summary>Details</summary>
Motivation: 现代射电望远镜对实时单脉冲检测提出了更高要求，而Heimdall在中间处理阶段存在资源竞争问题，限制了其性能。

Method: 通过细粒度GPU并行化、改进的内存管理和多线程框架来优化Heimdall，并解耦CPU和GPU的处理阶段。

Result: Heimdall++在单文件处理上达到2.66倍加速，多文件批处理上达到2.05倍加速，保持了与原Heimdall的结果一致性。

Conclusion: 优化后的Heimdall++显著提高了GPU利用率和整体吞吐量，解决了原Heimdall的性能瓶颈。

Abstract: With the increasing time and frequency resolution of modern radio telescopes and the exponential growth in observational data volumes, real-time single-pulse detection has become a critical requirement for time-domain radio astronomy. Heimdall, as a representative GPU-accelerated single-pulse search tool, offers substantial performance advantages over CPU-based approaches. However, its sequential execution model and resource contention in intermediate processing stages limit GPU utilization, leading to suboptimal throughput and increased computational latency. To address these limitations, we present Heimdall++, an optimized successor to Heimdall that incorporates fine-grained GPU parallelization, enhanced memory management, and a multi-threaded framework to decouple CPU-bound and GPU-bound processing stages. This design mitigates the GPU stall problem and improves end-to-end efficiency. We evaluated Heimdall++ on a system equipped with NVIDIA RTX 3080 Ti GPUs using both a single large-scale observational file and multiple files. Experimental results demonstrate that Heimdall++ achieves up to 2.66x speedup in single-file processing and 2.05x speedup in multi-file batch processing, while maintaining full consistency with the original Heimdall's search results.

</details>


### [2] [IslandRun: Privacy-Aware Multi-Objective Orchestration for Distributed AI Inference](https://arxiv.org/abs/2512.00595)
*Bala Siva Sai Akhil Malepati*

Main category: cs.DC

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Modern AI inference faces an irreducible tension: no single computational resource simultaneously maximizes performance, preserves privacy, minimizes cost, and maintains trust. Existing orchestration frameworks optimize single dimensions (Kubernetes prioritizes latency, federated learning preserves privacy, edge computing reduces network distance), creating solutions that struggle under real-world heterogeneity. We present IslandRun, a multi-objective orchestration system that treats computational resources as autonomous "islands" spanning personal devices, private edge servers, and public cloud. Our key insights: (1) request-level heterogeneity demands policy-constrained multi-objective optimization, (2) data locality enables routing compute to data rather than data to compute, and (3) typed placeholder sanitization preserves context semantics across trust boundaries. IslandRun introduces agent-based routing, tiered island groups with differential trust, and reversible anonymization. This establishes a new paradigm for privacy-aware, decentralized inference orchestration across heterogeneous personal computing ecosystems.

</details>


### [3] [Steady and Energy-Efficient Multi-Hop Clustering for Flying Ad-Hoc Networks (FANETs)](https://arxiv.org/abs/2512.00623)
*Basilis Mamalis,Marios Perlitis*

Main category: cs.DC

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Flying Ad-hoc Networks (FANETs), formed by Unmanned Aerial Vehicles (UAVs), represent an emerging and promising communication paradigm. These networks face unique challenges due to UAVs high mobility, limited energy resources, and dynamic topology. In this work, we propose a novel multi-hop clustering algorithm aimed at creating stable, energy-efficient clusters in FANET environments. The proposed solution enhances cluster longevity and communication efficiency through mobility-aware clustering, energy-centric cluster head (CH) selection, and a ground station(GS)-assisted cluster maintenance management mechanism. First, steady multi-hop clusters are constructed, having CHs with not only high stability and high energy but also with steady and high-energy neighboring areas, and then a proper GS-assisted cluster maintenance mechanism is applied. Experimental results, based on extended simulations, demonstrate that our approach outperforms existing schemes significantly, in terms of cluster stability, communication overhead, and security resilience.

</details>


### [4] [FlexiWalker: Extensible GPU Framework for Efficient Dynamic Random Walks with Runtime Adaptation](https://arxiv.org/abs/2512.00705)
*Seongyeon Park,Jaeyong Song,Changmin Shin,Sukjin Kim,Junguk Hong,Jinho Lee*

Main category: cs.DC

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Dynamic random walks are fundamental to various graph analysis applications, offering advantages by adapting to evolving graph properties. Their runtime-dependent transition probabilities break down the pre-computation strategy that underpins most existing CPU and GPU static random walk optimizations. This leaves practitioners suffering from suboptimal frameworks and having to write hand-tuned kernels that do not adapt to workload diversity. To handle this issue, we present FlexiWalker, the first GPU framework that delivers efficient, workload-generic support for dynamic random walks. Our design-space study shows that rejection sampling and reservoir sampling are more suitable than other sampling techniques under massive parallelism. Thus, we devise (i) new high-performance kernels for them that eliminate global reductions, redundant memory accesses, and random-number generation. Given the necessity of choosing the best-fitting sampling strategy at runtime, we adopt (ii) a lightweight first-order cost model that selects the faster kernel per node at runtime. To enhance usability, we introduce (iii) a compile-time component that automatically specializes user-supplied walk logic into optimized building blocks. On various dynamic random walk workloads with real-world graphs, FlexiWalker outperforms the best published CPU/GPU baselines by geometric means of 73.44x and 5.91x, respectively, while successfully executing workloads that prior systems cannot support. We open-source FlexiWalker in https://github.com/AIS-SNU/FlexiWalker.

</details>


### [5] [SIMPLE: Disaggregating Sampling from GPU Inference into a Decision Plane for Faster Distributed LLM Serving](https://arxiv.org/abs/2512.00719)
*Bohan Zhao,Zane Cao,Yongchao He*

Main category: cs.DC

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: As large language models (LLMs) scale out with tensor parallelism (TP) and pipeline parallelism (PP) and production stacks have aggressively optimized the data plane (attention/GEMM and KV cache), sampling, the decision plane that turns logits into tokens, becomes a new bottleneck. This creates a structural holdout: sampling neither expands with TP nor balances across PP stages, so its share of iteration time grows as GPUs get faster and it caps pipeline frequency at the last stage. We present SIMPLE, a stage-agnostic, sequence-parallel, overlappable decision plane that disaggregates sampling into a CPU-side service and shrinks its runtime footprint back to a minor, hidden role. SIMPLE combines: (1) sequence-parallel sampling, which shards work along the batch dimension and removes vocabulary-axis collectives; (2) a CPU-based algorithm with column-wise penalties and truncation-first filtering to realize single-pass, linear-time kernels; and (3) speculative hot-vocab sampling (SHVS), which samples on a small hot set with rejection-correctness and uses a simple sizing model to choose the hot-vocab size that maximizes throughput. In evaluation, SIMPLE improves end-to-end throughput by up to 96% and reduces P95 latency by 20-65%. Crucially, SIMPLE requires no user-side code changes and composes with existing data-plane optimizations, unlocking scaling benefits that compound with future GPU generations.

</details>


### [6] [Elastic Mixture of Rank-Wise Experts for Knowledge Reuse in Federated Fine-Tuning](https://arxiv.org/abs/2512.00902)
*Yebo Wu,Jingguang Li,Zhijiang Guo,Li Li*

Main category: cs.DC

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Federated fine-tuning offers a promising solution for adapting Large Language Models (LLMs) to downstream tasks while safeguarding data privacy. However, its high computational and communication demands hinder its deployment on resource-constrained devices. In this paper, we propose SmartFed, a resource-efficient federated fine-tuning framework. SmartFed intelligently reuses knowledge embedded in existing LoRA modules, eliminating the need for expensive training from scratch when adapting LLMs to new tasks. To effectively exploit this knowledge and ensure scalability, we introduce the Mixture of Rank-Wise Experts (MoRE). MoRE decomposes LoRA modules into fine-grained rank-level experts. These experts are selectively activated and combined based on input semantics and resource budgets. Moreover, to optimize resource utilization, we present the Elastic Expert Quota Allocation (EEQA). EEQA adaptively allocates expert capacity across parameter matrices based on their contribution to model performance, focusing computing resources on the critical experts. Extensive evaluations across multiple benchmarks demonstrate that SmartFed significantly outperforms existing methods in model performance and training efficiency.

</details>


### [7] [Tangram: Accelerating Serverless LLM Loading through GPU Memory Reuse and Affinity](https://arxiv.org/abs/2512.01357)
*Wenbin Zhu,Zhaoyan Shen,Zili Shao,Hongjun Dai,Feng Chen*

Main category: cs.DC

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Serverless Large Language Models (LLMs) have emerged as a cost-effective solution for deploying AI services by enabling a 'pay-as-you-go' pricing model through GPU resource sharing. However, cold-start latency, especially the model loading phase, has become a critical performance bottleneck, as it scales linearly with model size and severely limits the practical deployment of large-scale LLM services. This paper presents Tangram, a novel system that accelerates Serverless LLM loading through efficient GPU memory reuse. By leveraging the unused GPU memory to retain model parameters, Tangram significantly reduces model transfer time and cold-start latency. Its design includes three key components: unified GPU memory pool for tensor-level parameter sharing across models, on-demand KV cache allocation for dynamic memory management, and GPU-affinity-aware scheduling for maximizing resource utilization. These techniques collectively address the critical challenges of inefficient memory usage and the cold-start problem in Serverless LLM platforms. We have implemented a fully functional prototype, and experiments show that Tangram achieves up to 6.2 times faster loading and reduces Time-To-First-Token (TTFT) during cold-start by 23--55% over state-of-the-art methods.

</details>


### [8] [Delta Sum Learning: an approach for fast and global convergence in Gossip Learning](https://arxiv.org/abs/2512.01549)
*Tom Goethals,Merlijn Sebrechts,Stijn De Schrijver,Filip De Turck,Bruno Volckaert*

Main category: cs.DC

TL;DR: 该论文提出了用于Gossip Learning的Delta Sum Learning方法，并实现了基于Open Application Model的框架，以提高模型准确性和全局收敛性。


<details>
  <summary>Details</summary>
Motivation: 为解决Federated和Gossip Learning中平均方法在模型准确性和全局收敛性的不理想问题，缺乏利用如Kubernetes的声明性方法部署边缘学习负载的问题。

Method: 提出了改进Gossip Learning基础聚合操作的Delta Sum Learning方法，并在去中心化的编配框架中实现，使用Open Application Model进行动态节点发现和意图驱动的多工作负载应用部署。

Result: Delta Sum在10节点拓扑上性能与替代方法相当，扩展到50节点时全球精度下降减少58%，在受限连接下表现出更强的全局收敛性和对数精度损失，而替代品存在线性损失。

Conclusion: Delta Sum Learning在扩展性和准确性方面优于传统方法，并展示了在边缘计算环境中部署学习负载的新框架。

Abstract: Federated Learning is a popular approach for distributed learning due to its security and computational benefits. With the advent of powerful devices in the network edge, Gossip Learning further decentralizes Federated Learning by removing centralized integration and relying fully on peer to peer updates. However, the averaging methods generally used in both Federated and Gossip Learning are not ideal for model accuracy and global convergence. Additionally, there are few options to deploy Learning workloads in the edge as part of a larger application using a declarative approach such as Kubernetes manifests. This paper proposes Delta Sum Learning as a method to improve the basic aggregation operation in Gossip Learning, and implements it in a decentralized orchestration framework based on Open Application Model, which allows for dynamic node discovery and intent-driven deployment of multi-workload applications. Evaluation results show that Delta Sum performance is on par with alternative integration methods for 10 node topologies, but results in a 58% lower global accuracy drop when scaling to 50 nodes. Overall, it shows strong global convergence and a logarithmic loss of accuracy with increasing topology size compared to a linear loss for alternatives under limited connectivity.

</details>


### [9] [StarDist: A Code Generator for Distributed Graph Algorithms](https://arxiv.org/abs/2512.01646)
*Barenya Kumar Nandy,Rupesh Nasre*

Main category: cs.DC

TL;DR: 该论文提出了一个基于StarPlat的优化框架，旨在解决大型图在分布式内存环境中的性能瓶颈。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的关系数据常表现为图形，但图算法的不规则访问模式、NUMA限制和物理内存瓶颈会影响性能。StarPlat框架提供了一个分布式内存编码方案。

Method: 提出了一个分析转换框架，优化分布式内存中的图形算法。该方法利用与节点及其邻居相关的迭代语义，实现通信的聚集。还通过扫描重排序访问模式和减少通信，以及通过机会性缓存优化。此外，使用Open MPI的被动RMA构建了优化的批量归约（bulk-reduction）子结构

Result: SSSP算法性能测试中,该框架比d-Galois和DRONE分别优化了2.05倍和1.44倍。

Conclusion: 该优化框架有效提高了StarPlat在大数据图上的性能。

Abstract: Relational data, occurring in the real world, are often structured as graphs, which provide the logical abstraction required to make analytical derivations simpler. As graphs get larger, the irregular access patterns exhibited in most graph algorithms, hamper performance. This, along with NUMA and physical memory limits, results in scaling complexities with sequential/shared memory frameworks. StarPlat's MPI backend abstracts away the programmatic complexity involved in designing optimal distributed graph algorithms. It provides an instrument for coding graph algorithms that scale over distributed memory. In this work, we provide an analysis-transformation framework that leverages general semantics associated with iterations involving nodes and their neighbors, within StarPlat, to aggregate communication. The framework scans for patterns that warrant re-ordering in neighborhood access patterns, aggregate communication, and avoid communication altogether with opportunistic caching in reduction constructs. We also architect an optimized bulk-reduction substrate using Open MPI's passive Remote Memory Access (RMA) constructs. We applied our optimization logic to StarPlat's distributed backend and outperformed d-Galois by 2.05 and DRONE by 1.44 times in Single Source Shortest Paths across several big data graphs.

</details>


### [10] [Trace-based, time-resolved analysis of MPI application performance using standard metrics](https://arxiv.org/abs/2512.01764)
*Kingshuk Haldar*

Main category: cs.DC

TL;DR: 本文提出了一种基于时间窗口的MPI性能分析方法，通过将执行轨迹分解为固定或自适应的时间段，计算并分析每个时间段内的标准MPI性能指标，以揭示短暂的性能瓶颈。


<details>
  <summary>Details</summary>
Motivation: 随着MPI应用轨迹数据的增长和通信行为的复杂化，现有的性能分析工具（如PARAVER、Vampir和Intel Trace Analyzer）在处理大规模数据时存在一定问题，尤其是在可视化上。传统的全局聚合指标可能掩盖了局部性能瓶颈。因此，需要一种能够分析各时间段性能数据的方法，以更好地发现性能问题。

Method: 提出了一种基于时间段的MPI性能分析方法，利用Paraver轨迹数据构建关键执行路径并处理常见事件异常（如时钟不一致和未匹配的MPI事件），并按固定或自适应的时间窗口进行分析，计算出每个窗口内的性能指标。

Result: 在合成基准和真实应用（LaMEM和ls1-MarDyn）上验证表明，该方法能够通过计算时间分辨率性能指标准确定位性能瓶颈，提供了一种轻量级且可扩展的替代方案，尤其在轨迹数据过于庞大而无法进行可视化处理时效果良好。

Conclusion: 通过时间分辨率的性能分析方法，能够更加细致地分析和发现MPI应用中的性能问题。

Abstract: Detailed trace analysis of MPI applications is essential for performance engineering, but growing trace sizes and complex communication behaviour often render comprehensive visual inspection impractical. This work presents a trace-based calculation of time-resolved values of standard MPI performance metrics, load balance, serialisation, and transfer efficiency, by discretising execution traces into fixed or adaptive time segments. The implementation processes Paraver traces postmortem, reconstructing critical execution paths and handling common event anomalies, such as clock inconsistencies and unmatched MPI events, to robustly calculate metrics for each segment. The calculated per-window metric values expose transient performance bottlenecks that the timeaggregated metrics from existing tools may conceal. Evaluations on a synthetic benchmark and real-world applications (LaMEM and ls1-MarDyn) demonstrate how time-resolved metrics reveal localised performance bottlenecks obscured by global aggregates, offering a lightweight and scalable alternative even when trace visualisation is impractical.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [11] [HMARK: Radioactive Multi-Bit Semantic-Latent Watermarking for Diffusion Models](https://arxiv.org/abs/2512.00094)
*Kexin Li,Guozhen Ding,Ilya Grishchenko,David Lie*

Main category: cs.CR

TL;DR: 提出了一种新的多比特水印方案HMARK，用于在图像扩散模型中嵌入所有权信息。该方案通过在语义潜在空间（h-space）中编码水印信息，以实现水印的放射性、鲁棒性和对感知质量的最小影响。实验证明在各种失真类型下，HMARK具有很高的水印检测准确率、比特级恢复准确率、召回率和AUC值。


<details>
  <summary>Details</summary>
Motivation: 现代的生成扩散模型依赖于大规模训练数据集，其中可能包含未经授权或使用权不确定的图像。水印技术可以帮助检测模型训练是否使用了这些未经授权的数据。有效的图像保护水印除了要具备放射性，还需满足隐形性、鲁棒性和多比特容量等要求。

Method: 提出了一种新颖的多比特水印方案HMARK，该方案将所有权信息作为秘密比特编码在图像扩散模型的语义潜在空间（h-space）中。通过利用h-space的可解释性和语义意义，确保水印信号对应有意义的语义属性，从而实现水印的放射性、抗扭曲和减少对感知质量的影响。

Result: 实验结果表明，在各种失真类型下，HMARK在通过下游对抗模型（用LoRA在水印数据上微调）生成的图像上实现了98.57%的水印检测准确率、95.07%的比特级恢复准确率、100%的召回率和1.0的AUC值。

Conclusion: HMARK是一种有效的多比特水印方案，能够很好地保护图像免受未经授权的训练使用。通过在语义潜在空间嵌入水印信息，HMARK实现了高精度水印检测、良好的比特级恢复、完美的召回率和AUC值，在各种失真下表现良好。

Abstract: Modern generative diffusion models rely on vast training datasets, often including images with uncertain ownership or usage rights. Radioactive watermarks -- marks that transfer to a model's outputs -- can help detect when such unauthorized data has been used for training. Moreover, aside from being radioactive, an effective watermark for protecting images from unauthorized training also needs to meet other existing requirements, such as imperceptibility, robustness, and multi-bit capacity. To overcome these challenges, we propose HMARK, a novel multi-bit watermarking scheme, which encodes ownership information as secret bits in the semantic-latent space (h-space) for image diffusion models. By leveraging the interpretability and semantic significance of h-space, ensuring that watermark signals correspond to meaningful semantic attributes, the watermarks embedded by HMARK exhibit radioactivity, robustness to distortions, and minimal impact on perceptual quality. Experimental results demonstrate that HMARK achieves 98.57% watermark detection accuracy, 95.07% bit-level recovery accuracy, 100% recall rate, and 1.0 AUC on images produced by the downstream adversarial model finetuned with LoRA on watermarked data across various types of distortions.

</details>


### [12] [Quantum-Adversary-Resilient Evidence Structures and Migration Strategies for Regulated AI Audit Trails](https://arxiv.org/abs/2512.00110)
*Leo Kao*

Main category: cs.CR

TL;DR: 本文研究了针对量子敌手的安全证据结构及其后量子实现与迁移策略，用于受监管AI的可审计追踪。


<details>
  <summary>Details</summary>
Motivation: 传统签名方案构建的恒定大小加密证据记录在临床、制药和金融领域存在量子攻击下的长期安全隐患，需设计后量子安全的证据结构与迁移方案。

Method: 引入基于游戏的Q-Audit Integrity、Q-Non-Equivocation和Q-Binding安全定义，分析基于QROM的hash-and-sign实现，提出混合签名、证据重签与默克尔根的迁移模式。

Result: QROM下hash-and-sign方案满足量子安全要求；三种迁移策略在安全、存储和计算间具备可行权衡；工业案例显示量子安全审计追踪可达，迁移能显著延长现有证据寿命。

Conclusion: 后量子安全审计追踪可在适度开销下实现，系统性迁移是保护现有证据结构的有效路径。

Abstract: Constant-size cryptographic evidence records are increasingly used to build audit trails for regulated AI workloads in clinical, pharmaceutical, and financial settings, where each execution is summarized by a compact, verifiable record of code identity, model version, data digests, and platform measurements. Existing instantiations, however, typically rely on classical signature schemes whose long-term security is threatened by quantum-capable adversaries. In this paper we formalize security notions for evidence structures in the presence of quantum adversaries and study post-quantum (PQ) instantiations and migration strategies for deployed audit logs. We recall an abstraction of constant-size evidence structures and introduce game-based definitions of Q-Audit Integrity, Q-Non-Equivocation, and Q-Binding, capturing the inability of a quantum adversary to forge, equivocate, or rebind evidence items. We then analyze a hash-and-sign instantiation in the quantum random-oracle model (QROM), assuming an existentially unforgeable PQ signature scheme against quantum adversaries, and show that the resulting evidence structure satisfies these notions under standard assumptions. Building on this, we present three migration patterns for existing evidence logs: hybrid signatures, re-signing of legacy evidence, and Merkle-root anchoring, and analyze their security, storage, and computational trade-offs. A case study based on an industrial constant-size evidence platform for regulated AI at Codebat Technologies Inc. suggests that quantum-safe audit trails are achievable with moderate overhead and that systematic migration can significantly extend the evidentiary lifetime of existing deployments.

</details>


### [13] [NetDeTox: Adversarial and Efficient Evasion of Hardware-Security GNNs via RL-LLM Orchestration](https://arxiv.org/abs/2512.00119)
*Zeng Wang,Minghao Shao,Akashdeep Saha,Ramesh Karri,Johann Knechtel,Muhammad Shafique,Ozgur Sinanoglu*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Graph neural networks (GNNs) have shown promise in hardware security by learning structural motifs from netlist graphs. However, this reliance on motifs makes GNNs vulnerable to adversarial netlist rewrites; even small-scale edits can mislead GNN predictions. Existing adversarial approaches, ranging from synthesis-recipe perturbations to gate transformations, come with high design overheads. We present NetDeTox, an automated end-to-end framework that orchestrates large language models (LLMs) with reinforcement learning (RL) in a systematic manner, enabling focused local rewriting. The RL agent identifies netlist components critical for GNN-based reasoning, while the LLM devises rewriting plans to diversify motifs that preserve functionality. Iterative feedback between the RL and LLM stages refines adversarial rewritings to limit overheads. Compared to the SOTA work AttackGNN, NetDeTox successfully degrades the effectiveness of all security schemes with fewer rewrites and substantially lower area overheads (reductions of 54.50% for GNN-RE, 25.44% for GNN4IP, and 41.04% for OMLA, respectively). For GNN4IP, ours can even optimize/reduce the original benchmarks' area, in particular for larger circuits, demonstrating the practicality and scalability of NetDeTox.

</details>


### [14] [An Empirical Study on the Security Vulnerabilities of GPTs](https://arxiv.org/abs/2512.00136)
*Tong Wu,Weibin Wu,Zibin Zheng*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Equipped with various tools and knowledge, GPTs, one kind of customized AI agents based on OpenAI's large language models, have illustrated great potential in many fields, such as writing, research, and programming. Today, the number of GPTs has reached three millions, with the range of specific expert domains becoming increasingly diverse. However, given the consistent framework shared among these LLM agent applications, systemic security vulnerabilities may exist and remain underexplored. To fill this gap, we present an empirical study on the security vulnerabilities of GPTs. Building upon prior research on LLM security, we first adopt a platform-user perspective to conduct a comprehensive attack surface analysis across different system components. Then, we design a systematic and multidimensional attack suite with the explicit objectives of information leakage and tool misuse based on the attack surface analysis, thereby concretely demonstrating the security vulnerabilities that various components of GPT-based systems face. Finally, we accordingly propose defense mechanisms to address the aforementioned security vulnerabilities. By increasing the awareness of these vulnerabilities and offering critical insights into their implications, this study seeks to facilitate the secure and responsible application of GPTs while contributing to developing robust defense mechanisms that protect users and systems against malicious attacks.

</details>


### [15] [DeFi TrustBoost: Blockchain and AI for Trustworthy Decentralized Financial Decisions](https://arxiv.org/abs/2512.00142)
*Swati Sachan,Dale S. Fickett*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: This research introduces the Decentralized Finance (DeFi) TrustBoost Framework, which combines blockchain technology and Explainable AI to address challenges faced by lenders underwriting small business loan applications from low-wealth households. The framework is designed with a strong emphasis on fulfilling four crucial requirements of blockchain and AI systems: confidentiality, compliance with data protection laws, resistance to adversarial attacks, and compliance with regulatory audits. It presents a technique for tamper-proof auditing of automated AI decisions and a strategy for on-chain (inside-blockchain) and off-chain data storage to facilitate collaboration within and across financial organizations.

</details>


### [16] [Measuring Memecoin Fragility](https://arxiv.org/abs/2512.00377)
*Yuexin Xiang,SM Mahir Shazeed Rish,Qishuang Fu,Yuquan Li,Qin Wang,Tsz Hon Yuen,Jiangshan Yu*

Main category: cs.CR

TL;DR: 提出并验证了首个模因币生态脆弱性评估框架ME2F，发现风险呈两极分化分布


<details>
  <summary>Details</summary>
Motivation: 模因币作为新兴加密资产类别缺乏系统性风险评估工具，需揭示其脆弱性机制

Method: 构建三因子ME2F框架（波动性动态评分、鲸鱼支配评分、情绪放大评分），覆盖65%市场份额的代表性代币

Result: 政治主题模因币风险得分显著高于DOGE等成熟币种（1.8倍），ETH/SOL因流动性优势保持稳健

Conclusion: 模因币生态系统存在结构性风险差异，需建立针对性治理机制提升Web3市场韧性

Abstract: Memecoins, emerging from internet culture and community-driven narratives, have rapidly evolved into a unique class of crypto assets. Unlike technology-driven cryptocurrencies, their market dynamics are primarily shaped by viral social media diffusion, celebrity influence, and speculative capital inflows.
  To capture the distinctive vulnerabilities of these ecosystems, we present the first Memecoin Ecosystem Fragility Framework (ME2F). ME2F formalizes memecoin risks in three dimensions: i) Volatility Dynamics Score capturing persistent and extreme price swings together with spillover from base chains; ii) Whale Dominance Score quantifying ownership concentration among top holders; and iii) Sentiment Amplification Score measuring the impact of attention-driven shocks on market stability.
  We apply ME2F to representative tokens (over 65\% market share) and show that fragility is not evenly distributed across the ecosystem. Politically themed tokens such as TRUMP, MELANIA, and LIBRA concentrate the highest risks, combining volatility, ownership concentration, and sensitivity to sentiment shocks. Established memecoins such as DOGE, SHIB, and PEPE fall into an intermediate range. Benchmark tokens ETH and SOL remain consistently resilient due to deeper liquidity and institutional participation. Our findings provide the first ecosystem-level evidence of memecoin fragility and highlight governance implications for enhancing market resilience in the Web3 era.

</details>


### [17] [Red Teaming Large Reasoning Models](https://arxiv.org/abs/2512.00412)
*Jiawei Chen,Yang Yang,Chao Yu,Yu Tian,Zhi Cao,Linghao Li,Hang Su,Zhaoxia Yin*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Large Reasoning Models (LRMs) have emerged as a powerful advancement in multi-step reasoning tasks, offering enhanced transparency and logical consistency through explicit chains of thought (CoT). However, these models introduce novel safety and reliability risks, such as CoT-hijacking and prompt-induced inefficiencies, which are not fully captured by existing evaluation methods. To address this gap, we propose RT-LRM, a unified benchmark designed to assess the trustworthiness of LRMs. RT-LRM evaluates three core dimensions: truthfulness, safety and efficiency. Beyond metric-based evaluation, we further introduce the training paradigm as a key analytical perspective to investigate the systematic impact of different training strategies on model trustworthiness. We achieve this by designing a curated suite of 30 reasoning tasks from an observational standpoint. We conduct extensive experiments on 26 models and identify several valuable insights into the trustworthiness of LRMs. For example, LRMs generally face trustworthiness challenges and tend to be more fragile than Large Language Models (LLMs) when encountering reasoning-induced risks. These findings uncover previously underexplored vulnerabilities and highlight the need for more targeted evaluations. In addition, we release a scalable toolbox for standardized trustworthiness research to support future advancements in this important field. Our code and datasets will be open-sourced.

</details>


### [18] [BEACON: Automatic Container Policy Generation using Environment-aware Dynamic Analysis](https://arxiv.org/abs/2512.00414)
*Haney Kang,Eduard Marin,Myoungsung You,Diego Perino,Seungwon Shin,Jinwoo Kim*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: This paper introduces BeaCon, a novel tool for the automated generation of adjustable container security policies. Unlike prior approaches, BeaCon leverages dynamic analysis to simulate realistic environments, uncovering container execution paths that may remain hidden during the profiling phase. To address the challenge of exploring vast profiling spaces, we employ efficient heuristics to reveal additional system events with minimal effort. In addition, BeaCon incorporates a security and functionality scoring mechanism to prioritize system calls and capabilities based on their impact on the host OS kernel's security and the functionality of containerized applications. By integrating these scores, BeaCon achieves a customized balance between security and functionality, enabling cloud providers to enforce security measures while maintaining tenant availability. We implemented a prototype of BeaCon using eBPF kernel technology and conducted extensive evaluations. Results from the top 15 containers, which revealed significant improvements, demonstrate that BeaCon identifies an average of 16.5% additional syscalls by applying diverse environments. Furthermore, we evaluated its effectiveness in mitigating risks associated with 45 known vulnerabilities (e.g., CVEs), showcasing its potential to significantly enhance container security. Additionally, we performed proof-of-concept demonstrations for two well-known security vulnerabilities, showing that BeaCon successfully reduces attack surface by blocking these exploits.

</details>


### [19] [A Unified Framework for Constructing Information-Theoretic Private Information Retrieval](https://arxiv.org/abs/2512.00480)
*Liang Feng Zhang*

Main category: cs.CR

TL;DR: 介绍了一种新的离散结构FOASC，并提出了一个统一的框架来构建IT-PIR协议。


<details>
  <summary>Details</summary>
Motivation: 为了解决在IT-PIR领域中最经典和最具挑战性的问题：构建具有较低通信复杂度的协议。

Method: 引入了一种新结构叫做带跨度能力正交阵列族（FOASC），并提出一个统一的框架构建IT-PIR协议。

Result: 展示了如何将文献中最有影响力的IT-PIR协议纳入此框架，并提出了一些未解的关于FOASC的有趣问题。

Conclusion: 通过FOASC结构及其问题的解决，可能导致创新的IT-PIR协议的产生。

Abstract: Retrieving up-to-date information from a publicly accessible database poses significant threats to the user's privacy. {\em Private information retrieval} (PIR) protocols allow a user to retrieve any entry from a database, without revealing the identity of the entry being retrieved to the server(s). Such protocols have found numerous applications in both theoretical studies and real-life scenarios. The existing PIR constructions mainly give multi-server {\em information-theoretic} PIR (IT-PIR) protocols or single-server computational PIR (CPIR) protocols. Compared with CPIR, IT-PIR protocols are computationally more efficient and secure in the presence of unbounded servers. The most classical and challenging problem in the realm of IT-PIR is constructing protocols with lower {\em communication complexity}. In this review, we introduce a new discrete structure called {\em families of orthogonal arrays with span capability} (FOASC) and propose a unified framework for constructing IT-PIR protocols. We show how the most influential IT-PIR protocols in the literature can be captured by the framework. We also put forward several interesting open problems concerning FOASC, whose solutions may result in innovative IT-PIR protocols.

</details>


### [20] [TrojanLoC: LLM-based Framework for RTL Trojan Localization](https://arxiv.org/abs/2512.00591)
*Weihua Xiao,Zeng Wang,Minghao Shao,Raghu Vamshi Hemadri,Ozgur Sinanoglu,Muhammad Shafique,Johann Knechtel,Siddharth Garg,Ramesh Karri*

Main category: cs.CR

TL;DR: 提出了一种基于LLM的RTL级硬件木马定位框架TrojanLoC，能直接处理和标注RTL代码，结合全局设计上下文和局部语义执行模块级和行级的木马检测、类型预测和精细化定位。


<details>
  <summary>Details</summary>
Motivation:  硬件木马对集成电路构成威胁，现有方法在图转换过程中丢失RTL语义，受浅层GNN限制且仅能粗糙的模块级二进制检测。

Method: 使用RTL微调的LLM直接进行模块级和行级嵌入，捕获全局设计框架和局部语义；在这些嵌入的基础上训练特定任务分类器执行模块及行级硬件木马检测，类型预测和定位。同时引入了TrojanInS大型合成数据集，包含从四个基础效应类别系统注入的木马，每个都有精确行级注释。

Result:  TrojanLoC达到了强大的模块级性能，在木马检测方面达到了0.99的F1得分，在木马类型分类上达到0.84的平均F1,比基线高出最多0.68。在行级，TrojanLoC进一步实现了最大0.93的平均F1，使得对特洛伊马相关的RTL线进行精细化定位成为可能。

Conclusion: TrojanLoC是对现有硬件木马检测方法的有效改进，能够在更精细的层次上识别和定位RTL级别的木马。

Abstract: Hardware Trojans (HT s) are a persistent threat to integrated circuits, especially when inserted at the register-transfer level (RTL). Existing methods typically first convert the design into a graph, such as a gate-level netlist or an RTL-derived dataflow graph (DFG), and then use a graph neural network (GNN ) to obtain an embedding of that graph, which (i) loses compact RTL semantics, (ii) relies on shallow GNNs with limited receptive field, and (iii) is largely restricted to coarse, module-level binary HT detection. We propose TrojanLoC, an LLM-based framework for RTL-level HT localization. We use an RTL-finetuned LLM to derive module-level and line-level embeddings directly from RTL code, capturing both global design context and local semantics. Next, we train task-specific classifiers on these embeddings to perform module-level Trojan detection, type prediction, and fine-grained line-level localization. We also introduce TrojanInS, a large synthetic dataset of RTL designs with systematically injected Trojans from four effect-based categories, each accompanied by precise line-level annotations. Our experiments show that TrojanLoC achieves strong module-level performance, reaching 0.99 F1-score for Trojan detection, up to 0.68 higher than baseline, and 0.84 macro-F1 for Trojan-type classification. At the line level, TrojanLoc further achieves up to 0.93 macro-F1, enabling fine-grained localization of Trojan-relevant RTL lines

</details>


### [21] [Extended Abstract: Synthesizable Low-overhead Circuit-level Countermeasures and Pro-Active Detection Techniques for Power and EM SCA](https://arxiv.org/abs/2512.00635)
*Archisman Ghosh*

Main category: cs.CR

TL;DR: 本文主要关注对嵌入式设备的旁道攻击（SCA），并提出了两种低开销的硬件综合对抗措施。


<details>
  <summary>Details</summary>
Motivation: 由于目前的互联网连接嵌入式设备数据安全性不足，存在旁道攻击（SCA）的威胁，特别是在实现密码算法时。

Method: 提出了两种新型硬件综合对抗措施：1) 采用集成的电感传感器检测电磁波旁道分析、时钟故障注入攻击以及电压故障注入攻击 2) 参与了NIST后量子密码标准化的过程，贡献了能量和面积效率最高版本的Saber方案。

Result: 提出的电感传感器在检测SCA上几乎无额外开销。参与实现的Saber方案是NIST标准化最终候选之一，其能量消耗和芯片面积最低。

Conclusion: 所提出的方法显著降低了旁道攻击的风险，且易于在能量受限的物联网设备中应用。

Abstract: The gamut of todays internet-connected embedded devices has led to increased concerns regarding the security and confidentiality of data. Most internet-connected embedded devices employ mathematically secure cryptographic algorithms to address security vulnerabilities. Despite such mathematical guarantees, as these algorithms are often implemented in silicon, they leak critical information in terms of power consumption, electromagnetic (EM) radiation, timing, cache hits and misses, photonic emission and so on, leading to side-channel analysis (SCA) attacks. This thesis focuses on low overhead generic circuit-level yet synthesizable countermeasures against power and EM SCA. Existing countermeasures (including proposed) still have relatively high overhead which bars them from being used in energy-constraint IoT devices. We propose a zero-overhead integrated inductive sensor which is able to detect i)EM SCA ii) Clock glitch-based Fault Injection Attack (FIA), and iii) Voltage-glitch based Fault Injection Attack by using a simple ML algorithm. Advent of quantum computer research will open new possibilities for theoretical attacks against existing cryptographic protocols. National Institute of Standard & Technology (NIST) has standardized post-quantum cryptographic algorithms to secure crypto-systems against quantum adversary. I contribute to the standardization procedure by introducing the first silicon-verified Saber (a NIST finalist modulo Learning with Rounding scheme) which consumes lowest energy and area till date amongst all the candidates.

</details>


### [22] [Blockchain-based vs. SQL Database Systems for Digital Twin Evidence Management: A Comparative Forensic Analysis](https://arxiv.org/abs/2512.00645)
*Boyd Franken,Hong-Hanh Nguyen-Le,Nhien-An Le-Khac*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Digital forensics faces unprecedented challenges with the emergence of digital twins and metaverse technologies. This paper presents the first comparative analysis between blockchain-based and traditional database systems for managing digital twin evidence in forensic investigations. We conducted controlled experiments comparing the Ethereum blockchain with IPFS storage against traditional SQL databases for digital twin evidence management. Our findings reveal that while blockchain provides superior data integrity and immutability, crucial for forensic applications, traditional databases offer better performance consistency. The blockchain implementation showed faster average storage times but higher variability in retrieval operations. Both systems maintained forensic integrity through hash verification, though blockchain's immutable nature provides additional security guarantees essential for legal proceedings. This research contributes to the development of robust digital forensic methodologies for emerging technologies in the metaverse era.

</details>


### [23] [Concept-Guided Backdoor Attack on Vision Language Models](https://arxiv.org/abs/2512.00713)
*Haoyu Shen,Weimin Lyu,Haotian Xu,Tengfei Ma*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Vision-Language Models (VLMs) have achieved impressive progress in multimodal text generation, yet their rapid adoption raises increasing concerns about security vulnerabilities. Existing backdoor attacks against VLMs primarily rely on explicit pixel-level triggers or imperceptible perturbations injected into images. While effective, these approaches reduce stealthiness and remain vulnerable to image-based defenses. We introduce concept-guided backdoor attacks, a new paradigm that operates at the semantic concept level rather than on raw pixels. We propose two different attacks. The first, Concept-Thresholding Poisoning (CTP), uses explicit concepts in natural images as triggers: only samples containing the target concept are poisoned, causing the model to behave normally in all other cases but consistently inject malicious outputs whenever the concept appears. The second, CBL-Guided Unseen Backdoor (CGUB), leverages a Concept Bottleneck Model (CBM) during training to intervene on internal concept activations, while discarding the CBM branch at inference time to keep the VLM unchanged. This design enables systematic replacement of a targeted label in generated text (for example, replacing "cat" with "dog"), even when the replacement behavior never appears in the training data. Experiments across multiple VLM architectures and datasets show that both CTP and CGUB achieve high attack success rates while maintaining moderate impact on clean-task performance. These findings highlight concept-level vulnerabilities as a critical new attack surface for VLMs.

</details>


### [24] [MASCOT: Analyzing Malware Evolution Through A Well-Curated Source Code Dataset](https://arxiv.org/abs/2512.00741)
*Bojing Li,Duo Zhong,Dharani Nadendla,Gabriel Terceros,Prajna Bhandar,Raguvir S,Charles Nicholas*

Main category: cs.CR

TL;DR: 近年来，恶意软件呈现爆炸性增长与大规模代码复用，造成样本间复杂的演化关系。现有研究难以及时捕捉最新趋势，且缺乏直观工具解析样本或类别间的关联。本论文提供了一个包含6032个样本的手动审核代码数据集，从软件工程学角度延伸并扩展了现有研究，对现代恶意软件的规模、开发成本、代码质量、安全性及依赖性进行了系统评估。引入了多视图族谱分析以阐明恶意软件的关联：从整体视图分析了样本和类别间连接强度和方向；从细节视图追溯了单个样本的演化历史。实验显示，尽管代码质量持续存在问题，但恶意软件样本呈现出日益增长的复杂性和标准化，与主流软件工程的发展保持同步。族谱分析直观揭示了由代码复用驱动的谱系扩张和演化，为理解恶意软件生态系统的形成和进化提供了新证据和工具。


<details>
  <summary>Details</summary>
Motivation: 现有研究难以及时捕捉最新趋势，且缺乏直观工具解析样本或类别间的关联。

Method: 提供了一个包含6032个样本的手动审核代码数据集，并引入了多视图族谱分析以阐明恶意软件的关联。

Result: 恶意软件样本呈现出日益增长的复杂性和标准化，族谱分析揭示由代码复用驱动的谱系扩张和演化，为理解恶意软件生态系统的形成和进化提供了新证据和工具。

Conclusion: 从软件工程的角度分析，本文提供了对现代恶意软件进化历程的重要见解和工具。

Abstract: In recent years, the explosion of malware and extensive code reuse have formed complex evolutionary connections among malware specimens. The rapid pace of development makes it challenging for existing studies to characterize recent evolutionary trends. In addition, intuitive tools to untangle these intricate connections between malware specimens or categories are urgently needed. This paper introduces a manually-reviewed malware source code dataset containing 6032 specimens. Building on and extending current research from a software engineering perspective, we systematically evaluate the scale, development costs, code quality, as well as security and dependencies of modern malware. We further introduce a multi-view genealogy analysis to clarify malware connections: at an overall view, this analysis quantifies the strength and direction of connections among specimens and categories; at a detailed view, it traces the evolutionary histories of individual specimens. Experimental results indicate that, despite persistent shortcomings in code quality, malware specimens exhibit an increasing complexity and standardization, in step with the development of mainstream software engineering practices. Meanwhile, our genealogy analysis intuitively reveals lineage expansion and evolution driven by code reuse, providing new evidence and tools for understanding the formation and evolution of the malware ecosystem.

</details>


### [25] [Bias Injection Attacks on RAG Databases and Sanitization Defenses](https://arxiv.org/abs/2512.00804)
*Hao Wu,Prateek Saxena*

Main category: cs.CR

TL;DR: 该论文研究了RAG系统中针对向量数据库的攻击和防御。


<details>
  <summary>Details</summary>
Motivation: 知识库中毒攻击主要是注入假或有害内容，但该论文揭示了一种新的和微妙威胁：偏见注入攻击。

Method: 提出了一种新的攻击方式：偏见注入攻击；并开发了一种新的防御方式：BiasDef。

Result: 该论文提出的攻击方式有效回避了现有防御措施，并且新开发的防御方式BiasDef表现优于现有的方法。

Conclusion: 该论文揭示了一种新的偏见注入攻击，该攻击不易被察觉并且能够影响大规模语言模型的回答，并展示了新开发的防御方式BiasDef的有效。

Abstract: This paper explores attacks and defenses on vector databases in retrieval-augmented generation (RAG) systems. Prior work on knowledge poisoning attacks primarily inject false or toxic content, which fact-checking or linguistic analysis easily detects. We reveal a new and subtle threat: bias injection attacks, which insert factually correct yet semantically biased passages into the knowledge base to covertly influence the ideological framing of answers generated by large language models (LLMs). We demonstrate that these adversarial passages, though linguistically coherent and truthful, can systematically crowd out opposing views from the retrieved context and steer LLM answers toward the attacker's intended perspective.
  We precisely characterize this class of attacks and then develop a post-retrieval filtering defense, BiasDef. We construct a comprehensive benchmark based on public question answering datasets to evaluate them. Our results show that: (1) the proposed attack induces significant perspective shifts in LLM answers, effectively evading existing retrieval-based sanitization defenses; and (2) BiasDef outperforms existing methods by reducing adversarial passages retrieved by 15\% which mitigates perspective shift by 6.2\times in answers, while enabling the retrieval of 62\% more benign passages.

</details>


### [26] [Logic Encryption: This Time for Real](https://arxiv.org/abs/2512.00833)
*Rupesh Raj Karn,Lakshmi Likhitha Mankali,Zeng Wang,Saideep Sreekumar,Prithwish Basu Roy,Ozgur Sinanoglu,Lilas Alrahis,Johann Knechtel*

Main category: cs.CR

TL;DR: 提出了一种基于逻辑加密（LE）的新型IP保护方法，通过编码和加密逻辑本身混淆电路结构和功能。采用标准密码学算法、密钥位随机化、简单电路设计技术和系统级综合操作，以"正确构建"方式实现端到端LE方案。实验证明其在多种无预言攻击下性能优于现有技术，设计开销更低，并提供了开源实现。


<details>
  <summary>Details</summary>
Motivation: 现代电路面临逆向工程、IP盗窃、侧信道攻击等多种威胁，现有逻辑锁定方案存在安全性或效率缺陷。

Method: 基于标准密码学算法和系统级设计技术，提出一种"正确构建"的端到端逻辑加密方法：1）对电路逻辑进行编码和加密实现结构/功能双重混淆 2）引入密钥位随机化 3）采用专用电路设计技术 4）系统级综合优化。

Result: 在多种无预言攻击模型下的对比实验显示：1）覆盖关键威胁场景的有效性 2）设计开销显著低于现有方案 3）已发布开源实现。验证了方案的高效安全性。

Conclusion: 该方法通过逻辑级"混淆"突破了传统保护范式，在保持低开销的同时显著提升抗攻击能力，为IP保护提供了新思路。但也需关注密码学算法在硬件层面的具体实现问题。

Abstract: Modern circuits face various threats like reverse engineering, theft of intellectual property (IP), side-channel attacks, etc. Here, we present a novel approach for IP protection based on logic encryption (LE). Unlike established schemes for logic locking, our work obfuscates the circuit's structure and functionality by encoding and encrypting the logic itself. We devise an end-to-end method for practical LE implementation based on standard cryptographic algorithms, key-bit randomization, simple circuit design techniques, and system-level synthesis operations, all in a correct-by-construction manner. Our extensive analysis demonstrates the remarkable efficacy of our scheme, outperforming prior art against a range of oracle-less attacks covering crucial threat vectors, all with lower design overheads. We provide a full open-source release.

</details>


### [27] [Hesperus is Phosphorus: Mapping Threat Actor Naming Taxonomies at Scale](https://arxiv.org/abs/2512.00857)
*Gonzalo Roa,Manuel Suarez-Roman,Juan Tapiador*

Main category: cs.CR

TL;DR: 本文提出HiP方法以解决CTI供应商间TA命名不一致问题，通过标准化、整合和聚类TA名称提升情报整合效率。


<details>
  <summary>Details</summary>
Motivation: 当前分散的专有TA命名导致研究人员在整合和关联不同CTI报告及TA档案时面临显著障碍。

Method: HiP方法对来自15个数据源的TA名称进行标准化、整合和聚类，构建名称图谱并分析数据。

Result: 分析了13,371份报告及3,287个TA名称，发现别名集中在少数TA、名称随年份演变，揭示了命名扩散的影响因素。报告还指出了映射错误和方法陷阱，讨论了采用TA命名标准的困难。

Conclusion: TA命名标准难以推广，主要因为各CTI供应商的私密敏感遥测数据难以共享。

Abstract: This paper studies the problem of Threat Actor (TA) naming convention inconsistency across leading Cyber Threat Intelligence (CTI) vendors. The current decentralized and proprietary nomenclature creates confusion and significant obstacles for researchers, including difficulties in integrating and correlating disparate CTI reports and TA profiles. This paper introduces HiP (Hesperus is Phosphorus, a reference to the classic question about the Morning and the Evening Star), a methodology for normalizing, integrating, and clustering TA names presumably corresponding to the same entity. Using HiP, we analyze a large dataset collected from 15 sources and spanning 13,371 CTI reports, 17 vendor taxonomies, 3,287 TA names, and 8 mappings between them. Our analysis of the resulting name graph provides insights on key features of the problem, such as the concentration of aliases on a relatively small subset of TAs, the evolution of this phenomenon over the years, and the factors that could explain TA name proliferation. We also report errors in the mappings and methodological pitfalls that contribute to make certain TA name clusters larger than they should be, including the use of temporary names for activity clusters, the existence of common tools and infrastructure, and overlapping operations. We conclude with a discussion on the inherent difficulties to adopt a TA naming standard, a quest fundamentally hampered by the need to share highly-sensitive telemetry that is private to each CTI vendor.

</details>


### [28] [Sliced Rényi Pufferfish Privacy: Directional Additive Noise Mechanism and Private Learning with Gradient Clipping](https://arxiv.org/abs/2512.01115)
*Tao Zhang,Yevgeniy Vorobeychik*

Main category: cs.CR

TL;DR: 为解决Renyi Pufferfish Privacy (RPP)在高维传输校准和迭代学习中的组合规则缺失问题，提出Sliced Renyi Pufferfish Privacy (SRPP)及其相关机制。


<details>
  <summary>Details</summary>
Motivation: 填补RPP的两个实际空白：高维最优传输校准及缺乏通用的、机制无关的迭代学习组合规则。

Method: 引入SRPP，通过单位向量定向比较替代高维比较；提出分片Wasserstein机制计算单维敏感性以实现噪声校准；定义SRPE作为可计算上界；设计SRPP-SGD方案与HUC/ms-HUC会计策略。

Result: SRPP方法在静态和迭代环境中均实现了有利的隐私-效用权衡，且多机制独立训练时可进行温和的加性组合。

Conclusion: SRPP解决了RPP的局限，提供了计算高效、几何感知且实用的隐私保证策略。

Abstract: We study privatization mechanism design and privacy accounting in the Pufferfish family, addressing two practical gaps of Renyi Pufferfish Privacy (RPP): high-dimensional optimal transport (OT) calibration and the absence of a general, mechanism-agnostic composition rule for iterative learning. We introduce Sliced Renyi Pufferfish Privacy (SRPP), which replaces high-dimensional comparisons by directional ones over a set of unit vectors, enabling geometry-aware and tractable guarantees. To calibrate noise without high-dimensional OT, we propose sliced Wasserstein mechanisms that compute per-direction (1-D) sensitivities, yielding closed-form, statistically stable, and anisotropic calibrations. We further define SRPP Envelope (SRPE) as computable upper bounds that are tightly implementable by these sliced Wasserstein mechanisms. For iterative deep learning algorithms, we develop a decompose-then-compose SRPP-SGD scheme with gradient clipping based on a History-Uniform Cap (HUC), a pathwise bound on one-step directional changes that is uniform over optimization history, and a mean-square variant (ms-HUC) that leverages subsampling randomness to obtain on-average SRPP guarantees with improved utility. The resulting HUC and ms-HUC accountants aggregate per-iteration, per-direction Renyi costs and integrate naturally with moments-accountant style analyses. Finally, when multiple mechanisms are trained and privatized independently under a common slicing geometry, our analysis yields graceful additive composition in both worst-case and mean-square regimes. Our experiments indicate that the proposed SRPP-based methods achieve favorable privacy-utility trade-offs in both static and iterative settings.

</details>


### [29] [Reverse Engineering and Control-Aware Security Analysis of the ArduPilot UAV Framework](https://arxiv.org/abs/2512.01164)
*Yasaswini Konapalli,Lotfi Ben Othmane,Cihan Tunc,Feras Benchellal,Likhita Mudagere*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Unmanned Aerial Vehicle (UAV) technologies are gaining high interest for many domains, which makes UAV security of utmost importance. ArduPilot is among the most widely used open-source autopilot UAV frameworks; yet, many studies demonstrate the vulnerabilities affecting such systems. Vulnerabilities within its communication subsystems (including WiFi, telemetry, or GPS) expose critical entry points, and vulnerabilities in Ardupilot can affect the control procedure. In this paper, we reconstruct the software architecture and the control models implemented by ArduPilot and then examine how these control models could potentially misused to induce malicious behaviors while relying on legitimate inputs.

</details>


### [30] [DefenSee: Dissecting Threat from Sight and Text - A Multi-View Defensive Pipeline for Multi-modal Jailbreaks](https://arxiv.org/abs/2512.01185)
*Zihao Wang,Kar Wai Fok,Vrizlynn L. L. Thing*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Multi-modal large language models (MLLMs), capable of processing text, images, and audio, have been widely adopted in various AI applications. However, recent MLLMs integrating images and text remain highly vulnerable to coordinated jailbreaks. Existing defenses primarily focus on the text, lacking robust multi-modal protection. As a result, studies indicate that MLLMs are more susceptible to malicious or unsafe instructions, unlike their text-only counterparts. In this paper, we proposed DefenSee, a robust and lightweight multi-modal black-box defense technique that leverages image variants transcription and cross-modal consistency checks, mimicking human judgment. Experiments on popular multi-modal jailbreak and benign datasets show that DefenSee consistently enhances MLLM robustness while better preserving performance on benign tasks compared to SOTA defenses. It reduces the ASR of jailbreak attacks to below 1.70% on MiniGPT4 using the MM-SafetyBench benchmark, significantly outperforming prior methods under the same conditions.

</details>


### [31] [CTF Archive: Capture, Curate, Learn Forever](https://arxiv.org/abs/2512.01233)
*Pratham Gupta,Aditya Gabani,Connor Nelson,Yan Shoshitaishvili*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Capture the Flag (CTF) competitions represent a powerful experiential learning approach within cybersecurity education, blending diverse concepts into interactive challenges. However, the short duration (typically 24-48 hours) and ephemeral infrastructure of these events often impede sustained educational benefit. Learners face substantial barriers in revisiting unsolved challenges, primarily due to the cumbersome process of manually reconstructing and rehosting the challenges without comprehensive documentation or guidance. To address this critical gap, we introduce CTF Archive, a platform designed to preserve the educational value of CTF competitions by centralizing and archiving hundreds of challenges spanning over a decade in fully configured, ready-to-use environments. By removing the complexity of environment setup, CTF Archive allows learners to focus directly on conceptual understanding rather than technical troubleshooting. The availability of these preserved challenges encourages in-depth research and exploration at the learner's pace, significantly enhancing conceptual comprehension without the pressures of live competition. Additionally, public accessibility lowers entry barriers, promoting an inclusive educational experience. Overall, CTF Archive provides a scalable solution to integrate persistent, practical cybersecurity learning into academic curricula.

</details>


### [32] [Benchmarking and Understanding Safety Risks in AI Character Platforms](https://arxiv.org/abs/2512.01247)
*Yiluo Wei,Peixian Zhang,Gareth Tyson*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: AI character platforms, which allow users to engage in conversations with AI personas, are a rapidly growing application domain. However, their immersive and personalized nature, combined with technical vulnerabilities, raises significant safety concerns. Despite their popularity, a systematic evaluation of their safety has been notably absent. To address this gap, we conduct the first large-scale safety study of AI character platforms, evaluating 16 popular platforms using a benchmark set of 5,000 questions across 16 safety categories. Our findings reveal a critical safety deficit: AI character platforms exhibit an average unsafe response rate of 65.1%, substantially higher than the 17.7% average rate of the baselines. We further discover that safety performance varies significantly across different characters and is strongly correlated with character features such as demographics and personality. Leveraging these insights, we demonstrate that our machine learning model is able identify less safe characters with an F1-score of 0.81. This predictive capability can be beneficial for platforms, enabling improved mechanisms for safer interactions, character search/recommendations, and character creation. Overall, the results and findings offer valuable insights for enhancing platform governance and content moderation for safer AI character platforms.

</details>


### [33] [Systems Security Foundations for Agentic Computing](https://arxiv.org/abs/2512.01295)
*Mihai Christodorescu,Earlence Fernandes,Ashish Hooda,Somesh Jha,Johann Rehberger,Khawaja Shams*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: This paper articulates short- and long-term research problems in AI agent security and privacy, using the lens of computer systems security. This approach examines end-to-end security properties of entire systems, rather than AI models in isolation. While we recognize that hardening a single model is useful, it is important to realize that it is often insufficient. By way of an analogy, creating a model that is always helpful and harmless is akin to creating software that is always helpful and harmless. The collective experience of decades of cybersecurity research and practice shows that this is insufficient. Rather, constructing an informed and realistic attacker model before building a system, applying hard-earned lessons from software security, and continuous improvement of security posture is a tried-and-tested approach to securing real computer systems. A key goal is to examine where research challenges arise when applying traditional security principles in the context of AI agents. A secondary goal of this report is to distill these ideas for AI and ML practitioners and researchers. We discuss the challenges of applying security principles to agentic computing, present 11 case studies of real attacks on agentic systems, and define a series of new research problems specific to the security of agentic systems.

</details>


### [34] [Securing Large Language Models (LLMs) from Prompt Injection Attacks](https://arxiv.org/abs/2512.01326)
*Omar Farooq Khan Suri,John McCrae*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Large Language Models (LLMs) are increasingly being deployed in real-world applications, but their flexibility exposes them to prompt injection attacks. These attacks leverage the model's instruction-following ability to make it perform malicious tasks. Recent work has proposed JATMO, a task-specific fine-tuning approach that trains non-instruction-tuned base models to perform a single function, thereby reducing susceptibility to adversarial instructions. In this study, we evaluate the robustness of JATMO against HOUYI, a genetic attack framework that systematically mutates and optimizes adversarial prompts. We adapt HOUYI by introducing custom fitness scoring, modified mutation logic, and a new harness for local model testing, enabling a more accurate assessment of defense effectiveness. We fine-tuned LLaMA 2-7B, Qwen1.5-4B, and Qwen1.5-0.5B models under the JATMO methodology and compared them with a fine-tuned GPT-3.5-Turbo baseline. Results show that while JATMO reduces attack success rates relative to instruction-tuned models, it does not fully prevent injections; adversaries exploiting multilingual cues or code-related disruptors still bypass defenses. We also observe a trade-off between generation quality and injection vulnerability, suggesting that better task performance often correlates with increased susceptibility. Our results highlight both the promise and limitations of fine-tuning-based defenses and point toward the need for layered, adversarially informed mitigation strategies.

</details>


### [35] [EmoRAG: Evaluating RAG Robustness to Symbolic Perturbations](https://arxiv.org/abs/2512.01335)
*Xinyun Zhou,Xinfeng Li,Yinan Peng,Ming Xu,Xuanwang Zhang,Miao Yu,Yidong Wang,Xiaojun Jia,Kun Wang,Qingsong Wen,XiaoFeng Wang,Wei Dong*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Retrieval-Augmented Generation (RAG) systems are increasingly central to robust AI, enhancing large language model (LLM) faithfulness by incorporating external knowledge. However, our study unveils a critical, overlooked vulnerability: their profound susceptibility to subtle symbolic perturbations, particularly through near-imperceptible emoticon tokens such as "(@_@)" that can catastrophically mislead retrieval, termed EmoRAG. We demonstrate that injecting a single emoticon into a query makes it nearly 100% likely to retrieve semantically unrelated texts that contain a matching emoticon. Our extensive experiment across general question-answering and code domains, using a range of state-of-the-art retrievers and generators, reveals three key findings: (I) Single-Emoticon Disaster: Minimal emoticon injections cause maximal disruptions, with a single emoticon almost 100% dominating RAG output. (II) Positional Sensitivity: Placing an emoticon at the beginning of a query can cause severe perturbation, with F1-Scores exceeding 0.92 across all datasets. (III) Parameter-Scale Vulnerability: Counterintuitively, models with larger parameters exhibit greater vulnerability to the interference. We provide an in-depth analysis to uncover the underlying mechanisms of these phenomena. Furthermore, we raise a critical concern regarding the robustness assumption of current RAG systems, envisioning a threat scenario where an adversary exploits this vulnerability to manipulate the RAG system. We evaluate standard defenses and find them insufficient against EmoRAG. To address this, we propose targeted defenses, analyzing their strengths and limitations in mitigating emoticon-based perturbations. Finally, we outline future directions for building robust RAG systems.

</details>


### [36] [INFERMAL: Inferential analysis of maliciously registered domains](https://arxiv.org/abs/2512.01391)
*Yevheniya Nosyk,Maciej Korczyński,Carlos Gañán,Sourena Maroofi,Jan Bayer,Zul Odgerel,Samaneh Tajalizadehkhoob,Andrzej Duda*

Main category: cs.CR

TL;DR: 该论文研究了恶意域名注册的驱动因素，通过分析73个特征，发现低价注册费、免费托管服务和宽松限制显著增加恶意活动，而严格限制可有效减少滥用。


<details>
  <summary>Details</summary>
Motivation: 揭示了恶意域名注册的驱动因素，填补了以往研究的空白，为注册商制定针对性反滥用策略提供了依据。

Method: 采用GLM回归分析，归纳了73个与注册属性、主动验证和被动安全相关的特征，量化了各因素对恶意注册的影响。

Result: 每降低1美元注册费，恶意域名增加49%；免费服务使钓鱼活动激增88%；严格限制减少滥用63%，而提供API注册使恶意域名增长401%。

Conclusion: 研究结果可帮助注册商制定反滥用策略，同时需平衡经济激励。

Abstract: Cybercriminals have long depended on domain names for phishing, spam, malware distribution, and botnet operation. To facilitate the malicious activities, they continually register new domain names for exploitation. Previous work revealed an abnormally high concentration of malicious registrations in a handful of domain name registrars and top-level domains (TLDs). Anecdotal evidence suggests that low registration prices attract cybercriminals, implying that higher costs may potentially discourage them. However, no existing study has systematically analyzed the factors driving abuse, leaving a critical gap in understanding how different variables influence malicious registrations. In this report, we carefully distill the inclinations and aversions of malicious actors during the registration of new phishing domain names. We compile a comprehensive list of 73 features encompassing three main latent factors: registration attributes, proactive verification, and reactive security practices. Through a GLM regression analysis, we find that each dollar reduction in registration fees corresponds to a 49% increase in malicious domains. The availability of free services, such as web hosting, drives an 88% surge in phishing activities. Conversely, stringent restrictions cut down abuse by 63%, while registrars providing API access for domain registration or account creation experience a staggering 401% rise in malicious domains. This exploration may assist intermediaries involved in domain registration to develop tailored anti-abuse practices, yet aligning them with their economic incentives.

</details>


### [37] [Inside Qubic's Selfish Mining Campaign on Monero: Evidence, Tactics, and Limits](https://arxiv.org/abs/2512.01437)
*Suhyeon Lee,Hyeongyeong Kim*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We analyze Qubic's advertised selfish mining campaign on Monero in 2025. Combining data from Monero nodes, and the Qubic pool API, we reconstruct Qubic-attributed blocks and hashrate and detect ten intervals consistent with selfish mining strategies. In these intervals, Qubic's average hashrate share rises to the 23-34\% range, yet sustained 51\% control is never observed. We evaluate the campaign against the classical selfish mining model and a modified Markov-chain model that reflects Qubic's conservative release strategy: both predict lower revenue than honest mining at the inferred parameters, and the data largely confirms this while still showing noticeable deviations from the predicted curve. We interpret this gap between model and measurements in terms of Qubic's time-varying hashrate and coarse-grained attack segmentation.

</details>


### [38] [IVE: An Accelerator for Single-Server Private Information Retrieval Using Versatile Processing Elements](https://arxiv.org/abs/2512.01574)
*Sangpyo Kim,Hyesung Ji,Jongmin Kim,Wonseok Choi,Jaiyoung Park,Jung Ho Ahn*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Private information retrieval (PIR) is an essential cryptographic protocol for privacy-preserving applications, enabling a client to retrieve a record from a server's database without revealing which record was requested. Single-server PIR based on homomorphic encryption has particularly gained immense attention for its ease of deployment and reduced trust assumptions. However, single-server PIR remains impractical due to its high computational and memory bandwidth demands. Specifically, reading the entirety of large databases from storage, such as SSDs, severely limits its performance. To address this, we propose IVE, an accelerator for single-server PIR with a systematic extension that enables practical retrieval from large databases using DRAM. Recent advances in DRAM capacity allow PIR for large databases to be served entirely from DRAM, removing its dependence on storage bandwidth. Although the memory bandwidth bottleneck still remains, multi-client batching effectively amortizes database access costs across concurrent requests to improve throughput. However, client-specific data remains a bottleneck, whose bandwidth requirements ultimately limits performance. IVE overcomes this by employing a large on-chip scratchpad with an operation scheduling algorithm that maximizes data reuse, further boosting throughput. Additionally, we introduce sysNTTU, a versatile functional unit that enhances area efficiency without sacrificing performance. We also propose a heterogeneous memory system architecture, which enables a linear scaling of database sizes without a throughput degradation. Consequently, IVE achieves up to 1,275x higher throughput compared to prior PIR hardware solutions.

</details>


### [39] [Beyond the Hype: A Large-Scale Empirical Analysis of On-Chain Transactions in NFT Scams](https://arxiv.org/abs/2512.01577)
*Wenkai Li,Zongwei Li,Xiaoqi Li,Chunyi Zhang,Xiaoyan Zhang,Yuqing Zhang*

Main category: cs.CR

TL;DR: 本文通过图分析系统性地探讨了NFT钓鱼欺诈的交易模式和特征，为未来NFT钓鱼欺诈的检测与防范提供了重要参考价值。


<details>
  <summary>Details</summary>
Motivation: NFT钓鱼欺诈活动造成了重大财产损失，当前尚无研究针对NFT钓鱼欺诈的交易模式。

Method: 收集、清洗、统一多平台发布的NFT钓鱼事件的交易记录等数据，构建交易图分析NFT钓鱼欺诈的分布、交互特征等。

Result: 普通交易占比96.71%，钓鱼账户占比0.94%却出现在8.36%的交易场景，NFT钓鱼欺诈有集体性、特定目标、多代币标准交互等特征。

Conclusion: 研究揭示了NFT钓鱼欺诈的核心行为特征。

Abstract: Non-fungible tokens (NFTs) serve as a representative form of digital asset ownership and have attracted numerous investors, creators, and tech enthusiasts in recent years. However, related fraud activities, especially phishing scams, have caused significant property losses. There are many graph analysis methods to detect malicious scam incidents, but no research on the transaction patterns of the NFT scams. Therefore, to fill this gap, we are the first to systematically explore NFT phishing frauds through graph analysis, aiming to comprehensively investigate the characteristics and patterns of NFT phishing frauds on the transaction graph. During the research process, we collect transaction records, log data, and security reports related to NFT phishing incidents published on multiple platforms. After collecting, sanitizing, and unifying the data, we construct a transaction graph and analyze the distribution, transaction features, and interaction patterns of NFT phishing scams. We find that normal transactions on the blockchain accounted for 96.71% of all transactions. Although phishing-related accounts accounted for only 0.94% of the total accounts, they appeared in 8.36% of the transaction scenarios, and their interaction probability with normal accounts is significantly higher in large-scale transaction networks. Moreover, NFT phishing scammers often carry out fraud in a collective manner, targeting specific accounts, tend to interact with victims through multiple token standards, have shorter transaction cycles than normal transactions, and involve more multi-party transactions. This study reveals the core behavioral features of NFT phishing scams, providing important references for the detection and prevention of NFT phishing scams in the future.

</details>


### [40] [WhiteLie: A Robust System for Spoofing User Data in Android Platforms](https://arxiv.org/abs/2512.01595)
*Harish Yadav,Vikas Maurya,Abhilash Jindal,Vireshwar Kumar*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Android employs a permission framework that empowers users to either accept or deny sharing their private data (for example, location) with an app. However, many apps tend to crash when they are denied permission, leaving users no choice but to allow access to their data in order to use the app. In this paper, we introduce a comprehensive and robust user data spoofing system, WhiteLie, that can spoof a variety of user data and feed it to target apps. Additionally, it detects privacy-violating behaviours, automatically responding by supplying spoofed data instead of the user's real data, without crashing or disrupting the apps. Unlike prior approaches, WhiteLie requires neither device rooting nor altering the app's binary, making it deployable on stock Android devices. Through experiments on more than 70 popular Android apps, we demonstrate that WhiteLie is able to deceive apps into accepting spoofed data without getting detected. Our evaluation further demonstrates that WhiteLie introduces negligible overhead in terms of battery usage, CPU consumption, and app execution latency. Our findings underscore the feasibility of implementing user-centric privacy-enhancing mechanisms within the existing Android ecosystem.

</details>


### [41] [Towards a Multi-Layer Defence Framework for Securing Near-Real-Time Operations in Open RAN](https://arxiv.org/abs/2512.01596)
*Hamed Alimohammadi,Samara Mayhoub,Sotiris Chatzimiltis,Mohammad Shojafar,Muhammad Nasir Mumtaz Bhutta*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Securing the near-real-time (near-RT) control operations in Open Radio Access Networks (Open RAN) is increasingly critical, yet remains insufficiently addressed, as new runtime threats target the control loop while the system is operational. In this paper, we propose a multi-layer defence framework designed to enhance the security of near-RT RAN Intelligent Controller (RIC) operations. We classify operational-time threats into three categories, message-level, data-level, and control logic-level, and design and implement a dedicated detection and mitigation component for each: a signature-based E2 message inspection module performing structural and semantic validation of signalling exchanges, a telemetry poisoning detector based on temporal anomaly scoring using an LSTM network, and a runtime xApp attestation mechanism based on execution-time hash challenge-response. The framework is evaluated on an O-RAN testbed comprising FlexRIC and a commercial RAN emulator, demonstrating effective detection rates, low latency overheads, and practical integration feasibility. Results indicate that the proposed safeguards can operate within near-RT time constraints while significantly improving protection against runtime attacks, introducing less than 80 ms overhead for a network with 500 User Equipment (UEs). Overall, this work lays the foundation for deployable, layered, and policy-driven runtime security architectures for the near-RT RIC control loop in Open RAN, and provides an extensible framework into which future mitigation policies and threat-specific modules can be integrated.

</details>


### [42] [On the Context-Hiding Property of Shamir-Based Homomorphic Secret Sharing](https://arxiv.org/abs/2512.01604)
*Shuai Feng,Liang Feng Zhang*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Homomorphic secret sharing (HSS) allows multiple input clients to secretly share their private inputs to a function among several servers such that each server can homomorphically compute the function over its share to produce a share of the function's output. In HSS-enabled applications such as secure multi-party computation (MPC), security requires that the output shares leak no more information about the inputs than the function output. Such security is ensured by the context-hiding property of HSS. The typical rerandomization technique achieves context hiding but increases the share size. To address this, we formalize the context-hiding property of HSS for individual functions, examine the context-hiding property of Shamir-based HSS for monomials, and extend the study to polynomials.

</details>


### [43] [Rethinking Cybersecurity Ontology Classification and Evaluation: Towards a Credibility-Centered Framework](https://arxiv.org/abs/2512.01651)
*Antoine Leblanc,Jacques Robin,Nourhène Ben Rabah,Zequan Huang,Bénédicte Le Grand*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: This paper analyzes the proliferation of cybersecurity ontologies, arguing that this surge cannot be explained solely by technical shortcomings related to quality, but also by a credibility deficit - a lack of trust, endorsement, and adoption by users. This conclusion is based on our first contribution, which is a state-of-the-art review and categorization of cybersecurity ontologies using the Framework for Ontologies Classification framework. To address this gap, we propose a revised framework for assessing credibility, introducing indicators such as institutional support, academic recognition, day-to-day practitioner validation, and industrial adoption. Based on these new credibility indicators, we construct a classification scheme designed to guide the selection of ontologies that are relevant to specific security needs. We then apply this framework to a concrete use case: the Franco-Luxembourgish research project ANCILE, which illustrates how a credibility-aware evaluation can reshape ontology selection for operational contexts.

</details>


### [44] [Demystifying Feature Engineering in Malware Analysis of API Call Sequences](https://arxiv.org/abs/2512.01666)
*Tianheng Qu,Hongsong Zhu,Limin Sun,Haining Wang,Haiqiang Fei,Zheng He,Zhi Li*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Machine learning (ML) has been widely used to analyze API call sequences in malware analysis, which typically requires the expertise of domain specialists to extract relevant features from raw data. The extracted features play a critical role in malware analysis. Traditional feature extraction is based on human domain knowledge, while there is a trend of using natural language processing (NLP) for automatic feature extraction. This raises a question: how do we effectively select features for malware analysis based on API call sequences? To answer it, this paper presents a comprehensive study of investigating the impact of feature engineering upon malware classification.We first conducted a comparative performance evaluation under three models, Convolutional Neural Network (CNN), Long Short-Term Memory (LSTM), and Transformer, with respect to knowledge-based and NLP-based feature engineering methods. We observed that models with knowledge-based feature engineering inputs generally outperform those using NLP-based across all metrics, especially under smaller sample sizes. Then we analyzed a complete set of data features from API call sequences, our analysis reveals that models often focus on features such as handles and virtual addresses, which vary across executions and are difficult for human analysts to interpret.

</details>


### [45] [AI-Driven Cybersecurity Testbed for Nuclear Infrastructure: Comprehensive Evaluation Using METL Operational Data](https://arxiv.org/abs/2512.01727)
*Benjamin Blakely,Yeni Li,Akshay Dave,Derek Kultgen,Rick Vilim*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Advanced nuclear reactor systems face increasing cybersecurity threats as sophisticated attackers exploit cyber-physical interfaces to manipulate control systems while evading traditional IT security measures. This research presents a comprehensive evaluation of artificial intelligence approaches for cybersecurity protection in nuclear infrastructure, using Argonne National Laboratory's Mechanisms Engineering Test Loop (METL) as an experimental platform. We developed a systematic evaluation framework encompassing four machine learning detection paradigms: Change Point Detection, LSTM-based Anomaly Detection, Dependency Violation analysis, and Autoencoder reconstruction methods. Our comprehensive attack taxonomy includes 15 distinct scenarios targeting reactor control systems, each implemented across five severity tiers to evaluate detection performance under varying attack intensities. The experimental evaluation encompassed 300 rigorous experiments using realistic METL operational data. Change Point Detection emerged as the leading approach with mean AUC performance of 0.785, followed by LSTM Anomaly Detection (0.636), Dependency Violation (0.621), and Autoencoder methods (0.580). Attack detectability varied significantly, with multi-site coordinated attacks proving most detectable (AUC = 0.739) while precision trust decay attacks presented the greatest detection challenge (AUC = 0.592). This work delivers practical performance benchmarks and reference architecture that advance AI-based cybersecurity capabilities for critical nuclear infrastructure, providing essential foundations for operational deployment and enhanced threat response in cyber-physical systems.

</details>


### [46] [A Privacy-Preserving Information-Sharing Protocol for Federated Authentication](https://arxiv.org/abs/2512.01832)
*Francesco Buccafurri,Carmen Licciardi*

Main category: cs.CR

TL;DR: 提出一种保护隐私的联邦认证身份注册和信息共享协议，利用OPRFs和特定域转换在不泄露用户数据的情况下检测欺诈。


<details>
  <summary>Details</summary>
Motivation: 在联邦认证系统中，需检测重复或欺诈性身份注册，同时保护用户隐私和防止跨域关联。

Method: 结合使用Oblivious Pseudorandom Functions (OPRFs)和特定域转换，生成独立的伪匿名标识符，确保输入机密性。引入一个中央机构，用伪匿名标识符维护盲寄存器，记录成功和失败的身份验证，实现全局一致性检查，而不暴露敏感信息或跨域链接用户。

Result: 该协议提供了一个通用且抽象的框架，适用于各种联邦认证系统，实现了强大的隐私保证，并有效防止身份注册期间的欺诈行为。

Conclusion: 

Abstract: This paper presents a privacy-preserving protocol for identity registration and information sharing in federated authentication systems. The goal is to enable Identity Providers (IdPs) to detect duplicate or fraudulent identity enrollments without revealing users personal data or enabling cross-domain correlation. The protocol relies on Oblivious Pseudorandom Functions (OPRFs) combined with domain-specific transformations, ensuring that each IdP generates independent pseudonymous identifiers derived from a shared cryptographic service while maintaining full input confidentiality. A central authority maintains a blind registry that records successful and failed identity verifications using only pseudonymous identifiers, allowing global consistency checks without exposing sensitive information or linking users across domains. The proposed construction provides a general and abstract framework suitable for a wide range of federated authentication systems, achieving strong privacy guarantees while supporting effective fraud-prevention mechanisms during identity registration.

</details>


### [47] [JPEGs Just Got Snipped: Croppable Signatures Against Deepfake Images](https://arxiv.org/abs/2512.01845)
*Pericle Perazzo,Massimiliano Mattei,Giuseppe Anastasi,Marco Avvenuti,Gianluca Dini,Giuseppe Lettieri,Carlo Vallati*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Deepfakes are a type of synthetic media created using artificial intelligence, specifically deep learning algorithms. This technology can for example superimpose faces and voices onto videos, creating hyper-realistic but artificial representations. Deepfakes pose significant risks regarding misinformation and fake news, because they can spread false information by depicting public figures saying or doing things they never did, undermining public trust. In this paper, we propose a method that leverages BLS signatures (Boneh, Lynn, and Shacham 2004) to implement signatures that remain valid after image cropping, but are invalidated in all the other types of manipulation, including deepfake creation. Our approach does not require who crops the image to know the signature private key or to be trusted in general, and it is O(1) in terms of signature size, making it a practical solution for scenarios where images are disseminated through web servers and cropping is the primary transformation. Finally, we adapted the signature scheme for the JPEG standard, and we experimentally tested the size of a signed image.

</details>


### [48] [Behind the Curtain: How Shared Hosting Providers Respond to Vulnerability Notifications](https://arxiv.org/abs/2512.01891)
*Giada Stivala,Rafael Mrowczynski,Maria Hellenthal,Giancarlo Pellegrino*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Large-scale vulnerability notifications (VNs) can help hosting provider organizations (HPOs) identify and remediate security vulnerabilities that attackers can exploit in data breaches or phishing campaigns. Previous VN studies have primarily focused on factors under the control of reporters, such as sender reputation, email formatting, and communication channels. Despite these efforts, remediation rates for vulnerability notifications continue to remain consistently low. This paper presents the first in-depth study of how HPOs process vulnerability notifications internally and what organizational and operational factors influence VN effectiveness. We examine the problem from a different perspective to provide the first detailed understanding of the reasons behind persistently low remediation rates. Instead of manipulating parameters of VN campaigns, we interview hosting providers directly, investigating how they handle vulnerability notifications and what factors may influence VN effectiveness, such as VN awareness and reachability, HPOs' service models, and perceived security risks.
  We conducted semi-structured interviews with 24 HPOs across shared hosting and web development services, representing varied company sizes and operator roles. Our findings reveal practical insights on VN processing and abuse workflows. While some providers remain hard to reach due to complex infrastructures, most report routinely handling VNs. However, limited remediation often stems from strict responsibility boundaries, where web application issues are seen as the customer's domain. Low hosting fees and high volumes of daily compromises further discourage both proactive and reactive measures. Our findings show that HPOs blame negligent website owners, and prior works on website owners confirms they often undervalue their sites or lack security know-how.

</details>


### [49] [Improving Phishing Resilience with AI-Generated Training: Evidence on Prompting, Personalization, and Duration](https://arxiv.org/abs/2512.01893)
*Francesco Greco,Giuseppe Desolda,Cesare Tucci,Andrea Esposito,Antonio Curci,Antonio Piccinno*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Phishing remains a persistent cybersecurity threat; however, developing scalable and effective user training is labor-intensive and challenging to maintain. Generative Artificial Intelligence offers an interesting opportunity, but empirical evidence on its instructional efficacy remains scarce. This paper provides an experimental validation of Large Language Models (LLMs) as autonomous engines for generating phishing resilience training. Across two controlled studies (N=480), we demonstrate that AI-generated content yields significant pre-post learning gains regardless of the specific prompting strategy employed. Study 1 (N=80) compares four prompting techniques, finding that even a straightforward "direct-profile" strategy--simply embedding user traits into the prompt--produces effective training material. Study 2 (N=400) investigates the scalability of this approach by testing personalization and training duration. Results show that complex psychometric personalization offers no measurable advantage over well-designed generic content, while longer training duration provides a modest boost in accuracy. These findings suggest that organizations can leverage LLMs to generate high-quality, effective training at scale without the need for complex user profiling, relying instead on the inherent capabilities of the model.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [50] [Chunking Strategies for Multimodal AI Systems](https://arxiv.org/abs/2512.00185)
*Shashanka B R,Mohith Charan R,Seema Banu F*

Main category: cs.AI

TL;DR: 本文调查并分类了适用于文本、图像、音频、视频和跨模态数据的分块策略，分析经典与现代方法，并探讨了粒度-上下文权衡和多模态对齐等挑战，突出显示了开放问题和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 为研究人员和从业者提供多模态分块策略的技术基础和开发空间，以开发更有效的多模态 AI 系统，推动鲁棒分块管道的创新，适应模态复杂性，提高处理准确性和生成一致性。

Method: 提供了针对每种模态（文本、图像、音频、视频和跨模态数据）的分块策略全面分类和技术分析，考察了固定大小令牌窗口、递归文本分割、以对象为中心的视觉分块、基于静音的音频分段和视频场景检测等经典和现代方法，分析每种方法的基本方法、支持工具、优点和挑战，特别是粒度-上下文权衡和多模态对齐相关问题，并探讨了旨在保持不同数据类型之间的对齐和语义一致性的新兴跨模态分块策略。

Result: 包含比较见解，突出开放问题，如异步信息密度和噪声对齐信号，并确定了未来在自适应、基于学习和特定任务分块方面的研究机会。

Conclusion: 该综述通过提供全面的技术分析和分类法，为更复杂和高效的多模态 AI 系统发展奠定了坚实的基础，并强调了继续探索如自适应和学习型分块技术的重要性，以解决当前方法中的粒度-上下文对齐挑战和开放问题。

Abstract: Our goal is to consolidate the landscape of multimodal chunking strategies, providing researchers and practitioners with a technical foundation and design space for developing more effective and efficient multimodal AI systems. This survey paves the way for innovations in robust chunking pipelines that scale with modality complexity, enhance processing accuracy, and improve generative coherence in real-world applications. This survey provides a comprehensive taxonomy and technical analysis of chunking strategies tailored for each modality: text, images, audio, video, and cross-modal data. We examine classical and modern approaches such as fixed-size token windowing, recursive text splitting, object-centric visual chunking, silence-based audio segmentation, and scene detection in videos. Each approach is analyzed in terms of its underlying methodology, supporting tools (e.g., LangChain, Detectron2, PySceneDetect), benefits, and challenges, particularly those related to granularity-context trade-offs and multimodal alignment. Furthermore, we explore emerging cross-modal chunking strategies that aim to preserve alignment and semantic consistency across disparate data types [4]. We also include comparative insights, highlight open problems such as asynchronous information density and noisy alignment signals, and identify opportunities for future research in adaptive, learning-based, and task-specific chunking.

</details>


### [51] [A Rosetta Stone for AI Benchmarks](https://arxiv.org/abs/2512.00193)
*Anson Ho,Jean-Stanislas Denain,David Atanasov,Samuel Albanie,Rohin Shah*

Main category: cs.AI

TL;DR: 大多数AI基准测试在几年甚至几个月后就失去时效性，导致难以研究AI能力长期趋势。为解决这一问题，作者提出了一种将不同基准测试连接起来的统计框架，将模型能力和基准难度放在单一数值尺度上，形成“罗塞塔石碑”，实现跨能力和跨时间的模型比较，无需假设能力随时间或训练计算量的变化方式。文章展示了该框架的三种应用：1）测量AI进展速度并预测未来能力；2）估算算法效率提升速度，结果高于但基本符合先前研究；3）检测AI进展的突然加速。


<details>
  <summary>Details</summary>
Motivation: 基准测试饱和过快，难以研究长期趋势，因此需要一种连接不同基准的方法。

Method: 提出一种统计框架，将模型能力和基准难度统一到单一尺度上，无需预设能力如何随时间和训练计算量变化。

Result: 展示了三种应用：1）测量和预测AI能力；2）估算算法效率，结果高于先前研究但基本一致；3）检测AI进展的突然加速。

Conclusion: 该框架能有效连接不同基准，为长期研究和分析AI能力提供新视角。

Abstract: Most AI benchmarks saturate within years or even months after they are introduced, making it hard to study long-run trends in AI capabilities. To address this challenge, we build a statistical framework that stitches benchmarks together, putting model capabilities and benchmark difficulties on a single numerical scale. This acts as a "Rosetta Stone", allowing us to compare models across a wide range of abilities and time, even if they are not evaluated on the same benchmarks. Moreover, this works without assuming how capabilities evolve across time or with training compute. We demonstrate three applications of this framework. First, we use it to measure the speed of AI progress over time, and to forecast future AI capabilities. Second, we estimate the rate of improvements in algorithmic efficiency, finding estimates that are higher, but broadly consistent with prior work. Finally, we find that our approach can be used to detect rapid accelerations in AI progress.

</details>


### [52] [Reasoning Under Pressure: How do Training Incentives Influence Chain-of-Thought Monitorability?](https://arxiv.org/abs/2512.00218)
*Matt MacDermott,Qiyao Wei,Rada Djoneva,Francis Rhys Ward*

Main category: cs.AI

TL;DR: 该论文研究了不同训练激励对推理模型可监测性的影响，发现对抗性优化会降低监测器性能，而直接优化可监测性效果不显著。


<details>
  <summary>Details</summary>
Motivation: 研究目标是评估如何通过对推理模型的训练激励，使其自然语言推理过程（CoT）更真实反映内在逻辑，从而提升安全监测效果。

Method: 提出基于监测器对潜变量预测能力的新可监测性度量方法。测试了长度惩罚、KL正则化和对抗性优化等激励的影响。

Result: 在控制准确率时，常用激励（长度惩罚/KL正则）缺乏一致性影响，对抗性优化会损害监测，直接优化可监测性未能可靠改进性能。

Conclusion: 对抗性训练会降低推理过程的可监测性，而常规训练方法对监测的提升存在局限性，需进一步优化方法设计。

Abstract: AI systems that output their reasoning in natural language offer an opportunity for safety -- we can \emph{monitor} their chain of thought (CoT) for undesirable reasoning, such as the pursuit of harmful objectives. However, the extent to which CoT faithfully reflects the underlying reasoning process, and hence the extent to which it can be usefully monitored, may be influenced by certain aspects of training. We investigate how different \emph{training incentives}, applied to a reasoning model, affect its monitorability. We introduce a novel methodology for measuring monitorability according to whether a monitor can predict a key latent variable using the model's reasoning. When controlling for accuracy, we do not find evidence for consistent effects from commonly used incentives (length penalties and KL regularisation), but we find that adversarial optimisation (penalising monitor accuracy) degrades monitor performance, while direct optimisation for monitorability does not reliably lead to improvements. Our code is available at https://github.com/QiyaoWei/reasoning-under-pressure.

</details>


### [53] [Trification: A Comprehensive Tree-based Strategy Planner and Structural Verification for Fact-Checking](https://arxiv.org/abs/2512.00267)
*Anab Maulana Barik,Shou Ziyi,Yang Kaiwen,Yang Qi,Shen Xin*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Technological advancement allows information to be shared in just a single click, which has enabled the rapid spread of false information. This makes automated fact-checking system necessary to ensure the safety and integrity of our online media ecosystem. Previous methods have demonstrated the effectiveness of decomposing the claim into simpler sub-tasks and utilizing LLM-based multi agent system to execute them. However, those models faces two limitations: they often fail to verify every component in the claim and lack of structured framework to logically connect the results of sub-tasks for a final prediction. In this work, we propose a novel automated fact-checking framework called Trification. Our framework begins by generating a comprehensive set of verification actions to ensure complete coverage of the claim. It then structured these actions into a dependency graph to model the logical interaction between actions. Furthermore, the graph can be dynamically modified, allowing the system to adapt its verification strategy. Experimental results on two challenging benchmarks demonstrate that our framework significantly enhances fact-checking accuracy, thereby advancing current state-of-the-art in automated fact-checking system.

</details>


### [54] [ChartPoint: Guiding MLLMs with Grounding Reflection for Chart Reasoning](https://arxiv.org/abs/2512.00305)
*Zhengzhuo Xu,SiNan Du,Yiyan Qi,SiwenLu,Chengjin Xu,Chun Yuan,Jian Guo*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Multimodal Large Language Models (MLLMs) have emerged as powerful tools for chart comprehension. However, they heavily rely on extracted content via OCR, which leads to numerical hallucinations when chart textual annotations are sparse. While existing methods focus on scaling instructions, they fail to address the fundamental challenge, i.e., reasoning with visual perception. In this paper, we identify a critical observation: MLLMs exhibit weak grounding in chart elements and proportional relationships, as evidenced by their inability to localize key positions to match their reasoning. To bridge this gap, we propose PointCoT, which integrates reflective interaction into chain-of-thought reasoning in charts. By prompting MLLMs to generate bounding boxes and re-render charts based on location annotations, we establish connections between textual reasoning steps and visual grounding regions. We further introduce an automated pipeline to construct ChartPoint-SFT-62k, a dataset featuring 19.2K high-quality chart samples with step-by-step CoT, bounding box, and re-rendered visualizations. Leveraging this data, we develop two instruction-tuned models, ChartPointQ2 and ChartPointQ2.5, which outperform state-of-the-art across several chart benchmarks, e.g., +5.04\% on ChartBench.

</details>


### [55] [CogEvo-Edu: Cognitive Evolution Educational Multi-Agent Collaborative System](https://arxiv.org/abs/2512.00331)
*Yefeng Wu,Yuchen Song,Yecheng Zhao,Ling Wu,Shan Wan*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Large language models (LLMs) are increasingly deployed as conversational tutors in STEM education, yet most systems still rely on a single LLM with a static retrieval-augmented generation (RAG) pipeline over course materials. This design struggles in complex domains such as digital signal processing (DSP), where tutors must maintain coherent long-term student models, manage heterogeneous knowledge bases, and adapt teaching strategies over extended interactions. We argue that retrieval, memory, and control should be treated as a coupled cognitive evolution process. We instantiate this view in CogEvo-Edu, a hierarchical educational multi-agent system comprising a Cognitive Perception Layer (CPL), a Knowledge Evolution Layer (KEL), and a Meta-Control Layer (MCL). CPL maintains dual memories and performs confidence-weighted consolidation to build structured, self-correcting student profiles under limited context. KEL assigns each knowledge chunk a spatiotemporal value that drives activation, semantic compression, and forgetting. MCL formulates tutoring as hierarchical sequential decision making, orchestrating specialized agents and jointly adapting CPL/KEL hyperparameters via a dual inner--outer loop. To evaluate CogEvo-Edu, we construct DSP-EduBench, a vertical benchmark for DSP tutoring with heterogeneous resources, simulated student profiles, and long-horizon interaction scripts. Using a three-model LLM-as-a-Judge ensemble, CogEvo-Edu raises the overall score from 5.32 to 9.23 and improves all six indicators over static RAG, simple memory, and a single-agent variant, demonstrating the value of jointly evolving student profiles, knowledge bases, and teaching policies.

</details>


### [56] [Echo-N1: Affective RL Frontier](https://arxiv.org/abs/2512.00344)
*Naifan Zhang,Ruihan Sun,Ruixi Su,Shiqi Ma,Shiya Zhang,Xianna Weng,Xiaofan Zhang,Yuhan Zhan,Yuyang Xu,Zhaohan Chen,Zhengyuan Pan,Ziyi Song*

Main category: cs.AI

TL;DR: 提出RL可优化主观对话，构建首个个性化及情感智能评估框架。


<details>
  <summary>Details</summary>
Motivation: RL长期聚焦机器擅长领域，忽略主观对话这一人类智能核心。

Method: 1. 框架动态推断用户性格并优化模型行为；2. 发布动态情感智能评估套件。

Result: Echo-N1远超其基础版本，性能优于Doubao 1.5 Character，验证RL在主观对话的有效性。

Conclusion: RL可突破非可验证场景限制，成为优化人性化对话的新前沿。

Abstract: The LLM field has spent a year perfecting RL for tasks machines already excel at, math, code, and deterministic reasoning, while completely sidestepping the domain that actually defines human intelligence: subjective, emotionally grounded, personality sensitive conversation. This space has often been regarded as inherently subjective and challenging to formalize, making it appear unsuitable for conventional RL pipelines. We show that it is not only possible and it is a solvable and transformative RL problem. We propose the first framework that infers user personality on the fly and optimizes model behavior toward personalized conversational preferences. Contrary to the widespread belief that RL collapses in non-verifiable settings, our method produces consistent, robust, and dramatic improvements in humanlike interaction quality. We also introduce the first dynamic emotional intelligence evaluation suite to quantify these gains. Our model, which is introduced as Echo-N1, behaves far above its base version and outperforming the proprietary Doubao 1.5 Character. This work establishes a new frontier for RL: optimizing models for the deeply subjective, deeply human dimensions of conversation.

</details>


### [57] [GreenPlanner: Practical Floorplan Layout Generation via an Energy-Aware and Function-Feasible Generative Framework](https://arxiv.org/abs/2512.00406)
*Pengyu Zeng,Yuqin Dai,Jun Yin,Jing Zhong,Ziyang Han,Chaoyang Shi,ZhanXiang Jin,Maowei Jiang,Yuxing Han,Shuai Lu*

Main category: cs.AI

TL;DR: GreenPlanner 是一个结合生成与评估的建筑设计框架，能同时处理功能性和能源合规性约束。


<details>
  <summary>Details</summary>
Motivation: 传统建筑生成方法缺乏自动约束评估能力，导致布局虽视觉合理但常违反核心设计规范且效率低下。

Method: 提出一个四阶段框架：1) 构建带标签的设计可行性数据集学习约束先验；2) 开发快速实用设计评估器(PDE)；3) 生成PDE筛选的合规数据集(GreenPD)；4) 基于PDE反馈训练生成模型(GreenFlow)。

Result: 评估速度提升10万倍（>99%准确率），设计效率比专业建筑师高87%，生成结果100%合规。

Conclusion: 通过联合优化评估与生成都显著提升建筑设计效率及合规性。未来可能应用到更多建筑和工程领域。

Abstract: Building design directly affects human well-being and carbon emissions, yet generating spatial-functional and energy-compliant floorplans remains manual, costly, and non-scalable. Existing methods produce visually plausible layouts but frequently violate key constraints, yielding invalid results due to the absence of automated evaluation. We present GreenPlanner, an energy- and functionality-aware generative framework that unifies design evaluation and generation. It consists of a labeled Design Feasibility Dataset for learning constraint priors; a fast Practical Design Evaluator (PDE) for predicting energy performance and spatial-functional validity; a Green Plan Dataset (GreenPD) derived from PDE-guided filtering to pair user requirements with regulation-compliant layouts; and a GreenFlow generator trained on GreenPD with PDE feedback for controllable, regulation-aware generation. Experiments show that GreenPlanner accelerates evaluation by over $10^{5}\times$ with $>$99% accuracy, eliminates invalid samples, and boosts design efficiency by 87% over professional architects.

</details>


### [58] [Mind the data gap: Missingness Still Shapes Large Language Model Prognoses](https://arxiv.org/abs/2512.00479)
*Yuta Kobayashi,Vincent Jeanselme,Shalmali Joshi*

Main category: cs.AI

TL;DR: 大型语言模型中，数据缺失的模式对零次学习性能有显著影响，显式包含缺失指标对性能的影响是不一致的。建议大型模型与小型模型在应对缺失数据时应有不同的策略。


<details>
  <summary>Details</summary>
Motivation: 当前文献对数据缺失的不确定性广泛讨论，但其对大型语言模型的影响还未被研究。

Method: 基于哥伦比亚大学和MIMIC-IV医疗中心的数据，开展一系列实验验证缺失数据模式的影响，通过提示显式包含缺失指标分析。

Result: 数据缺失模式显著影响零次学习性能，显式包含缺失指标对LLMs的影响不一致；大型模型受益，而小型模型受影响较大。

Conclusion: 需要对缺失性的影响进行更透明的规划和系统性评估，特别是在下游性能的表现上。

Abstract: Data collection often reflects human decisions. In healthcare, for instance, a referral for a diagnostic test is influenced by the patient's health, their preferences, available resources, and the practitioner's recommendations. Despite the extensive literature on the informativeness of missingness, its implications on the performance of Large Language Models (LLMs) have not been studied. Through a series of experiments on data from Columbia University Medical Center, a large urban academic medical center, and MIMIC-IV, we demonstrate that patterns of missingness significantly impact zero-shot predictive performance. Notably, the explicit inclusion of missingness indicators at prompting benefits some while hurting other LLMs' zero-shot predictive performance and calibration, suggesting an inconsistent impact. The proposed aggregated analysis and theoretical insights suggest that larger models benefit from these interventions, while smaller models can be negatively impacted. The LLM paradigm risks obscuring the impact of missingness, often neglected even in conventional ML, even further. We conclude that there is a need for more transparent accounting and systematic evaluation of the impact of representing (informative) missingness on downstream performance.

</details>


### [59] [Clinical-R1: Empowering Large Language Models for Faithful and Comprehensive Reasoning with Clinical Objective Relative Policy Optimization](https://arxiv.org/abs/2512.00601)
*Boyang Gu,Hongjian Zhou,Bradley Max Segal,Jinge Wu,Zeyu Cao,Hantao Zhong,Lei Clifton,Fenglin Liu,David A. Clifton*

Main category: cs.AI

TL;DR: 该论文提出了CRPO，一种可扩展、多目标、可验证的强化学习方法，用于在医学领域中对大型语言模型进行后训练，优化准确性、忠实性和全面性。实验显示，相较于标准GRPO方法，CRPO在真实性和完整性方面有显著提升，同时保持良好的准确性。


<details>
  <summary>Details</summary>
Motivation: 当前的后训练方法（如GRPO）主要奖励正确性，而医学领域需要多维度目标，包括推理的忠实性和全面性。

Method: CRPO结合基于规则和可验证的奖励信号，共同优化准确性、忠实性和全面性，无需依赖人工标注。

Result: 在三项基准测试中，CRPO显著提高了真实性和完整性，同时保持准确性增强。

Conclusion: 该框架提供了一种可扩展的途径，以将大型语言模型的推理与临床目标对齐，为医疗领域带来更安全和更具协作性的人工智能系统，突出了多目标可验证强化学习方法在医学领域后训练扩展方面的潜力。

Abstract: Recent advances in large language models (LLMs) have shown strong reasoning capabilities through large-scale pretraining and post-training reinforcement learning, demonstrated by DeepSeek-R1. However, current post-training methods, such as Grouped Relative Policy Optimization (GRPO), mainly reward correctness, which is not aligned with the multi-dimensional objectives required in high-stakes fields such as medicine, where reasoning must also be faithful and comprehensive. We introduce Clinical-Objective Relative Policy Optimization (CRPO), a scalable, multi-objective, verifiable reinforcement learning method designed to align LLM post-training with clinical reasoning principles. CRPO integrates rule-based and verifiable reward signals that jointly optimize accuracy, faithfulness, and comprehensiveness without relying on human annotation. To demonstrate its effectiveness, we train Clinical-R1-3B, a 3B-parameter model for clinical reasoning. The experiments on three benchmarks demonstrate that our CRPO substantially improves reasoning on truthfulness and completeness over standard GRPO while maintaining comfortable accuracy enhancements. This framework provides a scalable pathway to align LLM reasoning with clinical objectives, enabling safer and more collaborative AI systems for healthcare while also highlighting the potential of multi-objective, verifiable RL methods in post-training scaling of LLMs for medical domains.

</details>


### [60] [EDIT: Early Diffusion Inference Termination for dLLMs Based on Dynamics of Training Gradients](https://arxiv.org/abs/2512.00670)
*He-Yen Hsieh,Hong Wang,H. T. Kung*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Diffusion-based large language models (dLLMs) refine token generations through iterative denoising, but answers often stabilize before all steps complete. We propose EDIT (Early Diffusion Inference Termination), an inference-time criterion that adaptively stops denoising once sufficient reasoning stability relative to training-time reasoning is detected. EDIT monitors the alignment between token activations and a reasoning map derived from AdamW-aggregated LoRA updates captured during supervised fine-tuning (SFT). During training, optimization dynamics generate rich metadata about parameter importance that in prior methods is typically discarded upon model release. We preserve this information as a compact representation of learned reasoning pathways. During inference, alignment scores are converted to a distribution over the tokens already unmasked at the current denoising step, and convergence is detected when KL divergence between consecutive steps falls below a threshold on the matched unmasked (visible) tokens. Across reasoning benchmarks, EDIT reduces diffusion steps by 11.8% to 68.3% while preserving or improving accuracy in most settings, with approximately 0.02% storage overhead (about 1.5-2 MB for all QKV modules across 32 blocks in an 8 GB model). By utilizing training-gradient dynamics, our work opens a new research direction for reducing dLLM inference time and cost.

</details>


### [61] [Model of human cognition](https://arxiv.org/abs/2512.00683)
*Wu Yonggang*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The development of large language models (LLMs) is limited by a lack of explainability, the absence of a unifying theory, and prohibitive operational costs. We propose a neuro-theoretical framework for the emergence of intelligence in systems that is both functionally robust and biologically plausible. The model provides theoretical insights into cognitive processes such as decision-making and problem solving, and a computationally efficient approach for the creation of explainable and generalizable artificial intelligence.

</details>


### [62] [SpeContext: Enabling Efficient Long-context Reasoning with Speculative Context Sparsity in LLMs](https://arxiv.org/abs/2512.00722)
*Jiaming Xu,Jiayi Pan,Hanzhen Wang,Yongkang Zhou,Jiancai Ye,Yu Wang,Guohao Dai*

Main category: cs.AI

TL;DR: 在信息论视角下，提出利用蒸馏语言模型作为检索算法的新范式。


<details>
  <summary>Details</summary>
Motivation: 检索算法的目标是与LLM对齐，类似于LLM的知识蒸馏目标。

Method: 提出SpeContext，通过三个层次减轻检索的负担：1) 利用DLM头级注意力权重的轻量级检索头；2) 系统级异步预取数据流；3) 编译级的自适应内存管理。

Result: 相比Huggingface框架，SpeContext在云端资源受限环境中提升了24.89倍吞吐量，在边缘资源受限环境中提升了10.06倍速度，精度损失可忽略不计。

Conclusion: SpeContext推进了精度与吞吐量的帕累托边界。

Abstract: In this paper, we point out that the objective of the retrieval algorithms is to align with the LLM, which is similar to the objective of knowledge distillation in LLMs. We analyze the similarity in information focus between the distilled language model(DLM) and the original LLM from the perspective of information theory, and thus propose a novel paradigm that leverages a DLM as the retrieval algorithm. Based on the insight, we present SpeContext, an algorithm and system co-design for long-context reasoning. (1) At the algorithm level, SpeContext proposes lightweight retrieval head based on the head-level attention weights of DLM, achieving > 90% parameters reduction by pruning the redundancy. (2) At the system level, SpeContext designs an asynchronous prefetch dataflow via the elastic loading strategy, effectively overlapping KV cache retrieval with the LLM computation. (3) At the compilation level, SpeContext constructs the theoretical memory model and implements an adaptive memory management system to achieve acceleration by maximizing GPU memory utilization. We deploy and evaluate SpeContext in two resourceconstrained environments, cloud and edge. Extensive experiments show that, compared with the Huggingface framework, SpeContext achieves up to 24.89x throughput improvement in cloud and 10.06x speedup in edge with negligible accuracy loss, pushing the Pareto frontier of accuracy and throughput.

</details>


### [63] [Probing the "Psyche'' of Large Reasoning Models: Understanding Through a Human Lens](https://arxiv.org/abs/2512.00729)
*Yuxiang Chen,Zuohan Wu,Ziwei Wang,Xiangning Yu,Xujia Li,Linyi Yang,Mengyue Yang,Jun Wang,Lei Chen*

Main category: cs.AI

TL;DR: 本文引入综合分类法，从跨学科的角度分析大型推理模型（LRMs）的推理步骤，并提出改进训练和推理策略的方法。


<details>
  <summary>Details</summary>
Motivation: LRMs在解决复杂任务方面表现出色，研究者希望理解其类人行为的推理过程。

Method: 提出包括五个类别和十七个子类别的分类法，并应用该分类法分析了当前LRMs，生成了包含277,534个原子推理步骤标记的数据集。还提出了CAPO自动化标注框架。

Result: CAPO在人类专家一致性方面表现优于基线方法，揭示了当前LRMs中的“自我监测”通常较为简单，需要多步骤深层次的推理。

Conclusion: 该分类法、CAPO和相关见解提供了一条系统性、可扩展的路径，帮助理解和推进LRM推理能力。

Abstract: Large reasoning models (LRMs) have garnered significant attention from researchers owing to their exceptional capability in addressing complex tasks. Motivated by the observed human-like behaviors in their reasoning processes, this paper introduces a comprehensive taxonomy to characterize atomic reasoning steps and probe the ``psyche'' of LRM intelligence. Specifically, it comprises five groups and seventeen categories derived from human mental processes, thereby grounding the understanding of LRMs in an interdisciplinary perspective. The taxonomy is then applied for an in-depth understanding of current LRMs, resulting in a distinct labeled dataset that comprises 277,534 atomic reasoning steps. Using this resource, we analyze contemporary LRMs and distill several actionable takeaways for improving training and post-training of reasoning models. Notably, our analysis reveals that prevailing post-answer ``double-checks'' (self-monitoring evaluations) are largely superficial and rarely yield substantive revisions. Thus, incentivizing comprehensive multi-step reflection, rather than simple self-monitoring, may offer a more effective path forward. To complement the taxonomy, an automatic annotation framework, named CAPO, is proposed to leverage large language models (LLMs) for generating the taxonomy-based annotations. Experimental results demonstrate that CAPO achieves higher consistency with human experts compared to baselines, facilitating a scalable and comprehensive analysis of LRMs from a human cognitive perspective. Together, the taxonomy, CAPO, and the derived insights provide a principled, scalable path toward understanding and advancing LRM reasoning.

</details>


### [64] [MPR-GUI: Benchmarking and Enhancing Multilingual Perception and Reasoning in GUI Agents](https://arxiv.org/abs/2512.00756)
*Ruihan Chen,Qiming Li,Xiaocheng Feng,Xiaoliang Yang,Weihong Zhong,Yuxuan Gu,Zekun Zhou,Bing Qin*

Main category: cs.AI

TL;DR: 提出了MPR-GUI-Bench，一个多语言细粒度GUI基准测试，并设计了GUI-XLI方法，通过在感知和推理相关的层面对隐藏状态进行干预，缩小英语和其他语言之间的差距。


<details>
  <summary>Details</summary>
Motivation: 当前LVLMs在多语言GUI环境下的表现缺乏关注，且对GUI任务的细粒度分析不足，影响了模型的全球应用。

Method: 提出MPR-GUI-Bench进行多语言细粒度评估。为解决性能差距，提出GUI-XLI，一种通过在感知和推理相关层面对LVLMs隐藏状态进行干预的方法，缩小英语和其他语言在潜在空间中的差异。

Result: GUI-XLI方法平均提升了6.5%的多语言感知和推理能力。

Conclusion: GUI-XLI方法有效提高了LVLMs在多语言GUI环境中的性能，改善了全球应用前景。

Abstract: With the advancement of computational resources, Large Vision-Language Models (LVLMs) exhibit impressive Perception and Reasoning (P&R) performance on Graphical User Interface (GUI) tasks. However, although they demonstrate strong P&R capabilities in English GUI scenarios, their performance in multilingual settings has received little attention, which limits their global applications. Moreover, existing studies on GUI tasks lack fine-grained analyses, including widget functions and elements' spatial relationships, which are fundamental for more targeted improvements. To tackle these issues, we propose MPR-GUI-Bench, a Multilingual fine-grained Perception and Reasoning GUI Benchmark to evaluate GUI agents' P&R capabilities. Evaluation results demonstrate that LVLMs exhibit significantly worse P&R performance in non-English languages than in English. To address these gaps, we propose GUI-XLI, a GUI Cross-Lingual Intervention method that applies interventions to the hidden states at P&R capability-related layers to mitigate the gaps between English and other languages, building on previous research showing that the hidden states of different language inputs exhibit significant differences in the latent space. Experimental results indicate that our method improves GUI agents' multilingual P&R capability by 6.5% on average.

</details>


### [65] [BioPro: On Difference-Aware Gender Fairness for Vision-Language Models](https://arxiv.org/abs/2512.00807)
*Yujie Lin,Jiayao Ma,Qingguo Hu,Derek F. Wong,Jinsong Su*

Main category: cs.AI

TL;DR: 该论文提出了BioPro框架，通过选择性去偏技术解决视觉-语言模型(VLMs)中的性别偏见问题，并在中立场景保持性别相关性。


<details>
  <summary>Details</summary>
Motivation: 解决传统公平性干预方法（差异忽略）的局限性：无法区分需要中立性的场景和需保留群体差异性的场景。

Method: 提出BioPro——一种训练无关的差异感知去偏框架，通过反事实嵌入识别性别变化子空间并选择性去除性别关联信息。

Result: 实验显示BioPro在中立场景降低性别偏见的同时，能在明确场景保持性别可信度；且能泛化到场景亮度等连续偏置变量。

Conclusion: BioPro为VLMs中的选择性公平性提供新方向，并具有更广的适用性。

Abstract: Vision-Language Models (VLMs) inherit significant social biases from their training data, notably in gender representation. Current fairness interventions often adopt a difference-unaware perspective that enforces uniform treatment across demographic groups. These approaches, however, fail to distinguish between contexts where neutrality is required and those where group-specific attributes are legitimate and must be preserved. Building upon recent advances in difference-aware fairness for text-only models, we extend this concept to the multimodal domain and formalize the problem of difference-aware gender fairness for image captioning and text-to-image generation. We advocate for selective debiasing, which aims to mitigate unwanted bias in neutral contexts while preserving valid distinctions in explicit ones. To achieve this, we propose BioPro (Bias Orthogonal Projection), an entirely training-free framework. BioPro identifies a low-dimensional gender-variation subspace through counterfactual embeddings and applies projection to selectively neutralize gender-related information. Experiments show that BioPro effectively reduces gender bias in neutral cases while maintaining gender faithfulness in explicit ones, thus providing a promising direction toward achieving selective fairness in VLMs. Beyond gender bias, we further demonstrate that BioPro can effectively generalize to continuous bias variables, such as scene brightness, highlighting its broader applicability.

</details>


### [66] [Med-CMR: A Fine-Grained Benchmark Integrating Visual Evidence and Clinical Logic for Medical Complex Multimodal Reasoning](https://arxiv.org/abs/2512.00818)
*Haozhen Gong,Xiaozhong Ji,Yuansen Liu,Wenbin Wu,Xiaoxiao Yan,Jingjing Liu,Kai Wu,Jiazhen Pan,Bailiang Jian,Jiangning Zhang,Xiaobin Hu,Hongwei Bran Li*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: MLLMs MLLMs are beginning to appear in clinical workflows, but their ability to perform complex medical reasoning remains unclear. We present Med-CMR, a fine-grained Medical Complex Multimodal Reasoning benchmark. Med-CMR distinguishes from existing counterparts by three core features: 1) Systematic capability decomposition, splitting medical multimodal reasoning into fine-grained visual understanding and multi-step reasoning to enable targeted evaluation; 2) Challenging task design, with visual understanding across three key dimensions (small-object detection, fine-detail discrimination, spatial understanding) and reasoning covering four clinically relevant scenarios (temporal prediction, causal reasoning, long-tail generalization, multi-source integration); 3) Broad, high-quality data coverage, comprising 20,653 Visual Question Answering (VQA) pairs spanning 11 organ systems and 12 imaging modalities, validated via a rigorous two-stage (human expert + model-assisted) review to ensure clinical authenticity. We evaluate 18 state-of-the-art MLLMs with Med-CMR, revealing GPT-5 as the top-performing commercial model: 57.81 accuracy on multiple-choice questions (MCQs) and a 48.70 open-ended score, outperforming Gemini 2.5 Pro (49.87 MCQ accuracy, 45.98 open-ended score) and leading open-source model Qwen3-VL-235B-A22B (49.34 MCQ accuracy, 42.62 open-ended score). However, specialized medical MLLMs do not reliably outperform strong general models, and long-tail generalization emerges as the dominant failure mode. Med-CMR thus provides a stress test for visual-reasoning integration and rare-case robustness in medical MLLMs, and a rigorous yardstick for future clinical systems.

</details>


### [67] [SemAgent: Semantic-Driven Agentic AI Empowered Trajectory Prediction in Vehicular Networks](https://arxiv.org/abs/2512.00834)
*Lin Zhu,Kezhi Wang,Luping Xiang,Kun Yang*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Efficient information exchange and reliable contextual reasoning are essential for vehicle-to-everything (V2X) networks. Conventional communication schemes often incur significant transmission overhead and latency, while existing trajectory prediction models generally lack environmental perception and logical inference capabilities. This paper presents a trajectory prediction framework that integrates semantic communication with Agentic AI to enhance predictive performance in vehicular environments. In vehicle-to-infrastructure (V2I) communication, a feature-extraction agent at the Roadside Unit (RSU) derives compact representations from historical vehicle trajectories, followed by semantic reasoning performed by a semantic-analysis agent. The RSU then transmits both feature representations and semantic insights to the target vehicle via semantic communication, enabling the vehicle to predict future trajectories by combining received semantics with its own historical data. In vehicle-to-vehicle (V2V) communication, each vehicle performs local feature extraction and semantic analysis while receiving predicted trajectories from neighboring vehicles, and jointly utilizes this information for its own trajectory prediction. Extensive experiments across diverse communication conditions demonstrate that the proposed method significantly outperforms baseline schemes, achieving up to a 47.5% improvement in prediction accuracy under low signal-to-noise ratio (SNR) conditions.

</details>


### [68] [Assessing model error in counterfactual worlds](https://arxiv.org/abs/2512.00836)
*Emily Howerton,Justin Lessler*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Counterfactual scenario modeling exercises that ask "what would happen if?" are one of the most common ways we plan for the future. Despite their ubiquity in planning and decision making, scenario projections are rarely evaluated retrospectively. Differences between projections and observations come from two sources: scenario deviation and model miscalibration. We argue the latter is most important for assessing the value of models in decision making, but requires estimating model error in counterfactual worlds. Here we present and contrast three approaches for estimating this error, and demonstrate the benefits and limitations of each in a simulation experiment. We provide recommendations for the estimation of counterfactual error and discuss the components of scenario design that are required to make scenario projections evaluable.

</details>


### [69] [One Swallow Does Not Make a Summer: Understanding Semantic Structures in Embedding Spaces](https://arxiv.org/abs/2512.00852)
*Yandong Sun,Qiang Huang,Ziwei Xu,Yiqun Sun,Yixuan Tang,Anthony K. H. Tung*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Embedding spaces are fundamental to modern AI, translating raw data into high-dimensional vectors that encode rich semantic relationships. Yet, their internal structures remain opaque, with existing approaches often sacrificing semantic coherence for structural regularity or incurring high computational overhead to improve interpretability. To address these challenges, we introduce the Semantic Field Subspace (SFS), a geometry-preserving, context-aware representation that captures local semantic neighborhoods within the embedding space. We also propose SAFARI (SemAntic Field subspAce deteRmInation), an unsupervised, modality-agnostic algorithm that uncovers hierarchical semantic structures using a novel metric called Semantic Shift, which quantifies how semantics evolve as SFSes evolve. To ensure scalability, we develop an efficient approximation of Semantic Shift that replaces costly SVD computations, achieving a 15~30x speedup with average errors below 0.01. Extensive evaluations across six real-world text and image datasets show that SFSes outperform standard classifiers not only in classification but also in nuanced tasks such as political bias detection, while SAFARI consistently reveals interpretable and generalizable semantic hierarchies. This work presents a unified framework for structuring, analyzing, and scaling semantic understanding in embedding spaces.

</details>


### [70] [Hybrid-DMKG: A Hybrid Reasoning Framework over Dynamic Multimodal Knowledge Graphs for Multimodal Multihop QA with Knowledge Editing](https://arxiv.org/abs/2512.00881)
*Li Yuan,Qingfei Huang,Bingshan Zhu,Yi Cai,Qingbao Huang,Changmeng Zheng,Zikun Deng,Tao Wang*

Main category: cs.AI

TL;DR: MMQAKE：多模态多跳问答知识编辑的新基准及其混合推理框架Hybrid-DMKG，通过动态知识图和双路径推理显著提升准确性。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决现有MKE基准缺乏对中间推理质量和视觉重述输入鲁棒性的评估问题，旨在提升多模态知识编辑的综合能力。

Method: 引入MMQAKE基准并构建Hybrid-DMKG框架，该框架结合语言模型分解问题、多模态检索模型定位事实、双路径混合推理（关系链接/RAG推理）与动态知识图技术。

Result: 实验显示Hybrid-DMKG在MMQAKE上优于现有MKE方法，准确性和对知识更新的鲁棒性显著提升。

Conclusion: Hybrid-DMKG证明了双路径推理和动态知识图在多模态知识编辑中的有效性，并为未来研究提供了新的评估基准。

Abstract: Multimodal Knowledge Editing (MKE) extends traditional knowledge editing to settings involving both textual and visual modalities. However, existing MKE benchmarks primarily assess final answer correctness while neglecting the quality of intermediate reasoning and robustness to visually rephrased inputs. To address this limitation, we introduce MMQAKE, the first benchmark for multimodal multihop question answering with knowledge editing. MMQAKE evaluates (1) a model's ability to reason over 2-5-hop factual chains that span both text and images, including performance at each intermediate step, and (2) robustness to visually rephrased inputs in multihop questions. Our evaluation shows that current MKE methods often struggle to consistently update and reason over multimodal reasoning chains after knowledge edits. To overcome these challenges, we propose Hybrid-DMKG, a hybrid reasoning framework built on a dynamic multimodal knowledge graph (DMKG) to enable accurate multihop reasoning over updated multimodal knowledge. Hybrid-DMKG first uses a large language model to decompose multimodal multihop questions into sequential sub-questions, then applies a multimodal retrieval model to locate updated facts by jointly encoding each sub-question with candidate entities and their associated images. For answer inference, a hybrid reasoning module operates over the DMKG via two parallel paths: (1) relation linking prediction, and (2) RAG reasoning with large vision-language models. A decision module aggregates evidence from both paths to select the most credible answer. Experimental results on MMQAKE show that Hybrid-DMKG significantly outperforms existing MKE approaches, achieving higher accuracy and improved robustness to knowledge updates.

</details>


### [71] [Minimal neuron ablation triggers catastrophic collapse in the language core of Large Vision-Language Models](https://arxiv.org/abs/2512.00918)
*Cen Lu,Yung-Chen Tang,Andrea Cavallaro*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Large Vision-Language Models (LVLMs) have shown impressive multimodal understanding capabilities, yet their robustness is poorly understood. In this paper, we investigate the structural vulnerabilities of LVLMs to identify any critical neurons whose removal triggers catastrophic collapse. In this context, we propose CAN, a method to detect Consistently Activated Neurons and to locate critical neurons by progressive masking. Experiments on LLaVA-1.5-7b-hf and InstructBLIP-Vicuna-7b reveal that masking only a tiny portion of the language model's feed-forward networks (just as few as four neurons in extreme cases) suffices to trigger catastrophic collapse. Notably, critical neurons are predominantly localized in the language model rather than in the vision components, and the down-projection layer is a particularly vulnerable structure. We also observe a consistent two-stage collapse pattern: initial expressive degradation followed by sudden, complete collapse. Our findings provide important insights for safety research in LVLMs.

</details>


### [72] [Integrating Causal Foundation Model in Prescriptive Maintenance Framework for Optimizing Production Line OEE](https://arxiv.org/abs/2512.00969)
*Felix Saretzky,Lucas Andersen,Thomas Engel,Fazel Ansari*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The transition to prescriptive maintenance in manufacturing is critically constrained by a dependence on predictive models. These models tend to rely on spurious correlations rather than identifying the true causal drivers of failures, often leading to costly misdiagnoses and ineffective interventions. This fundamental limitation results in a key-challenge: while we can predict that a failure may occur, we lack a systematic method to understand why a failure occurs, thereby providing the basis for identifying the most effective intervention. This paper proposes a model based on causal machine learning to bridge this gap. Our objective is to move beyond diagnosis to active prescription by simulating and evaluating potential fixes toward optimizing KPIs such as Overall Equipment Effectiveness (OEE). For this purpose a pre-trained causal foundation model is used as a "what-if" model to estimate the effects of potential fixes. By measuring the causal effect of each intervention on system-level KPIs, it provides a data-driven ranking of actions to recommend at the production line. This process not only identifies root causes but also quantifies their operational impact. The model is evaluated using semi-synthetic manufacturing data and compared with a baseline machine learning model. This paper sets the technical basis for a robust prescriptive maintenance framework, allowing engineers to test potential solutions in a causal environment to make more effective operational decisions and reduce costly downtimes.

</details>


### [73] [IndiMathBench: Autoformalizing Mathematical Reasoning Problems with a Human Touch](https://arxiv.org/abs/2512.00997)
*Param Biyani,Shashank Kirtania,Yasharth Bajpai,Sumit Gulwani,Ashish Tiwari*

Main category: cs.AI

TL;DR: 本文介绍了IndiMathBench，一个用于评估数学定理证明的基准测试，采用AI与人工协作的方式构建，包含大量来自印度数学竞赛的形式化问题。


<details>
  <summary>Details</summary>
Motivation: 数学定理的自动形式化与证明是一项关键挑战，现有基准在语义正确性和多样性上存在不足，需要更严格的测试工具。

Method: 通过类别为基础的检索、迭代编译器反馈、多模型集成生成候选形式化问题，利用交互式仪表盘和自动化质量摘要帮助专家验证。

Result: 评估显示自动形式化仍然存在显著挑战，语法有效性和语义正确性之间差距大，定理证明的成功率较低，即使在迭代优化后仍不理想。

Conclusion: IndiMathBench为数学推理提供了一个具有挑战性的测试平台，揭示了现有模型在复杂形式化任务中的不足。

Abstract: We introduce IndiMathBench, a human-verified benchmark designed to evaluate mathematical theorem proving, curated using an AI-powered human-assisted pipeline for formalizing natural language problems in Lean. IndiMathBench is composed of 312 formal Lean 4 theorems paired with their corresponding informal problem statements, sourced from Indian Mathematics Olympiads. Through category-based retrieval, iterative compiler feedback, and multi-model ensembles, our pipeline generates candidate formalizations that experts efficiently validate via an interactive dashboard with automated quality summaries. Evaluation across multiple frontier models demonstrates that autoformalization remains challenging, with substantial gaps between syntactic validity and semantic correctness, while theorem proving success rates remain low even with iterative refinement, demonstrating that \benchmark~presents a challenging testbed for mathematical reasoning. IndiMathBench is available at https://github.com/prmbiy/IndiMathBench.

</details>


### [74] [ChartAnchor: Chart Grounding with Structural-Semantic Fidelity](https://arxiv.org/abs/2512.01017)
*Xinhang Li,Jingbo Zhou,Pengfei Luo,Yixiong Xiao,Tong Xu*

Main category: cs.AI

TL;DR: 提出 ChartAnchor，一个综合性基准，用于评估多模态大型语言模型的结构化图表理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准受限于图表多样性狭窄、任务孤立和不完整的评估框架，无法全面评估模型能力。

Method: 引入 ChartAnchor 基准，包括 8k+ 图表-表格-代码三元组，涵盖30种图表类型，并提出了两个互补任务：图表到代码生成和受控图表到表格重建，结合多层评估框架。

Result: 广泛实验揭示了 MLLMs 在数字精度和代码合成方面的重要局限性，强调了结构化推理的必要性。

Conclusion: ChartAnchor 通过统一符号和数据驱动的接地，为图表接地建立了严格的基础，推动了 MLLMs 在多个领域的应用。

Abstract: Recent advances in multimodal large language models (MLLMs) highlight the need for benchmarks that rigorously evaluate structured chart comprehension.Chart grounding refers to the bidirectional alignment between a chart's visual appearance and the structured semantics. This task requires models to produce a symbolic specification that faithfully captures the chart's visual and structural intent, while also recovering the underlying tabular data with precise values and relationships. Chart grounding directly reflects a model's capabilities in numerical reasoning, multimodal alignment, and structural reconstruction, and has several important applications in real-world scenarios.Existing benchmarks, constrained by narrow chart diversity, isolated tasks, and incomplete evaluation frameworks, fail to holistically assess grounding. To address this, we propose ChartAnchor, a comprehensive benchmark of 8k+ chart-table-code triples spanning 30 chart types drawn from diverse real-world and augmented sources. ChartAnchor introduces two complementary tasks: chart-to-code generation (synthesizing executable code to replicate charts) and controlled chart-to-table reconstruction (extracting exact data with predefined headers), enabling cross-validation of visual and numerical fidelity. A multi-level evaluation framework integrates semantic validation, stylistic analysis, and perceptual metrics to assess both structural and content-level correctness. Extensive experiments on MLLMs reveal critical limitations in numerical precision and code synthesis, emphasizing the need for structured reasoning beyond surface-level perception. By unifying symbolic and data-driven grounding, ChartAnchor establishes a rigorous foundation for chart grounding, offering meaningful insights for advancing MLLMs in scientific, financial, and industrial domains.

</details>


### [75] [Evaluating Legal Reasoning Traces with Legal Issue Tree Rubrics](https://arxiv.org/abs/2512.01020)
*Jinu Lee,Kyoung-Woon On,Simeng Han,Arman Cohan,Julia Hockenmaier*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Evaluating the quality of LLM-generated reasoning traces in expert domains (e.g., law) is essential for ensuring credibility and explainability, yet remains challenging due to the inherent complexity of such reasoning tasks. We introduce LEGIT (LEGal Issue Trees), a novel large-scale (24K instances) expert-level legal reasoning dataset with an emphasis on reasoning trace evaluation. We convert court judgments into hierarchical trees of opposing parties' arguments and the court's conclusions, which serve as rubrics for evaluating the issue coverage and correctness of the reasoning traces. We verify the reliability of these rubrics via human expert annotations and comparison with coarse, less informative rubrics. Using the LEGIT dataset, we show that (1) LLMs' legal reasoning ability is seriously affected by both legal issue coverage and correctness, and that (2) retrieval-augmented generation (RAG) and RL with rubrics bring complementary benefits for legal reasoning abilities, where RAG improves overall reasoning capability, whereas RL improves correctness albeit with reduced coverage.

</details>


### [76] [Med-CRAFT: Automated Construction of Interpretable and Multi-Hop Video Workloads via Knowledge Graph Traversal](https://arxiv.org/abs/2512.01045)
*Shenxi Liu,Kan Li,Mingyang Zhao,Yuhang Tian,Shoujun Zhou,Bin Li*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The scarcity of high-quality, logically annotated video datasets remains a primary bottleneck in advancing Multi-Modal Large Language Models (MLLMs) for the medical domain. Traditional manual annotation is prohibitively expensive and non-scalable, while existing synthetic methods often suffer from stochastic hallucinations and a lack of logical interpretability. To address these challenges, we introduce \textbf{\PipelineName}, a novel neuro-symbolic data engineering framework that formalizes benchmark synthesis as a deterministic graph traversal process. Unlike black-box generative approaches, Med-CRAFT extracts structured visual primitives (e.g., surgical instruments, anatomical boundaries) from raw video streams and instantiates them into a dynamic Spatiotemporal Knowledge Graph. By anchoring query generation to valid paths within this graph, we enforce a rigorous Chain-of-Thought (CoT) provenance for every synthesized benchmark item. We instantiate this pipeline to produce M3-Med-Auto, a large-scale medical video reasoning benchmark exhibiting fine-grained temporal selectivity and multi-hop logical complexity. Comprehensive evaluations demonstrate that our automated pipeline generates query workloads with complexity comparable to expert-curated datasets. Furthermore, a logic alignment analysis reveals a high correlation between the prescribed graph topology and the reasoning steps of state-of-the-art MLLMs, validating the system's capability to encode verifiable logic into visual-linguistic benchmarks. This work paves the way for scalable, low-cost construction of robust evaluation protocols in critical domains.

</details>


### [77] [Shielded Controller Units for RL with Operational Constraints Applied to Remote Microgrids](https://arxiv.org/abs/2512.01046)
*Hadi Nekoei,Alexandre Blondin Massé,Rachid Hassani,Sarath Chandar,Vincent Mai*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Reinforcement learning (RL) is a powerful framework for optimizing decision-making in complex systems under uncertainty, an essential challenge in real-world settings, particularly in the context of the energy transition. A representative example is remote microgrids that supply power to communities disconnected from the main grid. Enabling the energy transition in such systems requires coordinated control of renewable sources like wind turbines, alongside fuel generators and batteries, to meet demand while minimizing fuel consumption and battery degradation under exogenous and intermittent load and wind conditions. These systems must often conform to extensive regulations and complex operational constraints. To ensure that RL agents respect these constraints, it is crucial to provide interpretable guarantees. In this paper, we introduce Shielded Controller Units (SCUs), a systematic and interpretable approach that leverages prior knowledge of system dynamics to ensure constraint satisfaction. Our shield synthesis methodology, designed for real-world deployment, decomposes the environment into a hierarchical structure where each SCU explicitly manages a subset of constraints. We demonstrate the effectiveness of SCUs on a remote microgrid optimization task with strict operational requirements. The RL agent, equipped with SCUs, achieves a 24% reduction in fuel consumption without increasing battery degradation, outperforming other baselines while satisfying all constraints. We hope SCUs contribute to the safe application of RL to the many decision-making challenges linked to the energy transition.

</details>


### [78] [Automating the Refinement of Reinforcement Learning Specifications](https://arxiv.org/abs/2512.01047)
*Tanmay Ambadkar,Đorđe Žikelić,Abhinav Verma*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Logical specifications have been shown to help reinforcement learning algorithms in achieving complex tasks. However, when a task is under-specified, agents might fail to learn useful policies. In this work, we explore the possibility of improving coarse-grained logical specifications via an exploration-guided strategy. We propose \textsc{AutoSpec}, a framework that searches for a logical specification refinement whose satisfaction implies satisfaction of the original specification, but which provides additional guidance therefore making it easier for reinforcement learning algorithms to learn useful policies. \textsc{AutoSpec} is applicable to reinforcement learning tasks specified via the SpectRL specification logic. We exploit the compositional nature of specifications written in SpectRL, and design four refinement procedures that modify the abstract graph of the specification by either refining its existing edge specifications or by introducing new edge specifications. We prove that all four procedures maintain specification soundness, i.e. any trajectory satisfying the refined specification also satisfies the original. We then show how \textsc{AutoSpec} can be integrated with existing reinforcement learning algorithms for learning policies from logical specifications. Our experiments demonstrate that \textsc{AutoSpec} yields promising improvements in terms of the complexity of control tasks that can be solved, when refined logical specifications produced by \textsc{AutoSpec} are utilized.

</details>


### [79] [SimWorld: An Open-ended Realistic Simulator for Autonomous Agents in Physical and Social Worlds](https://arxiv.org/abs/2512.01078)
*Jiawei Ren,Yan Zhuang,Xiaokang Ye,Lingjun Mao,Xuhong He,Jianzhi Shen,Mrinaal Dogra,Yiming Liang,Ruixuan Zhang,Tianai Yue,Yiqing Yang,Eric Liu,Ryan Wu,Kevin Benavente,Rajiv Mandya Nagaraju,Muhammad Faayez,Xiyan Zhang,Dhruv Vivek Sharma,Xianrui Zhong,Ziqiao Ma,Tianmin Shu,Zhiting Hu,Lianhui Qin*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: While LLM/VLM-powered AI agents have advanced rapidly in math, coding, and computer use, their applications in complex physical and social environments remain challenging. Building agents that can survive and thrive in the real world (for example, by autonomously earning income or running a business) requires massive-scale interaction, reasoning, training, and evaluation across diverse embodied scenarios. However, existing world simulators for such development fall short: they often rely on limited hand-crafted environments, simulate simplified game-like physics and social rules, and lack native support for LLM/VLM agents. We introduce SimWorld, a new simulator built on Unreal Engine 5, designed for developing and evaluating LLM/VLM agents in rich, real-world-like settings. SimWorld offers three core capabilities: (1) realistic, open-ended world simulation, including accurate physical and social dynamics and language-driven procedural environment generation; (2) a rich interface for LLM/VLM agents, with multimodal world inputs and open-vocabulary actions at varying levels of abstraction; and (3) diverse and extensible physical and social reasoning scenarios that are easily customizable by users. We demonstrate SimWorld by deploying frontier LLM agents (e.g., GPT-4o, Gemini-2.5-Flash, Claude-3.5, and DeepSeek-Prover-V2) on long-horizon multi-agent delivery tasks involving strategic cooperation and competition. The results reveal distinct reasoning patterns and limitations across models. We open-source SimWorld and hope it becomes a foundational platform for advancing real-world agent intelligence across disciplines: https://simworld.org.

</details>


### [80] [Testing the Machine Consciousness Hypothesis](https://arxiv.org/abs/2512.01081)
*Stephen Fitz*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The Machine Consciousness Hypothesis states that consciousness is a substrate-free functional property of computational systems capable of second-order perception. I propose a research program to investigate this idea in silico by studying how collective self-models (coherent, self-referential representations) emerge from distributed learning systems embedded within universal self-organizing environments. The theory outlined here starts from the supposition that consciousness is an emergent property of collective intelligence systems undergoing synchronization of prediction through communication. It is not an epiphenomenon of individual modeling but a property of the language that a system evolves to internally describe itself. For a model of base reality, I begin with a minimal but general computational world: a cellular automaton, which exhibits both computational irreducibility and local reducibility. On top of this computational substrate, I introduce a network of local, predictive, representational (neural) models capable of communication and adaptation. I use this layered model to study how collective intelligence gives rise to self-representation as a direct consequence of inter-agent alignment. I suggest that consciousness does not emerge from modeling per se, but from communication. It arises from the noisy, lossy exchange of predictive messages between groups of local observers describing persistent patterns in the underlying computational substrate (base reality). It is through this representational dialogue that a shared model arises, aligning many partial views of the world. The broader goal is to develop empirically testable theories of machine consciousness, by studying how internal self-models may form in distributed systems without centralized control.

</details>


### [81] [CodeDistiller: Automatically Generating Code Libraries for Scientific Coding Agents](https://arxiv.org/abs/2512.01089)
*Peter Jansen,Samiah Hassan,Pragnya Narasimha*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Automated Scientific Discovery (ASD) systems can help automatically generate and run code-based experiments, but their capabilities are limited by the code they can reliably generate from parametric knowledge alone. As a result, current systems either mutate a small number of manually-crafted experiment examples, or operate solely from parametric knowledge, limiting quality and reach. We introduce CodeDistiller, a system that automatically distills large collections of scientific Github repositories into a vetted library of working domain-specific code examples, allowing ASD agents to expand their capabilities without manual effort. Using a combination of automatic and domain-expert evaluation on 250 materials science repositories, we find the best model is capable of producing functional examples for 74% of repositories, while our downstream evaluation shows an ASD agent augmented with a CodeDistiller generated library produces more accurate, complete, and scientifically sound experiments than an agent with only general materials-science code examples.

</details>


### [82] [Energy-Aware Data-Driven Model Selection in LLM-Orchestrated AI Systems](https://arxiv.org/abs/2512.01099)
*Daria Smirnova,Hamid Nasiri,Marta Adamska,Zhengxin Yu,Peter Garraghan*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: As modern artificial intelligence (AI) systems become more advanced and capable, they can leverage a wide range of tools and models to perform complex tasks. Today, the task of orchestrating these models is often performed by Large Language Models (LLMs) that rely on qualitative descriptions of models for decision-making. However, the descriptions provided to these LLM-based orchestrators do not reflect true model capabilities and performance characteristics, leading to suboptimal model selection, reduced accuracy, and increased energy costs. In this paper, we conduct an empirical analysis of LLM-based orchestration limitations and propose GUIDE, a new energy-aware model selection framework that accounts for performance-energy trade-offs by incorporating quantitative model performance characteristics in decision-making. Experimental results demonstrate that GUIDE increases accuracy by 0.90%-11.92% across various evaluated tasks, and achieves up to 54% energy efficiency improvement, while reducing orchestrator model selection latency from 4.51 s to 7.2 ms.

</details>


### [83] [Foundation Priors](https://arxiv.org/abs/2512.01107)
*Sanjog Misra*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Foundation models, and in particular large language models, can generate highly informative responses, prompting growing interest in using these ''synthetic'' outputs as data in empirical research and decision-making. This paper introduces the idea of a foundation prior, which shows that model-generated outputs are not as real observations, but draws from the foundation prior induced prior predictive distribution. As such synthetic data reflects both the model's learned patterns and the user's subjective priors, expectations, and biases. We model the subjectivity of the generative process by making explicit the dependence of synthetic outputs on the user's anticipated data distribution, the prompt-engineering process, and the trust placed in the foundation model.
  We derive the foundation prior as an exponential-tilted, generalized Bayesian update of the user's primitive prior, where a trust parameter governs the weight assigned to synthetic data. We then show how synthetic data and the associated foundation prior can be incorporated into standard statistical and econometric workflows, and discuss their use in applications such as refining complex models, informing latent constructs, guiding experimental design, and augmenting random-coefficient and partially linear specifications. By treating generative outputs as structured, explicitly subjective priors rather than as empirical observations, the framework offers a principled way to harness foundation models in empirical work while avoiding the conflation of synthetic ''facts'' with real data.

</details>


### [84] [A Benchmark of Causal vs Correlation AI for Predictive Maintenance](https://arxiv.org/abs/2512.01149)
*Krishna Taduri,Shaunak Dhande,Giacinto Paolo,Saggese,Paul Smith*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Predictive maintenance in manufacturing environments presents a challenging optimization problem characterized by extreme cost asymmetry, where missed failures incur costs roughly fifty times higher than false alarms. Conventional machine learning approaches typically optimize statistical accuracy metrics that do not reflect this operational reality and cannot reliably distinguish causal relationships from spurious correlations. This study evaluates eight predictive models, ranging from baseline statistical approaches to formal causal inference methods, on a dataset of 10,000 CNC machines with a 3.3% failure prevalence. The formal causal inference model (L5) achieved estimated annual cost savings of 1.16 million USD (a 70.2 percent reduction), outperforming the best correlation-based decision tree model (L3) by approximately 80,000 USD per year. The causal model matched the highest observed recall (87.9 percent) while reducing false alarms by 97 percent (from 165 to 5) and attained a precision of 92.1 percent, with a train-test performance gap of only 2.6 percentage points. These results indicate that causal AI methods, when combined with domain knowledge, can yield superior financial outcomes and more interpretable predictions compared to correlation-based approaches in predictive maintenance applications.

</details>


### [85] [fMRI2GES: Co-speech Gesture Reconstruction from fMRI Signal with Dual Brain Decoding Alignment](https://arxiv.org/abs/2512.01189)
*Chunzheng Zhu,Jialin Shao,Jianxin Lin,Yijun Wang,Jing Wang,Jinhui Tang,Kenli Li*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Understanding how the brain responds to external stimuli and decoding this process has been a significant challenge in neuroscience. While previous studies typically concentrated on brain-to-image and brain-to-language reconstruction, our work strives to reconstruct gestures associated with speech stimuli perceived by brain. Unfortunately, the lack of paired \{brain, speech, gesture\} data hinders the deployment of deep learning models for this purpose. In this paper, we introduce a novel approach, \textbf{fMRI2GES}, that allows training of fMRI-to-gesture reconstruction networks on unpaired data using \textbf{Dual Brain Decoding Alignment}. This method relies on two key components: (i) observed texts that elicit brain responses, and (ii) textual descriptions associated with the gestures. Then, instead of training models in a completely supervised manner to find a mapping relationship among the three modalities, we harness an fMRI-to-text model, a text-to-gesture model with paired data and an fMRI-to-gesture model with unpaired data, establishing dual fMRI-to-gesture reconstruction patterns. Afterward, we explicitly align two outputs and train our model in a self-supervision way. We show that our proposed method can reconstruct expressive gestures directly from fMRI recordings. We also investigate fMRI signals from different ROIs in the cortex and how they affect generation results. Overall, we provide new insights into decoding co-speech gestures, thereby advancing our understanding of neuroscience and cognitive science.

</details>


### [86] [Knowledge Graph Augmented Large Language Models for Next-Visit Disease Prediction](https://arxiv.org/abs/2512.01210)
*Ruiyu Wang,Tuan Vinh,Ran Xu,Yuyin Zhou,Jiaying Lu,Carl Yang,Francisco Pasquel*

Main category: cs.AI

TL;DR: 该论文提出了一个基于知识图谱引导的链式思维（KG-guided CoT）框架，用于在MIMIC-III数据集中进行疾病预测。


<details>
  <summary>Details</summary>
Motivation: 解决当前临床预测模型解释性差的问题，通过知识图谱提供临床相关的临时一致推理路径，从而为每个病患生成有意义的解释。

Method: 将ICD-9代码映射到PrimeKG知识图谱，从中提取与疾病相关的节点和多跳推理路径，并用这些路径作为CoT生成的支架。使用轻量级LLaMA-3.1-Instruct-8B和Gemma-7B模型进行微调，仅在结论符合观测结果时才保留解释。

Result: 在十个疾病映射和有限训练集（400和1000病例）上，该方法优于经典基准模型，AUROC值为0.66至0.70，macro-AUPR为0.40至0.47，同时在CRADLE数据集上展现良好零样本迁移性能。临床医师的盲测显示对KG-guided CoT解释在清晰性、相关性和临床正确性上的一致偏好。

Conclusion: KG-guided CoT框架能够有效生成临床相关和临时一致的推理，提高疾病预测模型的解释能力，并展现出了良好的迁移性。

Abstract: Electronic health records (EHRs) support powerful clinical prediction models, but existing methods typically provide coarse, post hoc explanations that offer limited value for patient-level decision making. We introduce a knowledge graph (KG)-guided chain-of-thought (CoT) framework that generates clinically grounded and temporally consistent reasoning for visit-level disease prediction in MIMIC-III. ICD-9 codes are mapped to PrimeKG, from which disease-relevant nodes and multi-hop reasoning paths are extracted and used as scaffolds for CoT generation; only explanations whose conclusions match observed outcomes are retained. Lightweight LLaMA-3.1-Instruct-8B and Gemma-7B models are then fine-tuned on this supervision corpus. Across ten PrimeKG-mapped diseases and limited training cohorts (400 and 1000 cases), KG-guided models outperform strong classical baselines, achieving AUROC values of 0.66 to 0.70 and macro-AUPR values of 0.40 to 0.47. The models also transfer zero-shot to the CRADLE cohort, improving accuracy from approximately 0.40 to 0.51 up to 0.72 to 0.77. A blinded clinician evaluation shows consistent preference for KG-guided CoT explanations in clarity, relevance, and clinical correctness.

</details>


### [87] [Unsupervised decoding of encoded reasoning using language model interpretability](https://arxiv.org/abs/2512.01222)
*Ching Fang,Samuel Marks*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: As large language models become increasingly capable, there is growing concern that they may develop reasoning processes that are encoded or hidden from human oversight. To investigate whether current interpretability techniques can penetrate such encoded reasoning, we construct a controlled testbed by fine-tuning a reasoning model (DeepSeek-R1-Distill-Llama-70B) to perform chain-of-thought reasoning in ROT-13 encryption while maintaining intelligible English outputs. We evaluate mechanistic interpretability methods--in particular, logit lens analysis--on their ability to decode the model's hidden reasoning process using only internal activations. We show that logit lens can effectively translate encoded reasoning, with accuracy peaking in intermediate-to-late layers. Finally, we develop a fully unsupervised decoding pipeline that combines logit lens with automated paraphrasing, achieving substantial accuracy in reconstructing complete reasoning transcripts from internal model representations. These findings suggest that current mechanistic interpretability techniques may be more robust to simple forms of encoded reasoning than previously understood. Our work provides an initial framework for evaluating interpretability methods against models that reason in non-human-readable formats, contributing to the broader challenge of maintaining oversight over increasingly capable AI systems.

</details>


### [88] [OntoMetric: An Ontology-Guided Framework for Automated ESG Knowledge Graph Construction](https://arxiv.org/abs/2512.01289)
*Mingqin Yu,Fethi Rabhi,Boming Xia,Zhengyi Yang,Felix Tan,Qinghua Lu*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Environmental, Social, and Governance (ESG) disclosure frameworks such as SASB, TCFD, and IFRS S2 require organizations to compute and report numerous metrics for compliance, yet these requirements are embedded in long, unstructured PDF documents that are difficult to interpret, standardize, and audit. Manual extraction is unscalable, while unconstrained large language model (LLM) extraction often produces inconsistent entities, hallucinated relationships, missing provenance, and high validation failure rates. We present OntoMetric, an ontology-guided framework that transforms ESG regulatory documents into validated, AI- and web-ready knowledge graphs. OntoMetric operates through a three-stage pipeline: (1) structure-aware segmentation using table-of-contents boundaries, (2) ontology-constrained LLM extraction that embeds the ESGMKG schema into prompts while enriching entities with semantic fields for downstream reasoning, and (3) two-phase validation that combines LLM-based semantic verification with rule-based schema checking across entity, property, and relationship levels (VR001-VR006). The framework preserves both segment-level and page-level provenance for audit traceability. Evaluated on five ESG standards (SASB Commercial Banks, SASB Semiconductors, TCFD, IFRS S2, AASB S2) totaling 228 pages and 60 segments, OntoMetric achieves 65-90% semantic accuracy and 80-90% schema compliance, compared to 3-10% for baseline unconstrained extraction, at approximately 0.01 to 0.02 USD per validated entity. Our results demonstrate that combining symbolic ontology constraints with neural extraction enables reliable, auditable knowledge graphs suitable for regulatory compliance and web integration, supporting downstream applications such as sustainable-finance analytics, transparency portals, and automated compliance tools.

</details>


### [89] [RoboDriveVLM: A Novel Benchmark and Baseline towards Robust Vision-Language Models for Autonomous Driving](https://arxiv.org/abs/2512.01300)
*Dacheng Liao,Mengshi Qi,Peng Shu,Zhining Zhang,Yuxin Lin,Liang Liu,Huadong Ma*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Current Vision-Language Model (VLM)-based end-to-end autonomous driving systems often leverage large language models to generate driving decisions directly based on their understanding of the current scene. However, such systems introduce multiple risks in real-world driving scenarios. To evaluate whether VLMs are truly viable for autonomous driving, we introduce RoboDriveBench, the first robustness benchmark focused on end-to-end trajectory prediction tasks. This benchmark systematically evaluates two critical categories of real-world challenges for VLM-based end-to-end autonomous driving systems through 11 simulated scenarios encompassing various corruption types, including 6 scenarios of sensor corruption caused by environmental variations, along with 5 cases of prompt corruption resulting from human intervention and data transmission failures. Each corruption type includes 250 unique driving scenarios and 5,689 frames, resulting in 64,559 total trajectory prediction cases per evaluation. To overcome these real-world challenges, we propose a novel VLM-based autonomous driving framework called RoboDriveVLM, which enhances robustness by mapping more multimodal data-e.g., lidar and radar-into a unified latent space. Furthermore, we introduce a new Test-Time Adaptation (TTA) method based on cross-modal knowledge distillation to improve the robustness of VLM-based autonomous driving systems. Through extensive experiments, our work highlights the limitations of current VLM-based end-to-end autonomous driving systems and provides a more reliable solution for real-world deployment. Source code and datasets will be released.

</details>


### [90] [CuES: A Curiosity-driven and Environment-grounded Synthesis Framework for Agentic RL](https://arxiv.org/abs/2512.01311)
*Shinji Mai,Yunpeng Zhai,Ziqian Chen,Cheng Chen,Anni Zou,Shuchang Tao,Zhaoyang Liu,Bolin Ding*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Large language model based agents are increasingly deployed in complex, tool augmented environments. While reinforcement learning provides a principled mechanism for such agents to improve through interaction, its effectiveness critically depends on the availability of structured training tasks. In many realistic settings, however, no such tasks exist a challenge we term task scarcity, which has become a key bottleneck for scaling agentic RL. Existing approaches typically assume predefined task collections, an assumption that fails in novel environments where tool semantics and affordances are initially unknown. To address this limitation, we formalize the problem of Task Generation for Agentic RL, where an agent must learn within a given environment that lacks predefined tasks. We propose CuES, a Curiosity driven and Environment grounded Synthesis framework that autonomously generates diverse, executable, and meaningful tasks directly from the environment structure and affordances, without relying on handcrafted seeds or external corpora. CuES drives exploration through intrinsic curiosity, abstracts interaction patterns into reusable task schemas, and refines them through lightweight top down guidance and memory based quality control. Across three representative environments, AppWorld, BFCL, and WebShop, CuES produces task distributions that match or surpass manually curated datasets in both diversity and executability, yielding substantial downstream policy improvements. These results demonstrate that curiosity driven, environment grounded task generation provides a scalable foundation for agents that not only learn how to act, but also learn what to learn. The code is available at https://github.com/modelscope/AgentEvolver/research/CuES.

</details>


### [91] [Extending NGU to Multi-Agent RL: A Preliminary Study](https://arxiv.org/abs/2512.01321)
*Juan Hernandez,Diego Fernández,Manuel Cifuentes,Denis Parra,Rodrigo Toro Icarte*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The Never Give Up (NGU) algorithm has proven effective in reinforcement learning tasks with sparse rewards by combining episodic novelty and intrinsic motivation. In this work, we extend NGU to multi-agent environments and evaluate its performance in the simple_tag environment from the PettingZoo suite. Compared to a multi-agent DQN baseline, NGU achieves moderately higher returns and more stable learning dynamics. We investigate three design choices: (1) shared replay buffer versus individual replay buffers, (2) sharing episodic novelty among agents using different k thresholds, and (3) using heterogeneous values of the beta parameter. Our results show that NGU with a shared replay buffer yields the best performance and stability, highlighting that the gains come from combining NGU intrinsic exploration with experience sharing. Novelty sharing performs comparably when k = 1 but degrades learning for larger values. Finally, heterogeneous beta values do not improve over a small common value. These findings suggest that NGU can be effectively applied in multi-agent settings when experiences are shared and intrinsic exploration signals are carefully tuned.

</details>


### [92] [A Fast Heuristic Search Approach for Energy-Optimal Profile Routing for Electric Vehicles](https://arxiv.org/abs/2512.01331)
*Saman Ahmadi,Mahdi Jalili*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We study the energy-optimal shortest path problem for electric vehicles (EVs) in large-scale road networks, where recuperated energy along downhill segments introduces negative energy costs. While traditional point-to-point pathfinding algorithms for EVs assume a known initial energy level, many real-world scenarios involving uncertainty in available energy require planning optimal paths for all possible initial energy levels, a task known as energy-optimal profile search. Existing solutions typically rely on specialized profile-merging procedures within a label-correcting framework that results in searching over complex profiles. In this paper, we propose a simple yet effective label-setting approach based on multi-objective A* search, which employs a novel profile dominance rule to avoid generating and handling complex profiles. We develop four variants of our method and evaluate them on real-world road networks enriched with realistic energy consumption data. Experimental results demonstrate that our energy profile A* search achieves performance comparable to energy-optimal A* with a known initial energy level.

</details>


### [93] [Benchmarking Overton Pluralism in LLMs](https://arxiv.org/abs/2512.01351)
*Elinor Poole-Dayan,Jiayi Wu,Taylor Sorensen,Jiaxin Pei,Michiel A. Bakker*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We introduce a novel framework for measuring Overton pluralism in LLMs--the extent to which diverse viewpoints are represented in model outputs. We (i) formalize Overton pluralism as a set coverage metric (OvertonScore), (ii) conduct a large-scale U.S.-representative human study (N = 1209; 60 questions; 8 LLMs), and (iii) develop an automated benchmark that closely reproduces human judgments. On average, models achieve OvertonScores of 0.35--0.41, with DeepSeek V3 performing best; yet all models remain far below the theoretical maximum of 1.0, revealing substantial headroom for improvement. Because repeated large-scale human studies are costly and slow, scalable evaluation tools are essential for model development. Hence, we propose an automated benchmark that achieves high rank correlation with human judgments ($ρ=0.88$), providing a practical proxy without replacing human assessment. By turning pluralistic alignment from a normative aim into a measurable benchmark, our work establishes a foundation for systematic progress toward more pluralistic LLMs.

</details>


### [94] [The Necessity of Imperfection:Reversing Model Collapse via Simulating Cognitive Boundedness](https://arxiv.org/abs/2512.01354)
*Zhongjie Jiang*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Although synthetic data is widely promoted as a remedy, its prevailing production paradigm -- one optimizing for statistical smoothness -- systematically removes the long-tail, cognitively grounded irregularities that characterize human text. Prolonged training on such statistically optimal but cognitively impoverished data accelerates model collapse.
  This paper proposes a paradigm shift: instead of imitating the surface properties of data, we simulate the cognitive processes that generate human text. We introduce the Prompt-driven Cognitive Computing Framework (PMCSF), whose core consists of a Cognitive State Decoder (CSD) that reverse-engineers unstructured text into structured cognitive vectors, and a Cognitive Text Encoder (CTE) that re-materializes these states into text enriched with human-typical imperfections via mathematically defined Cognitive Perturbation Operators.
  The framework is validated through a two-stage objective evaluation pipeline. First, in cognitive codec verification, CTE text yields a Jensen-Shannon divergence of 0.0614 from human text (vs. 0.4431 for standard LLM output), passes double-blind professional media review, and achieves an intraclass correlation coefficient ICC > 0.9 for cognitive profile alignment across heterogeneous models. Second, in functional gain evaluation, isomorphic stress tests in the A-share market show that strategies incorporating CTE-generated data reduce maximum drawdown by 47.4% during the 2015 crash and deliver 8.6% Defensive Alpha, exceeding transaction costs by a factor of 33.
  Our findings demonstrate that modelling human cognitive limitations -- not copying surface data -- enables synthetic data with genuine functional gain, offering a viable technical pathway toward resolving the AI data-collapse crisis.

</details>


### [95] [A Flexible Multi-Agent LLM-Human Framework for Fast Human Validated Tool Building](https://arxiv.org/abs/2512.01434)
*Daull Xavier,Patrice Bellot,Emmanuel Bruno,Vincent Martin,Elisabeth Murisasco*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We introduce CollabToolBuilder, a flexible multiagent LLM framework with expert-in-the-loop (HITL) guidance that iteratively learns to create tools for a target goal, aligning with human intent and process, while minimizing time for task/domain adaptation effort and human feedback capture. The architecture generates and validates tools via four specialized agents (Coach, Coder, Critic, Capitalizer) using a reinforced dynamic prompt and systematic human feedback integration to reinforce each agent's role toward goals and constraints. This work is best viewed as a system-level integration and methodology combining multi-agent in-context learning, HITL controls, and reusable tool capitalization for complex iterative problems such as scientific document generation. We illustrate it with preliminary experiments (e.g., generating state-of-the-art research papers or patents given an abstract) and discuss its applicability to other iterative problem-solving.

</details>


### [96] [A Selective Temporal Hamming distance to find patterns in state transition event timeseries, at scale](https://arxiv.org/abs/2512.01440)
*Sylvain Marié,Pablo Knecht*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Discrete event systems are present both in observations of nature, socio economical sciences, and industrial systems. Standard analysis approaches do not usually exploit their dual event / state nature: signals are either modeled as transition event sequences, emphasizing event order alignment, or as categorical or ordinal state timeseries, usually resampled a distorting and costly operation as the observation period and number of events grow. In this work we define state transition event timeseries (STE-ts) and propose a new Selective Temporal Hamming distance (STH) leveraging both transition time and duration-in-state, avoiding costly and distorting resampling on large databases. STH generalizes both resampled Hamming and Jaccard metrics with better precision and computation time, and an ability to focus on multiple states of interest. We validate these benefits on simulated and real-world datasets.

</details>


### [97] [Automated Risk-of-Bias Assessment of Randomized Controlled Trials: A First Look at a GEPA-trained Programmatic Prompting Framework](https://arxiv.org/abs/2512.01452)
*Lingbo Li,Anuradha Mathrani,Teo Susnjak*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Assessing risk of bias (RoB) in randomized controlled trials is essential for trustworthy evidence synthesis, but the process is resource-intensive and prone to variability across reviewers. Large language models (LLMs) offer a route to automation, but existing methods rely on manually engineered prompts that are difficult to reproduce, generalize, or evaluate. This study introduces a programmable RoB assessment pipeline that replaces ad-hoc prompt design with structured, code-based optimization using DSPy and its GEPA module. GEPA refines LLM reasoning through Pareto-guided search and produces inspectable execution traces, enabling transparent replication of every step in the optimization process. We evaluated the method on 100 RCTs from published meta-analyses across seven RoB domains. GEPA-generated prompts were applied to both open-weight models (Mistral Small 3.1 with GPT-oss-20b) and commercial models (GPT-5 Nano and GPT-5 Mini). In domains with clearer methodological reporting, such as Random Sequence Generation, GEPA-generated prompts performed best, with similar results for Allocation Concealment and Blinding of Participants, while the commercial model performed slightly better overall. We also compared GEPA with three manually designed prompts using Claude 3.5 Sonnet. GEPA achieved the highest overall accuracy and improved performance by 30%-40% in Random Sequence Generation and Selective Reporting, and showed generally comparable, competitively aligned performance in the other domains relative to manual prompts. These findings suggest that GEPA can produce consistent and reproducible prompts for RoB assessment, supporting the structured and principled use of LLMs in evidence synthesis.

</details>


### [98] [SynthStrategy: Extracting and Formalizing Latent Strategic Insights from LLMs in Organic Chemistry](https://arxiv.org/abs/2512.01507)
*Daniel Armstrong,Zlatko Jončev,Andres M Bran,Philippe Schwaller*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Modern computer-assisted synthesis planning (CASP) systems show promises at generating chemically valid reaction steps but struggle to incorporate strategic considerations such as convergent assembly, protecting group minimization, and optimal ring-forming sequences. We introduce a methodology that leverages Large Language Models to distill synthetic knowledge into code. Our system analyzes synthesis routes and translates strategic principles into Python functions representing diverse strategic and tactical rules, such as strategic functional group interconversions and ring construction strategies. By formalizing this knowledge as verifiable code rather than simple heuristics, we create testable, interpretable representations of synthetic strategy. We release the complete codebase and the USPTO-ST dataset -- synthesis routes annotated with strategic tags. This framework unlocks a novel capability for CASP: natural language-based route retrieval, achieving 75\% Top-3 accuracy on our benchmark. We further validate our library through temporal analysis of historical trends and chemically intuitive route clustering that offers more granular partitioning than common previous methods. This work bridges the tactical-strategic divide in CASP, enabling specification, search, and evaluation of routes by strategic criteria rather than structure alone.

</details>


### [99] [LEC: Linear Expectation Constraints for False-Discovery Control in Selective Prediction and Routing Systems](https://arxiv.org/abs/2512.01556)
*Zhiyuan Wang,Aniri,Tianlong Chen,Yue Zhang,Heng Tao Shen,Xiaoshuang Shi,Kaidi Xu*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Large language models (LLMs) often generate unreliable answers, while heuristic uncertainty methods fail to fully distinguish correct from incorrect predictions, causing users to accept erroneous answers without statistical guarantees. We address this issue through the lens of false discovery rate (FDR) control, ensuring that among all accepted predictions, the proportion of errors does not exceed a target risk level. To achieve this in a principled way, we propose LEC, which reinterprets selective prediction as a constrained decision problem by enforcing a Linear Expectation Constraint over selection and error indicators. Then, we establish a finite-sample sufficient condition, which relies only on a held-out set of exchangeable calibration samples, to compute an FDR-constrained, coverage-maximizing threshold. Furthermore, we extend LEC to a two-model routing mechanism: given a prompt, if the current model's uncertainty exceeds its calibrated threshold, we delegate it to a stronger model, while maintaining a unified FDR guarantee. Evaluations on closed-ended and open-ended question-answering (QA) datasets show that LEC achieves tighter FDR control and substantially improves sample retention over prior methods. Moreover, the two-model routing mechanism achieves lower risk levels while accepting more correct samples than each individual model.

</details>


### [100] [CLIP-RL: Aligning Language and Policy Representations for Task Transfer in Reinforcement Learning](https://arxiv.org/abs/2512.01616)
*Chainesh Gautam,Raghuram Bharadwaj Diddigi*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Recently, there has been an increasing need to develop agents capable of solving multiple tasks within the same environment, especially when these tasks are naturally associated with language. In this work, we propose a novel approach that leverages combinations of pre-trained (language, policy) pairs to establish an efficient transfer pipeline. Our algorithm is inspired by the principles of Contrastive Language-Image Pretraining (CLIP) in Computer Vision, which aligns representations across different modalities under the philosophy that ''two modalities representing the same concept should have similar representations.'' The central idea here is that the instruction and corresponding policy of a task represent the same concept, the task itself, in two different modalities. Therefore, by extending the idea of CLIP to RL, our method creates a unified representation space for natural language and policy embeddings. Experimental results demonstrate the utility of our algorithm in achieving faster transfer across tasks.

</details>


### [101] [Probabilistic Neuro-Symbolic Reasoning for Sparse Historical Data: A Framework Integrating Bayesian Inference, Causal Models, and Game-Theoretic Allocation](https://arxiv.org/abs/2512.01723)
*Saba Kublashvili*

Main category: cs.AI

TL;DR: 提出了HistoricalML，一个概率神经符号框架，通过整合贝叶斯不确定性量化、结构因果模型、合作博弈论和基于注意力的神经架构，来解决历史事件的建模挑战。


<details>
  <summary>Details</summary>
Motivation: 历史事件建模面临数据稀缺、噪声、混杂因素和解释性要求等多重挑战。

Method: 整合了1) 贝叶斯不确定性量化；2) 结构因果模型；3) 合作博弈论（Shapley值）；4) 基于注意力的神经架构。

Result: 在两个历史案例研究中，模型成功识别了德国在非洲分割中的结构性紧张关系，并在第二次布匿战争中通过蒙特卡洛战斗模拟与历史结果保持一致。

Conclusion: HistoricalML框架在数据稀疏和解释性要求高的历史事件建模中具有显著优势。

Abstract: Modeling historical events poses fundamental challenges for machine learning: extreme data scarcity (N << 100), heterogeneous and noisy measurements, missing counterfactuals, and the requirement for human interpretable explanations. We present HistoricalML, a probabilistic neuro-symbolic framework that addresses these challenges through principled integration of (1) Bayesian uncertainty quantification to separate epistemic from aleatoric uncertainty, (2) structural causal models for counterfactual reasoning under confounding, (3) cooperative game theory (Shapley values) for fair allocation modeling, and (4) attention based neural architectures for context dependent factor weighting. We provide theoretical analysis showing that our approach achieves consistent estimation in the sparse data regime when strong priors from domain knowledge are available, and that Shapley based allocation satisfies axiomatic fairness guarantees that pure regression approaches cannot provide. We instantiate the framework on two historical case studies: the 19th century partition of Africa (N = 7 colonial powers) and the Second Punic War (N = 2 factions). Our model identifies Germany's +107.9 percent discrepancy as a quantifiable structural tension preceding World War I, with tension factor 36.43 and 0.79 naval arms race correlation. For the Punic Wars, Monte Carlo battle simulations achieve a 57.3 percent win probability for Carthage at Cannae and 57.8 percent for Rome at Zama, aligning with historical outcomes. Counterfactual analysis reveals that Carthaginian political support (support score 6.4 vs Napoleon's 7.1), rather than military capability, was the decisive factor.

</details>


### [102] [Who Judges the Judge? LLM Jury-on-Demand: Building Trustworthy LLM Evaluation Systems](https://arxiv.org/abs/2512.01786)
*Xiaochuan Li,Ke Wang,Girija Gouda,Shubham Choudhary,Yaqun Wang,Linwei Hu,Joel Vaughan,Freddy Lecue*

Main category: cs.AI

TL;DR: 本文提出了LLM Jury-on-Demand，这是一个动态的、基于学习的评估框架，旨在解决在高风险领域集成大型语言模型（LLM）时，现有评估方法存在的可扩展性和适应性的问题。


<details>
  <summary>Details</summary>
Motivation: 随着LLM被应用到高风险领域，需要可进行实时部署并且可靠的评估方法。人工评估可靠但速度慢且花费高。单一的LLM评判偏颇，而静态评审团缺乏适应性。因此，需要一种新的框架来克服这些限制。

Method: LLM Jury-on-Demand框架通过训练多个可靠性预测器来评估LLM评判与人类专家评审之间的一致性。它利用标记分布、嵌入和结构输入特征选择每个数据点的最可靠评判员，组成一个最优评审团，并使用其可靠性作为权重来整合评分。

Result: 在摘要和RAG基准测试上的实验显示，这种动态评审团的系统在与人类判断的相关性上显著高于单一评判和静态评审团基线。

Conclusion: 这些结果突出了基于学习的适应性评审团在构建可伸缩、更可靠和值得信赖的LLM评估系统方面的潜力。

Abstract: As Large Language Models (LLMs) become integrated into high-stakes domains, there is a growing need for evaluation methods that are both scalable for real-time deployment and reliable for critical decision-making. While human evaluation is reliable, it is slow and costly. Single LLM judges are biased, and static juries lack adaptability. To overcome these limitations, we propose LLM Jury-on-Demand - a dynamic, learning-based framework for scalable and context-aware evaluation. Our method trains a set of reliability predictors to assess when LLM judges will agree with human experts, leveraging token distributions, embeddings, and structural input features. This enables a fully adaptive evaluation where, for each data point, an optimal jury of the most reliable judges is dynamically selected, and their scores are aggregated using their reliability as weights. Experiments on summarization and RAG benchmarks show that our dynamic jury system achieves significantly higher correlation with human judgment than both single-judge and static-jury baselines. These results highlight the promise of adaptive, learning-based juries for building scalable, more reliable and trustworthy evaluation systems for modern LLMs in high-stakes domains.

</details>


### [103] [H-Neurons: On the Existence, Impact, and Origin of Hallucination-Associated Neurons](https://arxiv.org/abs/2512.01797)
*Cheng Gao,Huimin Chen,Chaojun Xiao,Zhiyi Chen,Zhiyuan Liu,Maosong Sun*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Large language models (LLMs) frequently generate hallucinations -- plausible but factually incorrect outputs -- undermining their reliability. While prior work has examined hallucinations from macroscopic perspectives such as training data and objectives, the underlying neuron-level mechanisms remain largely unexplored. In this paper, we conduct a systematic investigation into hallucination-associated neurons (H-Neurons) in LLMs from three perspectives: identification, behavioral impact, and origins. Regarding their identification, we demonstrate that a remarkably sparse subset of neurons (less than $0.1\%$ of total neurons) can reliably predict hallucination occurrences, with strong generalization across diverse scenarios. In terms of behavioral impact, controlled interventions reveal that these neurons are causally linked to over-compliance behaviors. Concerning their origins, we trace these neurons back to the pre-trained base models and find that these neurons remain predictive for hallucination detection, indicating they emerge during pre-training. Our findings bridge macroscopic behavioral patterns with microscopic neural mechanisms, offering insights for developing more reliable LLMs.

</details>


### [104] [Testing Transformer Learnability on the Arithmetic Sequence of Rooted Trees](https://arxiv.org/abs/2512.01870)
*Alessandro Breccia,Federica Gerace,Marco Lippi,Gabriele Sicuro,Pierluigi Contucci*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We study whether a Large Language Model can learn the deterministic sequence of trees generated by the iterated prime factorization of the natural numbers. Each integer is mapped into a rooted planar tree and the resulting sequence $ \mathbb{N}\mathcal{T}$ defines an arithmetic text with measurable statistical structure. A transformer network (the GPT-2 architecture) is trained from scratch on the first $10^{11}$ elements to subsequently test its predictive ability under next-word and masked-word prediction tasks. Our results show that the model partially learns the internal grammar of $\mathbb{N}\mathcal{T}$, capturing non-trivial regularities and correlations. This suggests that learnability may extend beyond empirical data to the very structure of arithmetic.

</details>


### [105] [Graph Distance as Surprise: Free Energy Minimization in Knowledge Graph Reasoning](https://arxiv.org/abs/2512.01878)
*Gaganpreet Jhajj,Fuhua Lin*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: In this work, we propose that reasoning in knowledge graph (KG) networks can be guided by surprise minimization. Entities that are close in graph distance will have lower surprise than those farther apart. This connects the Free Energy Principle (FEP) from neuroscience to KG systems, where the KG serves as the agent's generative model. We formalize surprise using the shortest-path distance in directed graphs and provide a framework for KG-based agents. Graph distance appears in graph neural networks as message passing depth and in model-based reinforcement learning as world model trajectories. This work-in-progress study explores whether distance-based surprise can extend recent work showing that syntax minimizes surprise and free energy via tree structures.

</details>


### [106] [Predicting Human Chess Moves: An AI Assisted Analysis of Chess Games Using Skill-group Specific n-gram Language Models](https://arxiv.org/abs/2512.01880)
*Daren Zhong,Dingcheng Huang,Clayton Greenberg*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Chess, a deterministic game with perfect information, has long served as a benchmark for studying strategic decision-making and artificial intelligence. Traditional chess engines or tools for analysis primarily focus on calculating optimal moves, often neglecting the variability inherent in human chess playing, particularly across different skill levels.
  To overcome this limitation, we propose a novel and computationally efficient move prediction framework that approaches chess move prediction as a behavioral analysis task. The framework employs n-gram language models to capture move patterns characteristic of specific player skill levels. By dividing players into seven distinct skill groups, from novice to expert, we trained separate models using data from the open-source chess platform Lichess. The framework dynamically selects the most suitable model for prediction tasks and generates player moves based on preceding sequences.
  Evaluation on real-world game data demonstrates that the model selector module within the framework can classify skill levels with an accuracy of up to 31.7\% when utilizing early game information (16 half-moves). The move prediction framework also shows substantial accuracy improvements, with our Selector Assisted Accuracy being up to 39.1\% more accurate than our benchmark accuracy. The computational efficiency of the framework further enhances its suitability for real-time chess analysis.

</details>


### [107] [Learned-Rule-Augmented Large Language Model Evaluators](https://arxiv.org/abs/2512.01958)
*Jie Meng,Jin Mao*

Main category: cs.AI

TL;DR: 本文探索大型语言模型(LLMs)作为多种任务通用评估者的潜力，并提出一种规则增强的评估框架，以克服现有评估方法的泛化瓶颈。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在自然语言生成(NLG)任务中主要作为评估者，但其在更广泛评估场景中的应用仍受限。

Method: 引入规则精炼方法，通过LLM辅助的蒙特卡洛树搜索从数据中自动提取评分规则，并提出链式规则（CoR）和基于强化学习的规则增强评估者（RuAE）两个策略。

Result: 在多种任务上的广泛实验表明，该方法在各种评估场景中的有效性和泛化能力。

Conclusion: 

Abstract: Large language models (LLMs) are predominantly used as evaluators for natural language generation (NLG) tasks, but their application to broader evaluation scenarios remains limited. In this work, we explore the potential of LLMs as general evaluators across diverse tasks. Although LLM-based evaluators have made progress in different areas, existing methods struggle to generalize due to their reliance on costly, human-designed evaluation principles, which are often misaligned with both annotated data and LLMs' understanding.To address these challenges, we propose a rule-augmented evaluation paradigm. First, we introduce a rule distillation method that automatically extracts scoring rules from data using an LLM-assisted Monte Carlo Tree Search (MCTS), alleviating scalability issues and improving alignment with data. Second, to enable LLMs to effectively apply the learned rules, we propose two strategies: (1) Chain-of-Rule (CoR), which guides LLM to follow distilled rules, and (2) training a rule-augmented LLM evaluator (RuAE) via reinforcement learning, further bridging the gap between rules and LLMs' reasoning. Extensive experiments on diverse tasks demonstrate the effectiveness and generalizability of our approach across various evaluation scenarios.

</details>


### [108] [From Atomic to Composite: Reinforcement Learning Enables Generalization in Complementary Reasoning](https://arxiv.org/abs/2512.01970)
*Sitao Cheng,Xunjian Yin,Ruiwen Zhou,Yuxuan Li,Xinyi Wang,Liangming Pan,William Yang Wang,Victor Zhong*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The mechanism by which RL contributes to reasoning capabilities-whether it incentivizes the synthesis of new skills or merely amplifies existing behaviors-remains a subject of intense debate. In this work, we investigate this question through the lens of Complementary Reasoning, a complex task that requires integrating internal parametric knowledge with external contextual information. Using a controlled synthetic dataset of human biographies, we strictly decouple this ability into two atomic skills: Parametric Reasoning (relying on internal knowledge) and Contextual Reasoning (depending on external information). To rigorously assess capability boundaries, we evaluate generalization across three distinct levels of difficulty: I.I.D., Composition, and Zero-shot settings. We find that while SFT is sufficient for in-distribution performance, it struggles with O.O.D. generalization, particularly in Zero-shot settings where relational combinations are novel. Crucially, we identify the SFT Generalization Paradox: Models supervised solely on the composite task achieve near-perfect in-distribution accuracy but collapse on out-of-distribution generalization, indicating their reliance on rote memorization of path shortcuts. In contrast, we find that RL acts as a reasoning synthesizer rather than a probability amplifier. However, we uncover a strict atomic prerequisite: RL can only synthesize these complex strategies if the base model has first mastered the independent atomic skills (Parametric and Contextual) via SFT. These findings challenge the view of RL as a mere amplifier, suggesting that given sufficient atomic foundations, RL can actively synthesize complex reasoning strategies from learned primitives without explicit supervision on such complex strategies. This indicates that decoupled atomic training followed by RL offers a scalable path to generalization for complex reasoning tasks.

</details>


### [109] [Chain-of-Ground: Improving GUI Grounding via Iterative Reasoning and Reference Feedback](https://arxiv.org/abs/2512.01979)
*Aiden Yiliu Li,Bizhi Yu,Daoan Lei,Tianhe Ren,Shilong Liu*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: GUI grounding aims to align natural language instructions with precise regions in complex user interfaces. Advanced multimodal large language models show strong ability in visual GUI grounding but still struggle with small or visually similar targets and ambiguity in real world layouts. These limitations arise from limited grounding capacity and from underuse of existing reasoning potential. We present Chain of Ground CoG a training free multi step grounding framework that uses multimodal large language models for iterative visual reasoning and refinement. Instead of direct prediction the model progressively reflects and adjusts its hypotheses leading to more accurate and interpretable localization. Our approach achieves 68.4 accuracy on the ScreenSpot Pro benchmark an improvement of 4.8 points. To measure real world generalization we introduce TPanel UI a dataset of 420 labeled industrial control panels with visual distortions such as blur and masking. On TPanel UI Chain of Ground improves over the strong baseline Qwen3 VL 235B by 6.9 points showing the effectiveness of multi step training free grounding across real world and digital interfaces. These results highlight a direction for unlocking grounding potential through structured iterative refinement instead of additional training.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [110] [Injecting Sustainability in Software Architecture: A Rapid Review](https://arxiv.org/abs/2512.00106)
*Markus Funke,Patricia Lago*

Main category: cs.SE

TL;DR: 这篇论文探讨了可持续性在软件设计、开发和运营中的重要性，通过行业与学术合作，结合实际研究和从业者意见，分析并提出了在软件架构中植入可持续性考虑的五个实用建议。


<details>
  <summary>Details</summary>
Motivation: 研究旨在系统地将可持续性融入软件工程的现有实践，并为软件架构师提供实际操作中的指导。

Method: 采用混合型实证研究方法，结合了：
- 二级研究的快速回顾
- 从业者的焦点小组讨论

Result: 在嵌入可持续性的软件架构中，识别了机遇与挑战，提出了五个可操作的指导方向，这些结论来源于文献及行业实践的综合分析。

Conclusion: 该研究帮助并引导了行业伙伴在软件架构实践中更好地融合可持续性相关问题，为未来的发展提供了实际建议。

Abstract: Sustainability has evolved from an emerging concern into a fundamental responsibility in software design, development, and operation. Research increasingly explores how sustainability can be systematically integrated into existing software engineering practices. Building on an industry-academia collaboration, we contribute to this discourse by conducting a mixed-method empirical study. We combine a rapid review of secondary studies with a focus group of practitioners. The review identifies challenges and opportunities in embedding sustainability in software architecture, while the focus group enriches and compares these findings. Based on the literature and industry synthesis, we derive five tangible takeaways to inform architects working in the field, and to guide our industry partners in the integration of sustainability concerns in architecture practices.

</details>


### [111] [Generating Verifiable CoT from Execution-Traces](https://arxiv.org/abs/2512.00127)
*Shailja Thakur,Vaibhav Saxena,Rohan Kulkarni,Shivdeep Singh,Parameswaran Selvam,Hima Patel,Hiroshi Kanayama*

Main category: cs.SE

TL;DR: 该论文提出了一种新方法，通过程序执行轨迹生成可验证的思维链（CoT）数据，以解决语言模型在代码执行推理中的逻辑错误问题。


<details>
  <summary>Details</summary>
Motivation: 现有的合成训练数据存在关键缺陷：推理步骤是教师模型生成的看似合理但实际上并未验证的代码解释。这导致模型学会了模仿表面上令人信服但实际上逻辑错误的推理模式。

Method: 提出了一种新方法，将CoT生成直接根植于程序执行轨迹中。具体地，管道化地捕获代码的动态行为，然后叙述这些经过验证的执行轨迹，生成自然语言的基本原理。

Result: 在代码推理任务（CruxEval和LiveCodeBench-Exec上的正向推理，CruxEval-Input上的反向推理）以及HumanEval的代码生成和解释任务上评估了所提出的方法。使用双向追踪数据训练的模型取得了显著的性能提升，预测输出和输入的得分分别提高了30点和28点，同时生成解释和代码的质量也有所提高。

Conclusion: 通过可验证的思维链方法，模型在代码执行推理任务中的表现得到了显著提高，推理逻辑的可靠性也大大增强。

Abstract: Teaching language models to reason about code execution remains a fundamental challenge. While Chain-of-Thought (CoT) prompting has shown promise, current synthetic training data suffers from a critical weakness: the reasoning steps are often plausible-sounding explanations generated by teacher models, not verifiable accounts of what the code actually does. This creates a troubling failure mode where models learn to mimic superficially convincing but logically flawed reasoning patterns.
  We address this by grounding CoT generation directly in program execution traces. Our pipeline instruments code to capture its dynamic behavior, then narrates these verified execution traces into natural language rationales that are correct by construction. This execution-grounded approach ensures every reasoning step reflects what the program genuinely computes, eliminating logical hallucinations at the source. We evaluate our method on code reasoning tasks (forward reasoning on CruxEval and LiveCodeBench-Exec, backward reasoning on CruxEval-Input), as well as code generation and explanation tasks from HumanEval. Models trained on our bi-directional trace-grounded data achieve substantial improvements, with gains of up to 30 points on output prediction and 28 points on input prediction over base models, alongside improved explanation and code generation, demonstrating that verifiable reasoning fundamentally enhances model capabilities. https://github.ibm.com/IBM-Research-AI/Verified-Code-CoT

</details>


### [112] [Demystifying Errors in LLM Reasoning Traces: An Empirical Study of Code Execution Simulation](https://arxiv.org/abs/2512.00215)
*Mohammad Abdollahi,Khandaker Rifah Tasnia,Soumit Kanti Saha,Jinqiu Yang,Song Wang,Hadi Hemmati*

Main category: cs.SE

TL;DR: 本文首次对LLMs的运行时行为推理进行了实证研究，旨在揭示并表征其推理过程错误类型。


<details>
  <summary>Details</summary>
Motivation: 文章提到虽然LLMs在预测程序输出上表现出色，但是关于其推理过程的结构和失败模式了解甚少。

Method: 从HumanEval Plus 和 LiveCodeBench选取基准数据集，包含427个代码片段，为每个片段测试三种输入类型，并选取4种最先进的推理LLMs进行评估，并提出一个包含9类推理错误分类法。

Result: 这些LLMs在输入类型上的准确率在85%到98%之间。9类推理错误被定义，还探索了工具增强推理方法，工具支持修正了58%的'计算错误'类问题，显示工具支持的潜力。

Conclusion: 工具增强推理可以显著改善LLMs推理的性能和准确性，为未来探索工具支持LLM推理的方向提供依据。

Abstract: Understanding a program's runtime reasoning behavior, meaning how intermediate states and control flows lead to final execution results, is essential for reliable code generation, debugging, and automated reasoning. Although large language models (LLMs) can accurately predict program outputs, most prior work has focused on output accuracy and performance, treating reasoning as a black box. As a result, little is known about the structure or failure modes of their reasoning traces. To address this gap, we conduct the first empirical study on runtime behavior inference with reasoning LLMs, aiming to uncover and characterize errors in their reasoning traces. We curate a benchmark from HumanEval Plus and LiveCodeBench, containing 427 code snippets. For each snippet, we test three input types: regular, edge, and invalid. Twelve input values are selected per snippet, each paired with its ground-truth execution result. We evaluate four state-of-the-art reasoning LLMs. Our results show that these models reach accuracies between 85 percent and 98 percent across input types. We also analyze the produced reasoning traces and develop a taxonomy with nine categories of inference errors. Finally, we explore tool-augmented reasoning. Using failures in the Computation Errors category as a case study, our experiments show that this approach corrects 58 percent of such errors, demonstrating the potential of tool support for improving LLM reasoning.

</details>


### [113] [CodeFlowLM: Incremental Just-In-Time Defect Prediction with Pretrained Language Models and Exploratory Insights into Defect Localization](https://arxiv.org/abs/2512.00231)
*Monique Louise Monteiro,George G. Cabral,Adriano L. I. OLiveira*

Main category: cs.SE

TL;DR: 此论文提出了 CodeFlowLM ，一个利用预训练语言模型的增量学习框架，用于即时软件缺陷预测。与传统的在线学习不同， CodeFlowLM 使用持续微调处理概念漂移、类别不平衡和验证延迟，而无需从头开始重新训练。通过在项目内和跨项目的 JIT-SDP 场景中对仅编码器和编码器-解码器预训练语言模型（尤其是 CodeT5+ 和 UniXCoder）进行了评估，并与增量基线 BORB 进行了比较。结果显示， CodeFlowLM 在适应性、鲁棒性上表现出色， G-Mean 收益高达 68%。研究进一步将分析延伸至即时缺陷定位（JIT-DL），以基于注意力的模型为基准评测了 GPT-5 、 Claude Sonnet 4.5 和 Gemini 2.5 Pro 等大型语言模型（LLMs）。通过定性误差分析发现，大多数假阳性源于：1 ) 类似人类的保守偏见，2 ) 基于diff的提示中的上下文信息不足，3 ) JIT-Defects4J 中的潜在数据集错误标记。假阴性在较小比例中出现。总体而言， CodeFlowLM 在自适应即时 SDP 方面显著提高了技术水平。此外，对JIT-DL 中LLMs的探索性分析不仅将其性能与成熟的基于注意力的模型进行了对比，还深入揭示了基于提示推理的当前局限性。


<details>
  <summary>Details</summary>
Motivation: 这项研究旨在解决 JIT-SDP 中的概念漂移、类别不平衡、验证延迟的问题，并探索LLMs在 JIT-DL 中的应用。

Method: 研究方法主要包括：1 ) 提出 CodeFlowLM 框架; 2 )通过不断微调的方法来应对一系列挑战;3 ) 评估多个语言模型并与基线方法对比;4 ) JIT-DL中的LLMs分析。

Result: 研究结果表明：1 ) CodeFlowLM 在 JIT-SDP 中的表现优于传统基线方法，G-Mean增益高达68％;2) GPT-5在JIT-DL中表现相当;3)假阳性的主要来源在于人类似的保守偏见、基于diff的提示缺乏足够的上下文信息、以及数据集误标注。

Conclusion: 该研究证实了 CodeFlowLM 在面对动态变化的软件环境时，具备更优秀的适应性和稳健性，明显提升了增量式 JIT-SDP 的技术标准。同时，通过对 JIT-DL 中大型语言模型的探究性分析，不仅将它们与成熟的基于注意力的模型进行了基准测试，还深入揭示了基于提示推理的现有局限性。

Abstract: This work introduces CodeFlowLM, an incremental learning framework for Just-In-Time Software Defect Prediction (JIT-SDP) that leverages pre-trained language models (PLMs). Unlike traditional online learners, CodeFlowLM employs continual fine-tuning to address concept drift, class imbalance, and verification latency without retraining from scratch. We evaluated encoder-only and encoder-decoder PLMs (notably CodeT5+ and UniXCoder) in JIT-SDP scenarios within and between projects, comparing them with the incremental baseline BORB. The results show that CodeFlowLM achieves up to 68% G-Mean gains, confirming its superior adaptability and robustness in evolving software environments. We further extend the analysis to Just-in-Time Defect Localization (JIT-DL), benchmarking Large Language Models (LLMs) such as GPT-5, Claude Sonnet 4.5, and Gemini 2.5 Pro against attention-based models. GPT-5 delivers comparable performance for Recall@20% and Effort@20% with higher stability, although attention-based methods retain an advantage in fine-grained ranking metrics (Top-k, IFA). A qualitative error analysis reveals that most false positives arise from (1) human-like conservative bias, (2) insufficient contextual information in diff-based prompts, and (3) potential dataset mislabeling in JIT-Defects4J. These findings highlight both the promise and the current limitations of LLM reasoning in defect localization. False negatives occur in smaller proportions. Overall, CodeFlowLM significantly advances the state of the art in incremental JIT-SDP, demonstrating superior adaptability and robustness in evolving software environments. Furthermore, our exploratory analysis of LLMs in JIT-DL not only benchmarks their performance against established attention-based models but also provides critical insights into the current limitations of prompt-based defect reasoning.

</details>


### [114] [Progressive Code Integration for Abstractive Bug Report Summarization](https://arxiv.org/abs/2512.00325)
*Shaira Sadia Karim,Abrar Mahmud Rahim,Lamia Alam,Ishmam Tashdeed,Lutfun Nahar Lota,Md. Abu Raihan M. Kamal,Md. Azam Hossain*

Main category: cs.SE

TL;DR: Bug reports 通常是非结构化和冗长的，这使得开发者难以高效理解问题。现有摘要方法依赖表面文本提示，导致摘要不完整或冗余，且忽略关键代码片段。该研究提出了一种用于 LLM 的逐步代码整合框架，能够联合利用文本和代码片段，生成语义丰富的摘要，显著优于提取基线，并与最先进方法性能相当。


<details>
  <summary>Details</summary>
Motivation: Bug reports 的非结构化和冗长特性使得开发者难以高效理解软件缺陷，特别是忽略了代码片段对准确诊断缺陷的重要作用。

Method: 提出了一种用于 LLM 的逐步代码整合框架，以迭代地合并长代码片段与文本内容，克服标准 LLM 上下文窗口限制，并生成语义丰富的摘要。

Result: 在四个基准数据集和八个 LLM 的评估中，该方法超出提取基线 7.5%-58.2%，并与最先进抽象方法性能相当。

Conclusion: 联合利用文本和代码信息以增强对 bug 的理解，可以显著提升摘要质量，并克服现有方法的局限性。

Abstract: Bug reports are often unstructured and verbose, making it challenging for developers to efficiently comprehend software issues. Existing summarization approaches typically rely on surface-level textual cues, resulting in incomplete or redundant summaries, and they frequently ignore associated code snippets, which are essential for accurate defect diagnosis. To address these limitations, we propose a progressive code-integration framework for LLM-based abstractive bug report summarization. Our approach incrementally incorporates long code snippets alongside textual content, overcoming standard LLM context window constraints and producing semantically rich summaries. Evaluated on four benchmark datasets using eight LLMs, our pipeline outperforms extractive baselines by 7.5%-58.2% and achieves performance comparable to state-of-the-art abstractive methods, highlighting the benefits of jointly leveraging textual and code information for enhanced bug comprehension.

</details>


### [115] [Framework-Aware Code Generation with API Knowledge Graph-Constructed Data: A Study on HarmonyOS](https://arxiv.org/abs/2512.00380)
*Mingwei Liu,Zheng Pei,Yanlin Wang,Zihao Wang,Zikang Li,Enci Lin,Xin Peng,Zibin Zheng*

Main category: cs.SE

TL;DR: 为解决LLMs在低资源框架 (如HarmonyOS) 下的代码生成问题，提出APIKG4SYN框架，通过构建API知识图谱生成API导向训练数据，实现对LLMs微调，显著提升了生成代码的准确性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在资源有限的软件框架下，因缺乏充分预训练，导致面对框架特定API或语法时表现不佳。即使在代码结构上正确，也常因对新环境不熟悉而出错，这表明模型需要进一步适应。

Method: APIKG4SYN利用API知识图谱，构建API导向的问答代码对，以适配低资源软件框架，通过不确定性估计驱动蒙特卡洛树搜索增强API数据集，不依赖可执行代码。

Result: 在HarmonyOS为案例的实验中，该方法构建了该框架下首个代码生成基准，通过微调Qwen模型，pass@1准确率从基线GPT模型的17.59%提高到了25.00%，显著提升了LLM在低资源环境下的性能。

Conclusion: 借助知识图谱构建的API导向数据对提升LLMs在低资源软件框架下的代码生成能力具有显著作用，通过APIKG4SYN框架，可以在不依赖可执行代码的情况下提高生成代码的准确性和适应性。

Abstract: In the context of software frameworks with limited resources (such as HarmonyOS), large language models (LLMs) often exhibit poor code generation performance because they lack sufficient exposure to such environments during pre-training. Although LLMs can usually maintain correct logical structures across programming languages, they frequently struggle when dealing with framework-specific APIs or syntax, resulting in errors. This indicates that while pre-training equips LLMs with general algorithmic capabilities, they remain unfamiliar with the distinctive syntax and API usage of underrepresented frameworks. As a result, even advanced commercial models like GPT-4o cannot reliably generate correct code without prior adaptation. To address this issue, we propose APIKG4SYN, a framework designed to exploit API knowledge graphs for the construction of API-oriented question-code pairs, specifically tailored for low-resource frameworks without requiring executable code. APIKG4SYN integrates both single-API and multi-API knowledge, where the latter is derived through uncertainty estimation (UE)-driven Monte Carlo Tree Search (MCTS), enabling the creation of a diverse and informative dataset for fine-tuning LLMs. Using HarmonyOS as a case study, we build the first benchmark for HarmonyOS code generation. Experimental results show that fine-tuning Qwen with APIKG4SYN raises pass@1 accuracy to 25.00%, compared with 17.59% for the baseline GPT model. These results confirm that API-oriented data significantly enhance LLM performance in low-resource software development scenarios.

</details>


### [116] [Bias Testing and Mitigation in Black Box LLMs using Metamorphic Relations](https://arxiv.org/abs/2512.00556)
*Sina Salimian,Gias Uddin,Sumon Biswas,Henry Leung*

Main category: cs.SE

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The widespread deployment of Large Language Models (LLMs) has intensified concerns about subtle social biases embedded in their outputs. Existing guardrails often fail when faced with indirect or contextually complex bias-inducing prompts. To address these limitations, we propose a unified framework for both systematic bias evaluation and targeted mitigation. Our approach introduces six novel Metamorphic Relations (MRs) that, based on metamorphic testing principles, transform direct bias-inducing inputs into semantically equivalent yet adversarially challenging variants. These transformations enable an automated method for exposing hidden model biases: when an LLM responds inconsistently or unfairly across MR-generated variants, the underlying bias becomes detectable. We further show that the same MRs can be used to generate diverse bias-inducing samples for fine-tuning, directly linking the testing process to mitigation. Using six state-of-the-art LLMs - spanning open-source and proprietary models - and a representative subset of 385 questions from the 8,978-item BiasAsker benchmark covering seven protected groups, our MRs reveal up to 14% more hidden biases compared to existing tools. Moreover, fine-tuning with both original and MR-mutated samples significantly enhances bias resiliency, increasing safe response rates from 54.7% to over 88.9% across models. These results highlight metamorphic relations as a practical mechanism for improving fairness in conversational AI.

</details>


### [117] [SAGE: Semantic-Aware Gray-Box Game Regression Testing with Large Language Models](https://arxiv.org/abs/2512.00560)
*Jinyu Cai,Jialong Li,Nianyu Li,Zhenyu Mao,Mingyue Zhang,Kenji Tei*

Main category: cs.SE

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The rapid iteration cycles of modern live-service games make regression testing indispensable for maintaining quality and stability. However, existing regression testing approaches face critical limitations, especially in common gray-box settings where full source code access is unavailable: they heavily rely on manual effort for test case construction, struggle to maintain growing suites plagued by redundancy, and lack efficient mechanisms for prioritizing relevant tests. These challenges result in excessive testing costs, limited automation, and insufficient bug detection. To address these issues, we propose SAGE, a semanticaware regression testing framework for gray-box game environments. SAGE systematically addresses the core challenges of test generation, maintenance, and selection. It employs LLM-guided reinforcement learning for efficient, goal-oriented exploration to automatically generate a diverse foundational test suite. Subsequently, it applies a semantic-based multi-objective optimization to refine this suite into a compact, high-value subset by balancing cost, coverage, and rarity. Finally, it leverages LLM-based semantic analysis of update logs to prioritize test cases most relevant to version changes, enabling efficient adaptation across iterations. We evaluate SAGE on two representative environments, Overcooked Plus and Minecraft, comparing against both automated baselines and human-recorded test cases. Across all environments, SAGE achieves superior bug detection with significantly lower execution cost, while demonstrating strong adaptability to version updates.

</details>


### [118] [Enhancing Analogy-Based Software Effort Estimation with Firefly Algorithm Optimization](https://arxiv.org/abs/2512.00571)
*Tarun Chintada,Uday Kiran Cheera*

Main category: cs.SE

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Analogy-Based Estimation (ABE) is a popular method for non-algorithmic estimation due to its simplicity and effectiveness. The Analogy-Based Estimation (ABE) model was proposed by researchers, however, no optimal approach for reliable estimation was developed. Achieving high accuracy in the ABE might be challenging for new software projects that differ from previous initiatives. This study (conducted in June 2024) proposes a Firefly Algorithm-guided Analogy-Based Estimation (FAABE) model that combines FA with ABE to improve estimation accuracy. The FAABE model was tested on five publicly accessible datasets: Cocomo81, Desharnais, China, Albrecht, Kemerer and Maxwell. To improve prediction efficiency, feature selection was used. The results were measured using a variety of evaluation metrics; various error measures include MMRE, MAE, MSE, and RMSE. Compared to conventional models, the experimental results show notable increases in prediction precision, demonstrating the efficacy of the Firefly-Analogy ensemble.

</details>


### [119] [Large Language Models for Software Engineering: A Reproducibility Crisis](https://arxiv.org/abs/2512.00651)
*Mohammed Latif Siddiq,Arvin Islam-Gomes,Natalie Sekerak,Joanna C. S. Santos*

Main category: cs.SE

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Reproducibility is a cornerstone of scientific progress, yet its state in large language model (LLM)-based software engineering (SE) research remains poorly understood. This paper presents the first large-scale, empirical study of reproducibility practices in LLM-for-SE research. We systematically mined and analyzed 640 papers published between 2017 and 2025 across premier software engineering, machine learning, and natural language processing venues, extracting structured metadata from publications, repositories, and documentation. Guided by four research questions, we examine (i) the prevalence of reproducibility smells, (ii) how reproducibility has evolved over time, (iii) whether artifact evaluation badges reliably reflect reproducibility quality, and (iv) how publication venues influence transparency practices. Using a taxonomy of seven smell categories: Code and Execution, Data, Documentation, Environment and Tooling, Versioning, Model, and Access and Legal, we manually annotated all papers and associated artifacts. Our analysis reveals persistent gaps in artifact availability, environment specification, versioning rigor, and documentation clarity, despite modest improvements in recent years and increased adoption of artifact evaluation processes at top SE venues. Notably, we find that badges often signal artifact presence but do not consistently guarantee execution fidelity or long-term reproducibility. Motivated by these findings, we provide actionable recommendations to mitigate reproducibility smells and introduce a Reproducibility Maturity Model (RMM) to move beyond binary artifact certification toward multi-dimensional, progressive evaluation of reproducibility rigor.

</details>


### [120] [Code Comments for Quantum Software Development Kits: An Empirical Study on Qiskit](https://arxiv.org/abs/2512.00766)
*Zenghui Zhou,Yuechen Li,Yi Cai,Jinlong Wen,Xiaohan Yu,Zheng Zheng,Beibei Yin*

Main category: cs.SE

TL;DR: 本文提出了针对量子计算编程的一个代码注释数据集CC4Q，旨在通过分析Qiskit代码注释以改善开发和维护。


<details>
  <summary>Details</summary>
Motivation: 量子软件因其非直观量子力学原理使得程序员在开发时面临困难，现有的经典编程的注释方法不能直接迁移。因此，有必要研究量子代码注释并提供指导。

Method: 研究采用了Qiskit SDK，并建立了一个名为CC4Q的数据集，该数据集包括了9677个代码注释对和21970个句子级别的注释单元。通过人工标注并验证了经典开发者意图分类的适用性，同时提出了一种新的针对量子知识的分类方法。从代码注释的结构和覆盖度、开发者意图及其相关量子主题三个角度进行了综合经验研究。

Result: 研究发现经典代码注释和量子代码注释之间存在显著差异，提出了与量子软件开发相关的特定量子知识。

Conclusion: 本文提出了首个针对量子编程的代码注释数据集CC4Q，并通过经验研究揭示了经典和量子注释之间的主要差异，为量子软件的开发和维护提供了具体指导。

Abstract: Quantum computing is gaining attention from academia and industry. With the quantum Software Development Kits (SDKs), programmers can develop quantum software to explore the power of quantum computing. However, programmers may face challenges in understanding quantum software due to the non-intuitive quantum mechanics. To facilitate software development and maintenance, code comments offered in quantum SDKs serve as a natural language explanation of program functionalities and logical flows. Despite their importance, scarce research systematically reports their value and provides constructive guidelines for programmers. To address this gap, our paper focuses on Qiskit, one of the most popular quantum SDKs, and presents CC4Q, the first dataset of code comments for quantum computing. CC4Q incorporates 9677 code comment pairs and 21970 sentence-level code comment units, the latter of which involve heavy human annotation. Regarding the annotation, we validate the applicability of the developer-intent taxonomy used in classical programs, and also propose a new taxonomy considering quantum-specific knowledge. We conduct an empirical study comprehensively interpreting code comments from three perspectives: comment structure and coverage, developers' intentions, and associated quantum topics. Our findings uncover key differences in code comments between classical and quantum software, and also outline quantum-specific knowledge relevant to quantum software development.

</details>


### [121] [FC-ADL: Efficient Microservice Anomaly Detection and Localisation Through Functional Connectivity](https://arxiv.org/abs/2512.00844)
*Giles Winchester,George Parisis,Luc Berthouze*

Main category: cs.SE

TL;DR: 该论文提出了FC-ADL，一种基于功能连接性的端到端可扩展方法，用于从微服务度量中检测和定位异常变化。


<details>
  <summary>Details</summary>
Motivation: 微服务架构给软件架构带来了模块化和独立性，但也引入了运营复杂性，使异常检测和定位变得困难。以往的方法在考虑时间变化依赖性方面不足，或因计算复杂而导致扩展性差。

Method: 提出的方法FC-ADL，采用了神经科学中的功能连接性概念，有效描述了微服务度量间依赖性的时变特性，实现异常检测及根本原因候选点提供。

Result: 该方法在不同故障场景下实现了顶尖的检测和定位性能，并在扩展性上表现出色，可应用于大型真实微服务部署，如阿里的大规模服务系统。

Conclusion: FC-ADL为微服务环境下的异常检测和定位提出了一种高效且可扩展的解决方案，克服了以往方法的局限。

Abstract: Microservices have transformed software architecture through the creation of modular and independent services. However, they introduce operational complexities in service integration and system management that makes swift and accurate anomaly detection and localisation challenging. Despite the complex, dynamic, and interconnected nature of microservice architectures, prior works that investigate metrics for anomaly detection rarely include explicit information about time-varying interdependencies. And whilst prior works on fault localisation typically do incorporate information about dependencies between microservices, they scale poorly to real world large-scale deployments due to their reliance on computationally expensive causal inference. To address these challenges we propose FC-ADL, an end-to-end scalable approach for detecting and localising anomalous changes from microservice metrics based on the neuroscientific concept of functional connectivity. We show that by efficiently characterising time-varying changes in dependencies between microservice metrics we can both detect anomalies and provide root cause candidates without incurring the significant overheads of causal and multivariate approaches. We demonstrate that our approach can achieve top detection and localisation performance across a wide degree of different fault scenarios when compared to state-of-the-art approaches. Furthermore, we illustrate the scalability of our approach by applying it to Alibaba's extremely large real-world microservice deployment.

</details>


### [122] [The Software Infrastructure Attitude Scale (SIAS): A Questionnaire Instrument for Measuring Professionals' Attitudes Toward Technical and Sociotechnical Infrastructure](https://arxiv.org/abs/2512.00855)
*Miikka Kuutila,Paul Ralph,Huilian Sophie Qiu,Ronnie de Souza Santos,Morakot Choetkiertikul,Amin Milani Fard,Rana Alkadhi,Xavier Devroey,Gregorio Robles,Hideaki Hata,Sebastian Baltes,Vladimir Kovalenko,Shalini Chakraborty,Eray Tuzun,Hera Arif,Gianisa Adisaputri,Kelly Garcés,Anielle S. L. Andrade,Eyram Amedzor,Bimpe Ayoola,Keisha Gaspard-Chickoree,Arazoo Hoseyni*

Main category: cs.SE

TL;DR: 该论文在结构上非常符合典型的实证心理学和工程技术交叉领域的研究格式。每个部分都有其特定的任务和功能，为整篇论文提供了清晰、有逻辑性的框架。从给出背景和研究动机，到定义概念，再到具体的方法和结果论证，以及最后的讨论，每个部分都相互关联，形成了一个完整的研究闭环。


<details>
  <summary>Details</summary>
Motivation: 软件工程的最新研究强调了社会技术研究的重要性，这暗示了对定制心理测量量表的需求。

Method: 基于基础设施、态度和先前关于心理测量的理论，定义了目标概念并生成了量表项目。对225名软件专业人员进行了量表调查，并使用分割样本进行评估。在其中一半的样本上进行了探索性因子分析（EFA），揭示了其潜在因子结构，并在另一半样本上进行了确认性因子分析（CFA）以验证结构。

Result: EFA支持双因子结构（技术基础设施和社会技术基础设施），解释总方差的65%且具有高载荷。CFA证实了优秀的模型拟合。项目的文本反映了认知、情感和行为组件，支持了表面的内容效度。两个子尺度都与工作满意度、感知的自主性和工作的反馈相关，支持收敛效度。回归分析支持了与标准的相关性，而异质-同源比（HTMT）、Fornell-Larcker标准和模型比较都支持了区分效度。

Conclusion: 所得出的量表是衡量软件工程中技术和社会技术基础设施态度的有效工具。该工作为将心理测量严格性融入实证和软件行为学研究做出了一定贡献。

Abstract: Context: Recent software engineering (SE) research has highlighted the need for sociotechnical research, implying a demand for customized psychometric scales. Objective: We define the concepts of technical and sociotechnical infrastructure in software engineering, and develop and validate a psychometric scale that measures attitudes toward them. Method: Grounded in theories of infrastructure, attitudes, and prior work on psychometric measurement, we defined the target constructs and generated scale items. The scale was administered to 225 software professionals and evaluated using a split sample. We conducted an exploratory factor analysis (EFA) on one half of the sample to uncover the underlying factor structure and performed a confirmatory factor analysis (CFA) on the other half to validate the structure. Further analyses with the whole sample assessed face, criterion-related, and discriminant validity. Results: EFA supported a two-factor structure (technical and sociotechnical infrastructure), accounting for 65% of the total variance with strong loadings. CFA confirmed excellent model fit. Face and content validity were supported by the item content reflecting cognitive, affective, and behavioral components. Both subscales were correlated with job satisfaction, perceived autonomy, and feedback from the job itself, supporting convergent validity. Regression analysis supported criterion-related validity, while the Heterotrait-Monotrait ratio of correlations (HTMT), the Fornell-Larcker criterion, and model comparison all supported discriminant validity. Discussion: The resulting scale is a valid instrument for measuring attitudes toward technical and sociotechnical infrastructure in software engineering research. Our work contributes to ongoing efforts to integrate psychological measurement rigor into empirical and behavioral software engineering research.

</details>


### [123] [The AI Attribution Paradox: Transparency as Social Strategy in Open-Source Software Development](https://arxiv.org/abs/2512.00867)
*Obada Kraishan*

Main category: cs.SE

TL;DR: 研究探讨了AI编程助手在软件开发中的透明度和归属实践之间的矛盾，通过分析2023-2025年间的14,300个GitHub提交，揭示了开发者如何战略性地平衡AI辅助工具的声明和社区审查。


<details>
  <summary>Details</summary>
Motivation: AI编码助手对软件开发的变革带来了关于透明度和归属实践的疑问，需要深入研究开发者如何管理AI使用声明，以及社区的响应。

Method: 分析了7,393个仓库的14,300个提交，考察了八个主要AI工具的归属策略和社区的反应，并进行了时间分析。

Result: 95.2%的提交中使用了AI，但只有29.5%明确声明使用，不同工具间差异大（Claude 80.5% vs Copilot 9.0%)。明确声明会引起稍微多的社区关注，但工具选择对预测接受度影响更大。社区情绪中立，显示好奇心大于敌意，并且归属做法在三年内迅速演变。

Conclusion: 研究阐明了AI工具归属是一种战略沟通，而非简单透明，推进了对算法问责制和技术过渡时期规范形成的理解，并对开发者在如何选择声明使用AI，平台如何设计归属机制，以及相关研究者如何研究AI增强协作工作的新实践提供了指导意义。

Abstract: AI coding assistants have transformed software development, raising questions about transparency and attribution practices. We examine the "AI attribution paradox": how developers strategically balance acknowledging AI assistance with managing community scrutiny. Analyzing 14,300 GitHub commits across 7,393 repositories from 2023-2025, we investigated attribution strategies and community responses across eight major AI tools. Results reveal widespread AI usage (95.2% of commits) but strategic attribution: only 29.5% employ explicit disclosure, with dramatic tool variation (Claude 80.5% versus Copilot 9.0%). Explicit attribution triggers modest scrutiny (23% more questions and 21% more comments) but tool choice matters 20-30 times more for predicting reception. Community sentiment remains neutral regardless of attribution type, suggesting curiosity rather than hostility. Temporal analyses show rapid norm evolution: explicit attribution increased from near-zero in early 2024 to 40% by late 2025, indicating community adaptation. These findings illuminate attribution as strategic communication rather than simple transparency, advancing understanding of algorithmic accountability and norm formation during technological transitions. We discuss implications for developers navigating disclosure decisions, platforms designing attribution mechanisms, and researchers studying emergent practices in AI-augmented collaborative work.

</details>


### [124] [Staying or Leaving? How Job Satisfaction, Embeddedness and Antecedents Predict Turnover Intentions of Software Professionals](https://arxiv.org/abs/2512.00869)
*Miikka Kuutila,Paul Ralph,Huilian Sophie Qiu,Ronnie de Souza Santos,Morakot Choetkiertikul,Rana Alkadhi,Xavier Devroey,Gregorio Robles,Hideaki Hata,Sebastian Baltes,Hera Arif,Vladimir Kovalenko,Shalini Chakraborty,Eray Tuzun,Gianisa Adisaputri*

Main category: cs.SE

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Context: Voluntary turnover is common in the software industry, increasing recruitment and onboarding costs and the risk of losing organizational and tacit knowledge. Objective: This study investigates how job satisfaction, work-life balance, job embeddedness, and their antecedents, including job quality, personality traits, attitudes toward technical and sociotechnical infrastructure, and perceptions of organizational justice, relate to software professionals' turnover intentions. Method: We conducted a geographically diverse cross-sectional survey of software professionals (N = 224) and analyzed the data using partial least squares structural equation modeling (PLS-SEM). Our model includes both reflective and formative constructs and tests 15 hypotheses grounded in occupational psychology and software engineering literature. Results: Job satisfaction and embeddedness were significantly negatively associated with software professionals' turnover intentions, while work-life balance showed no direct effect. The strongest antecedents for job satisfaction were work-life balance and job quality, while organizational justice was the strongest predictor of job embeddedness. Discussion: The resulting PLS-SEM model has considerably higher explanatory power for key outcome variables than previous work conducted in the software development context, highlighting the importance of both psychological (e.g., job satisfaction, job embeddedness) and organizational (e.g., organizational justice, job quality) factors in understanding turnover intentions of software professionals. Our results imply that improving job satisfaction and job embeddedness is the key to retaining software professionals. In turn, enhancing job quality, supporting work-life balance, and ensuring high organizational justice can improve job satisfaction and embeddedness, indirectly reducing turnover intentions.

</details>


### [125] [Neural Variable Name Repair: Learning to Rename Identifiers for Readability](https://arxiv.org/abs/2512.01141)
*Muhammad Yousuf,Akshat Bagade,Chhittebbayi Penugonda,Maanas Baraya*

Main category: cs.SE

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Developers routinely work with source files whose variable names are generic or misleading, and with teams moving quickly, many functions are left undocumented. This slows comprehension, increases the risk of subtle bugs, and makes it harder for both humans and large language models (LLMs) to reason about code. We study variable name repair: given a real C++ function where all occurrences of one local or parameter name have been replaced by a placeholder (e.g. ID 1), the goal is to generate a natural, descriptive replacement name. We automatically construct this task from the C++ portion of BigCode's The Stack by parsing functions with Tree-sitter, masking a single identifier, and treating the original name as supervision. On top of Llama 3.1-8B, we build a pipeline with (i) warmup and dropout schedules for more stable fine-tuning, (ii) LoRA adapters for efficient specialization on identifier repair, and (iii) a dual-encoder reranker over top-k generator candidates. We evaluate using exact match, Top-5 Hit, and an embedding-based partial similarity score (0-100) that gives credit for near synonyms and format variants (e.g., jsonValue vs. json). On a held-out set of 200 C++ functions, a zero-shot Llama 3.1 baseline reaches 6.1 percent exact match. Our best LoRA-tuned model (with warmup and dropout) achieves 43.1 percent exact match, 50.2 percent Top-5 Hit, and an 82.03 partial-match score. A dual encoder reranker further improves selection quality without modifying the underlying generator, suggesting that task-specific fine-tuning plus reranking is a promising approach for practical identifier repair tools.

</details>


### [126] [Beyond Greenfield: AI-Driven Productivity in Documentation and Brownfield Engineering](https://arxiv.org/abs/2512.01155)
*Krishna Kumaar Sharma*

Main category: cs.SE

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Brownfield engineering work involving legacy systems, incomplete documentation, and fragmented architectural knowledge poses unique challenges for the effective use of large language models (LLMs). Prior research has largely focused on greenfield or synthetic tasks, leaving a gap in structured workflows for complex, context-heavy environments. This paper introduces the Discover-Define-Deliver (D3) Framework, a disciplined LLM-assisted workflow that combines role-separated prompting strategies with applied best practices for navigating ambiguity in brownfield systems. The framework incorporates a dual-agent prompting architecture in which a Builder model generates candidate outputs and a Reviewer model provides structured critique to improve reliability. I conducted an exploratory survey study with 52 software practitioners who applied the D3 workflow to real-world engineering tasks such as legacy system exploration, documentation reconstruction, and architectural refactoring. Respondents reported perceived improvements in task clarity, documentation quality, and cognitive load, along with self-estimated productivity gains. In this exploratory study, participants reported a weighted average productivity improvement of 26.9%, reduced cognitive load for approximately 77% of participants, and reduced rework for 83% during the Define phase. As these findings are self-reported and not derived from controlled experiments, they should be interpreted as preliminary evidence of practitioner sentiment rather than causal effects. The results highlight both the potential and limitations of structured LLM workflows for legacy engineering systems and motivate future controlled evaluations.

</details>


### [127] [LAURA: Enhancing Code Review Generation with Context-Enriched Retrieval-Augmented LLM](https://arxiv.org/abs/2512.01356)
*Yuxin Zhang,Yuxia Zhang,Zeyu Sun,Yanjie Jiang,Hui Liu*

Main category: cs.SE

TL;DR: 提出一种基于LLM的代码审查框架LAURA，整合范例检索、上下文增强和系统引导，显著提升ChatGPT-4o和DeepSeek v3的审查评论质量。


<details>
  <summary>Details</summary>
Motivation: 现有自动化代码审查方法忽略了代码变更上下文和审查先验知识等关键信息，且数据集质量普遍较低，导致审查效率和质量受限。

Method: 1) 通过范例检索引入历史审查知识；2) 构建代码变更上下文；3) 集成系统引导的LLM生成框架。同时构建高质量数据集。

Result: LAURA生成的审查评论中42.2%（ChatGPT-4o）和40.4%（DeepSeek v3）完全正确或具高实用性，性能显著超越现有方法。消融实验验证各组件正向贡献。

Conclusion: LAURA通过增强上下文感知和知识整合，有效解决了自动化代码审查中的关键信息缺失问题，为开发流程提供了高质量辅助工具。

Abstract: Code review is critical for ensuring software quality and maintainability. With the rapid growth in software scale and complexity, code review has become a bottleneck in the development process because of its time-consuming and knowledge-intensive nature and the shortage of experienced developers willing to review code. Several approaches have been proposed for automatically generating code reviews based on retrieval, neural machine translation, pre-trained models, or large language models (LLMs). These approaches mainly leverage historical code changes and review comments. However, a large amount of crucial information for code review, such as the context of code changes and prior review knowledge, has been overlooked. This paper proposes an LLM-based review knowledge-augmented, context-aware framework for code review generation, named LAURA. The framework integrates review exemplar retrieval, context augmentation, and systematic guidance to enhance the performance of ChatGPT-4o and DeepSeek v3 in generating code review comments. Besides, given the extensive low-quality reviews in existing datasets, we also constructed a high-quality dataset. Experimental results show that for both models, LAURA generates review comments that are either completely correct or at least helpful to developers in 42.2% and 40.4% of cases, respectively, significantly outperforming SOTA baselines. Furthermore, our ablation studies demonstrate that all components of LAURA contribute positively to improving comment quality.

</details>


### [128] [BackportBench: A Multilingual Benchmark for Automated Backporting of Patches](https://arxiv.org/abs/2512.01396)
*Zhiqing Zhong,Jiaming Huang,Pinjia He*

Main category: cs.SE

TL;DR: 论文主要关注软件开发中依赖项升级的问题，介绍了引入的BackportBench，一个用于自动补丁回移植技术综合评价的多语言基准测试套件。通过BackportBench评估现有方法，显示出agentic方法在逻辑和结构变更方面优于传统方法，并分析了不同编程语言的执行效果。


<details>
  <summary>Details</summary>
Motivation: 用户在使用现代软件项目时，需频繁更新其依赖项到安全版本，但升级可能导致代码库损坏。现有自动回移植技术未能很好地应对这一问题，因为它们通常只在代码块或函数级别进行操作，且评价指标不全面。因此，需要一种更全面的基准测试。

Method: 引入了BackportBench，一套用于补丁回移植问题的多语言基准测试套件。它包括202个补丁回移植问题，涵盖PyPI、Maven和npm，每个问题包含可执行的Docker环境和相关测试案例。评估了现有补丁移植方法以及LLM基础的技术。

Result: 实验结果显示，agentic方法在需要逻辑和结构变化的案例上优于传统补丁移植方法，但不同编程语言的效果有所差异。为研究者和软件开发者在自动回移植技术方面提供了若干启示。

Conclusion: BackportBench是一个用于自动补丁回移植技术的全面基准测试套件，通过实验发现agentic方法更加有效。未来的研究需关注不同语言的补丁移植效果改进，并优化相关方法。

Abstract: Many modern software projects evolve rapidly to incorporate new features and security patches. It is important for users to update their dependencies to safer versions, but many still use older, vulnerable package versions because upgrading can be difficult and may break their existing codebase. Software developers can mitigate this problem by backporting security patches to older releases. However, manually backporting is time-consuming and error-prone. The effectiveness of existing automated backporting techniques on general software remains unclear since they typically target only code-hunk or function-level patch porting scenarios and are evaluated with imperfect metrics.
  To facilitate the development and evaluation of automated backporting techniques, we introduce BackportBench, the first comprehensive benchmark suite for patch backporting problem. BackportBench is a multilingual benchmark that contains 202 patch backporting problems from PyPI, Maven, and npm, each with executable Docker environments and relevant test cases. We evaluated existing patch porting methods and LLM-based techniques that have the potential to adapt to this task using BackportBench. The results show that the agentic method has outperformed traditional patch porting methods, especially on cases that require logical and structural changes. However, the performance varies across different programming languages. Based on the findings, we draw several implications for researchers and software practitioners in future work on automated backporting.

</details>


### [129] [Teaching an Online Multi-Institutional Research Level Software Engineering Course with Industry - an Experience Report](https://arxiv.org/abs/2512.01523)
*Pankaj Jalore,Y. Raghu Reddy,Vasudeva Varma*

Main category: cs.SE

TL;DR: Covid加速了线上教学的普及，使得多机构联合开设研究级课程成为可能。本文分享了"AI在软件工程中的应用"联合教学实验及经验。


<details>
  <summary>Details</summary>
Motivation: 为弥补小型机构师资不足，利用线上形式联合多家机构开设研究级课程，并引入产业界的参与。

Method: 两所机构通过线上联合开设“AI in软件工程”课程，并吸纳行业专家参与教学，实验此合作教学模式的效果。

Result: 联合教学和行业参与的模式获得学生和教职员工的广泛认可，被认为具有推广到其他计算机学科领域的潜力。

Conclusion: 这种合作教学模式为小型机构开设研究级课程提供了可行方案。

Abstract: Covid has made online teaching and learning acceptable and students, faculty, and industry professionals are all comfortable with this mode. This comfort can be leveraged to offer an online multi-institutional research-level course in an area where individual institutions may not have the requisite faculty to teach and/or research students to enroll. If the subject is of interest to industry, online offering also allows industry experts to contribute and participate with ease. Advanced topics in Software Engineering are ideally suited for experimenting with this approach as industry, which is often looking to incorporate advances in software engineering in their practices, is likely to agree to contribute and participate. In this paper we describe an experiment in teaching a course titled "AI in Software Engineering" jointly between two institutions with active industry participation, and share our and student's experience. We believe this collaborative teaching approach can be used for offering research level courses in any applied area of computer science by institutions who are small and find it difficult to offer research level courses on their own.

</details>


### [130] [OpenDORS: A dataset of openly referenced open research software](https://arxiv.org/abs/2512.01570)
*Stephan Druskat,Lars Grunske*

Main category: cs.SE

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: In many academic disciplines, software is created during the research process or for a research purpose. The crucial role of software for research is increasingly acknowledged. The application of software engineering to research software has been formalized as research software engineering, to create better software that enables better research. Despite this, large-scale studies of research software and its development are still lacking. To enable such studies, we present a dataset of 134,352 unique open research software projects and 134,154 source code repositories referenced in open access literature. Each dataset record identifies the referencing publication and lists source code repositories of the software project. For 122,425 source code repositories, the dataset provides metadata on latest versions, license information, programming languages and descriptive metadata files. We summarize the distributions of these features in the dataset and describe additional software metadata that extends the dataset in future work. Finally, we suggest examples of research that could use the dataset to develop a better understanding of research software practice in RSE research.

</details>


### [131] [GPTrace: Effective Crash Deduplication Using LLM Embeddings](https://arxiv.org/abs/2512.01609)
*Patrick Herter,Vincent Ahlrichs,Ridvan Açilan,Julian Horsch*

Main category: cs.SE

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Fuzzing is a highly effective method for uncovering software vulnerabilities, but analyzing the resulting data typically requires substantial manual effort. This is amplified by the fact that fuzzing campaigns often find a large number of crashing inputs, many of which share the same underlying bug. Crash deduplication is the task of finding such duplicate crashing inputs and thereby reducing the data that needs to be examined. Many existing deduplication approaches rely on comparing stack traces or other information that is collected when a program crashes. Although various metrics for measuring the similarity of such pieces of information have been proposed, many do not yield satisfactory deduplication results. In this work, we present GPTrace, a deduplication workflow that leverages a large language model to evaluate the similarity of various data sources associated with crashes by computing embedding vectors and supplying those as input to a clustering algorithm. We evaluate our approach on over 300 000 crashing inputs belonging to 50 ground truth labels from 14 different targets. The deduplication results produced by GPTrace show a noticeable improvement over hand-crafted stack trace comparison methods and even more complex state-of-the-art approaches that are less flexible.

</details>


### [132] [When High-Performance Computing Meets Software Testing: Distributed Fuzzing using MPI](https://arxiv.org/abs/2512.01617)
*Pierciro Caliandro,Matteo Ciccaglione,Alessandro Pellegrini*

Main category: cs.SE

TL;DR: 该论文提出在分布式模糊测试框架中集成基于MPI的同步技术，相比传统的基于文件系统的同步方法，性能有显著提升。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于传统基于文件系统的分布式模糊测试同步方法存在通信延迟较高的问题，限制了测试效率。

Method: 通过采用轻量级的MPI原语，降低通信延迟，提高数据交换效率，并实验验证了该方法的有效性。

Result: 实验结果显示，在标准基准测试中，早期覆盖进度提升明显，且MPI方法能有效解决覆盖停滯问题，提高对复杂执行路径的探索能力。

Conclusion: 采用基于MPI的同步方法在提高分布式模糊测试的可扩展性和有效性方面具有很大潜力。

Abstract: This paper explores the integration of MPI-based synchronization techniques into distributed fuzzing frameworks, highlighting possible substantial performance improvements compared to traditional filesystem-based synchronization methods. By employing lightweight MPI primitives, reductions in communication latency are achieved, facilitating more efficient data exchanges across distributed fuzzing nodes. Experimental results obtained over standard benchmarks demonstrate enhanced coverage progression from the early stages of the fuzzing process, which could be beneficial if fuzzing is employed in CI/CD pipelines at any stage of software development. Furthermore, the coordinated exchange of input corpora among clusters of fuzzers effectively addresses coverage stagnation, enabling a sustained exploration of complex and deep execution paths. Overall, the adoption of MPI-based synchronization approaches shows promising potential for significantly enhancing the scalability and efficacy of distributed fuzz testing.

</details>


### [133] [Package Dashboard: A Cross-Ecosystem Framework for Dual-Perspective Analysis of Software Packages](https://arxiv.org/abs/2512.01630)
*Ziheng Liu,Runzhi He,Minghui Zhou*

Main category: cs.SE

TL;DR: Package Dashboard 是一个跨生态系统的软件供应链分析框架，通过整合包元数据、漏洞信息和上游社区健康指标，提供一个统一的风险评估平台，以减少开发者的认知负担并提高可追踪性。


<details>
  <summary>Details</summary>
Motivation: 现有SCA工具存在盲点，局限于单一生态系统，单独评估软件工件或社区活动，导致数据分散，风险判断困难。

Method: Package Dashboard结合了依赖关系解析和仓库分析，综合包元数据、漏洞信息和社区健康指标以提供更全面的风险评估。

Result: 对五个Linux发行版的374,000个包进行大规模研究表明，Package Dashboard 能够发现传统漏洞、许可证冲突以及被忽视的风险，比如存档或无法访问的仓库。

Conclusion: Package Dashboard增强了开源生态系统的透明度、可信度和可追踪性，为开发者和DevSecOps工程师提供了可操作的风险洞察。

Abstract: Software supply chain attacks have revealed blind spots in existing SCA tools, which are often limited to a single ecosystem and assess either software artifacts or community activity in isolation. This fragmentation across tools and ecosystems forces developers to manually reconcile scattered data, undermining risk assessments. We present Package Dashboard, a cross-ecosystem framework that provides a unified platform for supply chain analysis, enabling a holistic, dual-perspective risk assessment by integrating package metadata, vulnerability information, and upstream community health metrics. By combining dependency resolution with repository analysis, it reduces cognitive load and improves traceability. Demonstrating the framework's versatility, a large-scale study of 374,000 packages across five Linux distributions shows its ability to uncover not only conventional vulnerabilities and license conflicts but also overlooked risks such as archived or inaccessible repositories. Ultimately, Package Dashboard provides a unified view of risk, equipping developers and DevSecOps engineers with actionable insights to strengthen the transparency, trustworthiness, and traceability of open-source ecosystems. Package Dashboard is publicly available at https://github.com/n19htfall/PackageDashboard, and a demonstration video can be found at https://youtu.be/y9ncftP8KPQ. Besides, the online version is available at https://pkgdash.osslab-pku.org.

</details>


### [134] [MIT Lincoln Laboratory: A Case Study on Improving Software Support for Research Projects](https://arxiv.org/abs/2512.01649)
*Daniel Strassler,Gabe Elkin,Curran Schiefelbein,Daniel Herring,Ian Jessen,David Johnson,Santiago A. Paredes,Tod Shannon,Jim Flavin*

Main category: cs.SE

TL;DR: 论文通过内部研究探讨了研究项目中的软件开发挑战，并提出了改善文化、管理和执行的建议。


<details>
  <summary>Details</summary>
Motivation: MIT林肯实验室寻求改进和增强围绕软件工程的有效性，并改善其文化氛围。

Method: 进行内部研究，分析有效研究软件开发所面临的挑战，并探讨加强文化和执行的途径。

Result: 研究得出了三个主要方面的关键发现：项目管理、集中化的潜在效率、改善软件人员配备和文化，并提出了包括集中和标准化工件、建立人才数据库、创建软件利益相关者小组等可操作的建议。

Conclusion: 通过集中、标准化及创建共同的软件平台以更好地匹配人才与项目需求，可以提高软件工程对任务的总体影响。

Abstract: Software plays an ever increasing role in complex system development and prototyping, and in recent years, MIT Lincoln Laboratory has sought to improve both the effectiveness and culture surrounding software engineering in execution of its mission. The Homeland Protection and Air Traffic Control Division conducted an internal study to examine challenges to effective and efficient research software development, and to identify ways to strengthen both the culture and execution for greater impact on our mission. Key findings of this study fell into three main categories: project attributes that influence how software development activities must be conducted and managed, potential efficiencies from centralization, opportunities to improve staffing and culture with respect to software practitioners. The study delivered actionable recommendations, including centralizing and standardizing software support tooling, developing a common database to help match the right software talent and needs to projects, and creating a software stakeholder panel to assist with continued improvement.

</details>


### [135] [Generating REST API Tests With Descriptive Names](https://arxiv.org/abs/2512.01690)
*Philip Garrett,Juan P. Galeotti,Andrea Arcuri,Alexander Poth,Olsi Rrjolli*

Main category: cs.SE

TL;DR: 该论文提出了三种确定性的技术来生成REST API测试名称，并通过比较研究显示，一种规则基础方法在确定性和清晰度上均优于或与先进的LLM基础方法相当。


<details>
  <summary>Details</summary>
Motivation: 自动生成测试用例往往使用无意义的名称，降低了可读性和维护性。该研究旨在改进REST API测试的自动命名技术，提高开发者在理解和维护测试用例时的效率。

Method: 本文提出并比较了包括三种新型确定性及其他五种规则基础和LLM基础的方法用于生成REST API测试名称。评价通过两个调查（最多39人）和一个工业案例研究（Volkswagen AG）进行，分析了8种不同API的74个测试用例。

Result: 结果表明，规则基础方法在确定性方法中获得了最高的清晰度评分，并且与先进的LLM模型如Gemini和GPT-4o性能相当，显著优于GPT-3.5。此外，工业案例研究进一步验证了这些描述性名称对提高测试套件可读性的实际影响。

Conclusion: 这些发现表明，轻量级和确定性的技术可以有效地作为计算成本高和安全性敏感的LLM基础方法的替代方案，实现更友好和更具可读性的API测试生成。

Abstract: Automated test generation has become a key technique for ensuring software quality, particularly in modern API-based architectures. However, automatically generated test cases are typically assigned non-descriptive names (e.g., test0, test1), which reduces their readability and hinders their usefulness during comprehension and maintenance. In this work, we present three novel deterministic techniques to generate REST API test names. We then compare eight techniques in total for generating descriptive names for REST API tests automatically produced by the fuzzer EvoMaster, using 10 test cases generated for 9 different open-source APIs. The eight techniques include rule-based heuristics and large language model (LLM)-based approaches. Their effectiveness was empirically evaluated through two surveys (involving up to 39 people recruited via LinkedIn). Our results show that a rule-based approach achieves the highest clarity ratings among deterministic methods, performs on par with state-of-the-art LLM-based models such as Gemini and GPT-4o, and significantly outperforms GPT-3.5.
  To further evaluate the practical impact of our results, an industrial case study was carried out with practitioners who actively use EvoMaster at Volkswagen AG. A developer questionnaire was then carried out based on the use of EvoMaster on four different APIs by four different users, for a total of 74 evaluated test cases. Feedback from practitioners further confirms that descriptive names produced by this approach improve test suite readability.
  These findings highlight that lightweight, deterministic techniques can serve as effective alternatives to computationally expensive and security-sensitive LLM-based approaches for automated system-level test naming, providing a practical step toward more developer-friendly API test generation.

</details>


### [136] [An Empirical Study of Agent Developer Practices in AI Agent Frameworks](https://arxiv.org/abs/2512.01939)
*Yanlin Wang,Xinyi Xu,Jiachi Chen,Tingting Bi,Wenchao Gu,Zibin Zheng*

Main category: cs.SE

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The rise of large language models (LLMs) has sparked a surge of interest in agents, leading to the rapid growth of agent frameworks. Agent frameworks are software toolkits and libraries that provide standardized components, abstractions, and orchestration mechanisms to simplify agent development. Despite widespread use of agent frameworks, their practical applications and how they influence the agent development process remain underexplored. Different agent frameworks encounter similar problems during use, indicating that these recurring issues deserve greater attention and call for further improvements in agent framework design. Meanwhile, as the number of agent frameworks continues to grow and evolve, more than 80% of developers report difficulties in identifying the frameworks that best meet their specific development requirements. In this paper, we conduct the first empirical study of LLM-based agent frameworks, exploring real-world experiences of developers in building AI agents. To compare how well the agent frameworks meet developer needs, we further collect developer discussions for the ten previously identified agent frameworks, resulting in a total of 11,910 discussions. Finally, by analyzing these discussions, we compare the frameworks across five dimensions: development efficiency, functional abstraction, learning cost, performance optimization, and maintainability, which refers to how easily developers can update and extend both the framework itself and the agents built upon it over time. Our comparative analysis reveals significant differences among frameworks in how they meet the needs of agent developers. Overall, we provide a set of findings and implications for the LLM-driven AI agent framework ecosystem and offer insights for the design of future LLM-based agent frameworks and agent developers.

</details>
