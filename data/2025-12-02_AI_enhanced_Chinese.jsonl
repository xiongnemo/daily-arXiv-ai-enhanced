{"id": "2512.00106", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.00106", "abs": "https://arxiv.org/abs/2512.00106", "authors": ["Markus Funke", "Patricia Lago"], "title": "Injecting Sustainability in Software Architecture: A Rapid Review", "comment": "(Accepted/In press) 10th IEEE/ACM International Workshop on Green and Sustainable Software (GREENS '26): GREENS@ICSE 2026", "summary": "Sustainability has evolved from an emerging concern into a fundamental responsibility in software design, development, and operation. Research increasingly explores how sustainability can be systematically integrated into existing software engineering practices. Building on an industry-academia collaboration, we contribute to this discourse by conducting a mixed-method empirical study. We combine a rapid review of secondary studies with a focus group of practitioners. The review identifies challenges and opportunities in embedding sustainability in software architecture, while the focus group enriches and compares these findings. Based on the literature and industry synthesis, we derive five tangible takeaways to inform architects working in the field, and to guide our industry partners in the integration of sustainability concerns in architecture practices.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63a2\u8ba8\u4e86\u53ef\u6301\u7eed\u6027\u5728\u8f6f\u4ef6\u8bbe\u8ba1\u3001\u5f00\u53d1\u548c\u8fd0\u8425\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u901a\u8fc7\u884c\u4e1a\u4e0e\u5b66\u672f\u5408\u4f5c\uff0c\u7ed3\u5408\u5b9e\u9645\u7814\u7a76\u548c\u4ece\u4e1a\u8005\u610f\u89c1\uff0c\u5206\u6790\u5e76\u63d0\u51fa\u4e86\u5728\u8f6f\u4ef6\u67b6\u6784\u4e2d\u690d\u5165\u53ef\u6301\u7eed\u6027\u8003\u8651\u7684\u4e94\u4e2a\u5b9e\u7528\u5efa\u8bae\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u7cfb\u7edf\u5730\u5c06\u53ef\u6301\u7eed\u6027\u878d\u5165\u8f6f\u4ef6\u5de5\u7a0b\u7684\u73b0\u6709\u5b9e\u8df5\uff0c\u5e76\u4e3a\u8f6f\u4ef6\u67b6\u6784\u5e08\u63d0\u4f9b\u5b9e\u9645\u64cd\u4f5c\u4e2d\u7684\u6307\u5bfc\u3002", "method": "\u91c7\u7528\u6df7\u5408\u578b\u5b9e\u8bc1\u7814\u7a76\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86\uff1a\n- \u4e8c\u7ea7\u7814\u7a76\u7684\u5feb\u901f\u56de\u987e\n- \u4ece\u4e1a\u8005\u7684\u7126\u70b9\u5c0f\u7ec4\u8ba8\u8bba", "result": "\u5728\u5d4c\u5165\u53ef\u6301\u7eed\u6027\u7684\u8f6f\u4ef6\u67b6\u6784\u4e2d\uff0c\u8bc6\u522b\u4e86\u673a\u9047\u4e0e\u6311\u6218\uff0c\u63d0\u51fa\u4e86\u4e94\u4e2a\u53ef\u64cd\u4f5c\u7684\u6307\u5bfc\u65b9\u5411\uff0c\u8fd9\u4e9b\u7ed3\u8bba\u6765\u6e90\u4e8e\u6587\u732e\u53ca\u884c\u4e1a\u5b9e\u8df5\u7684\u7efc\u5408\u5206\u6790\u3002", "conclusion": "\u8be5\u7814\u7a76\u5e2e\u52a9\u5e76\u5f15\u5bfc\u4e86\u884c\u4e1a\u4f19\u4f34\u5728\u8f6f\u4ef6\u67b6\u6784\u5b9e\u8df5\u4e2d\u66f4\u597d\u5730\u878d\u5408\u53ef\u6301\u7eed\u6027\u76f8\u5173\u95ee\u9898\uff0c\u4e3a\u672a\u6765\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u5b9e\u9645\u5efa\u8bae\u3002"}}
{"id": "2512.00127", "categories": ["cs.SE", "cs.AI", "cs.PL"], "pdf": "https://arxiv.org/pdf/2512.00127", "abs": "https://arxiv.org/abs/2512.00127", "authors": ["Shailja Thakur", "Vaibhav Saxena", "Rohan Kulkarni", "Shivdeep Singh", "Parameswaran Selvam", "Hima Patel", "Hiroshi Kanayama"], "title": "Generating Verifiable CoT from Execution-Traces", "comment": null, "summary": "Teaching language models to reason about code execution remains a fundamental challenge. While Chain-of-Thought (CoT) prompting has shown promise, current synthetic training data suffers from a critical weakness: the reasoning steps are often plausible-sounding explanations generated by teacher models, not verifiable accounts of what the code actually does. This creates a troubling failure mode where models learn to mimic superficially convincing but logically flawed reasoning patterns.\n  We address this by grounding CoT generation directly in program execution traces. Our pipeline instruments code to capture its dynamic behavior, then narrates these verified execution traces into natural language rationales that are correct by construction. This execution-grounded approach ensures every reasoning step reflects what the program genuinely computes, eliminating logical hallucinations at the source. We evaluate our method on code reasoning tasks (forward reasoning on CruxEval and LiveCodeBench-Exec, backward reasoning on CruxEval-Input), as well as code generation and explanation tasks from HumanEval. Models trained on our bi-directional trace-grounded data achieve substantial improvements, with gains of up to 30 points on output prediction and 28 points on input prediction over base models, alongside improved explanation and code generation, demonstrating that verifiable reasoning fundamentally enhances model capabilities. https://github.ibm.com/IBM-Research-AI/Verified-Code-CoT", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u7a0b\u5e8f\u6267\u884c\u8f68\u8ff9\u751f\u6210\u53ef\u9a8c\u8bc1\u7684\u601d\u7ef4\u94fe\uff08CoT\uff09\u6570\u636e\uff0c\u4ee5\u89e3\u51b3\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u6267\u884c\u63a8\u7406\u4e2d\u7684\u903b\u8f91\u9519\u8bef\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u5408\u6210\u8bad\u7ec3\u6570\u636e\u5b58\u5728\u5173\u952e\u7f3a\u9677\uff1a\u63a8\u7406\u6b65\u9aa4\u662f\u6559\u5e08\u6a21\u578b\u751f\u6210\u7684\u770b\u4f3c\u5408\u7406\u4f46\u5b9e\u9645\u4e0a\u5e76\u672a\u9a8c\u8bc1\u7684\u4ee3\u7801\u89e3\u91ca\u3002\u8fd9\u5bfc\u81f4\u6a21\u578b\u5b66\u4f1a\u4e86\u6a21\u4eff\u8868\u9762\u4e0a\u4ee4\u4eba\u4fe1\u670d\u4f46\u5b9e\u9645\u4e0a\u903b\u8f91\u9519\u8bef\u7684\u63a8\u7406\u6a21\u5f0f\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u5c06CoT\u751f\u6210\u76f4\u63a5\u6839\u690d\u4e8e\u7a0b\u5e8f\u6267\u884c\u8f68\u8ff9\u4e2d\u3002\u5177\u4f53\u5730\uff0c\u7ba1\u9053\u5316\u5730\u6355\u83b7\u4ee3\u7801\u7684\u52a8\u6001\u884c\u4e3a\uff0c\u7136\u540e\u53d9\u8ff0\u8fd9\u4e9b\u7ecf\u8fc7\u9a8c\u8bc1\u7684\u6267\u884c\u8f68\u8ff9\uff0c\u751f\u6210\u81ea\u7136\u8bed\u8a00\u7684\u57fa\u672c\u539f\u7406\u3002", "result": "\u5728\u4ee3\u7801\u63a8\u7406\u4efb\u52a1\uff08CruxEval\u548cLiveCodeBench-Exec\u4e0a\u7684\u6b63\u5411\u63a8\u7406\uff0cCruxEval-Input\u4e0a\u7684\u53cd\u5411\u63a8\u7406\uff09\u4ee5\u53caHumanEval\u7684\u4ee3\u7801\u751f\u6210\u548c\u89e3\u91ca\u4efb\u52a1\u4e0a\u8bc4\u4f30\u4e86\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u3002\u4f7f\u7528\u53cc\u5411\u8ffd\u8e2a\u6570\u636e\u8bad\u7ec3\u7684\u6a21\u578b\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u9884\u6d4b\u8f93\u51fa\u548c\u8f93\u5165\u7684\u5f97\u5206\u5206\u522b\u63d0\u9ad8\u4e8630\u70b9\u548c28\u70b9\uff0c\u540c\u65f6\u751f\u6210\u89e3\u91ca\u548c\u4ee3\u7801\u7684\u8d28\u91cf\u4e5f\u6709\u6240\u63d0\u9ad8\u3002", "conclusion": "\u901a\u8fc7\u53ef\u9a8c\u8bc1\u7684\u601d\u7ef4\u94fe\u65b9\u6cd5\uff0c\u6a21\u578b\u5728\u4ee3\u7801\u6267\u884c\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u5f97\u5230\u4e86\u663e\u8457\u63d0\u9ad8\uff0c\u63a8\u7406\u903b\u8f91\u7684\u53ef\u9760\u6027\u4e5f\u5927\u5927\u589e\u5f3a\u3002"}}
{"id": "2512.00215", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.00215", "abs": "https://arxiv.org/abs/2512.00215", "authors": ["Mohammad Abdollahi", "Khandaker Rifah Tasnia", "Soumit Kanti Saha", "Jinqiu Yang", "Song Wang", "Hadi Hemmati"], "title": "Demystifying Errors in LLM Reasoning Traces: An Empirical Study of Code Execution Simulation", "comment": null, "summary": "Understanding a program's runtime reasoning behavior, meaning how intermediate states and control flows lead to final execution results, is essential for reliable code generation, debugging, and automated reasoning. Although large language models (LLMs) can accurately predict program outputs, most prior work has focused on output accuracy and performance, treating reasoning as a black box. As a result, little is known about the structure or failure modes of their reasoning traces. To address this gap, we conduct the first empirical study on runtime behavior inference with reasoning LLMs, aiming to uncover and characterize errors in their reasoning traces. We curate a benchmark from HumanEval Plus and LiveCodeBench, containing 427 code snippets. For each snippet, we test three input types: regular, edge, and invalid. Twelve input values are selected per snippet, each paired with its ground-truth execution result. We evaluate four state-of-the-art reasoning LLMs. Our results show that these models reach accuracies between 85 percent and 98 percent across input types. We also analyze the produced reasoning traces and develop a taxonomy with nine categories of inference errors. Finally, we explore tool-augmented reasoning. Using failures in the Computation Errors category as a case study, our experiments show that this approach corrects 58 percent of such errors, demonstrating the potential of tool support for improving LLM reasoning.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u5bf9LLMs\u7684\u8fd0\u884c\u65f6\u884c\u4e3a\u63a8\u7406\u8fdb\u884c\u4e86\u5b9e\u8bc1\u7814\u7a76\uff0c\u65e8\u5728\u63ed\u793a\u5e76\u8868\u5f81\u5176\u63a8\u7406\u8fc7\u7a0b\u9519\u8bef\u7c7b\u578b\u3002", "motivation": "\u6587\u7ae0\u63d0\u5230\u867d\u7136LLMs\u5728\u9884\u6d4b\u7a0b\u5e8f\u8f93\u51fa\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u662f\u5173\u4e8e\u5176\u63a8\u7406\u8fc7\u7a0b\u7684\u7ed3\u6784\u548c\u5931\u8d25\u6a21\u5f0f\u4e86\u89e3\u751a\u5c11\u3002", "method": "\u4eceHumanEval Plus \u548c LiveCodeBench\u9009\u53d6\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5305\u542b427\u4e2a\u4ee3\u7801\u7247\u6bb5\uff0c\u4e3a\u6bcf\u4e2a\u7247\u6bb5\u6d4b\u8bd5\u4e09\u79cd\u8f93\u5165\u7c7b\u578b\uff0c\u5e76\u9009\u53d64\u79cd\u6700\u5148\u8fdb\u7684\u63a8\u7406LLMs\u8fdb\u884c\u8bc4\u4f30\uff0c\u5e76\u63d0\u51fa\u4e00\u4e2a\u5305\u542b9\u7c7b\u63a8\u7406\u9519\u8bef\u5206\u7c7b\u6cd5\u3002", "result": "\u8fd9\u4e9bLLMs\u5728\u8f93\u5165\u7c7b\u578b\u4e0a\u7684\u51c6\u786e\u7387\u572885%\u523098%\u4e4b\u95f4\u30029\u7c7b\u63a8\u7406\u9519\u8bef\u88ab\u5b9a\u4e49\uff0c\u8fd8\u63a2\u7d22\u4e86\u5de5\u5177\u589e\u5f3a\u63a8\u7406\u65b9\u6cd5\uff0c\u5de5\u5177\u652f\u6301\u4fee\u6b63\u4e8658%\u7684'\u8ba1\u7b97\u9519\u8bef'\u7c7b\u95ee\u9898\uff0c\u663e\u793a\u5de5\u5177\u652f\u6301\u7684\u6f5c\u529b\u3002", "conclusion": "\u5de5\u5177\u589e\u5f3a\u63a8\u7406\u53ef\u4ee5\u663e\u8457\u6539\u5584LLMs\u63a8\u7406\u7684\u6027\u80fd\u548c\u51c6\u786e\u6027\uff0c\u4e3a\u672a\u6765\u63a2\u7d22\u5de5\u5177\u652f\u6301LLM\u63a8\u7406\u7684\u65b9\u5411\u63d0\u4f9b\u4f9d\u636e\u3002"}}
{"id": "2512.00094", "categories": ["cs.CR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.00094", "abs": "https://arxiv.org/abs/2512.00094", "authors": ["Kexin Li", "Guozhen Ding", "Ilya Grishchenko", "David Lie"], "title": "HMARK: Radioactive Multi-Bit Semantic-Latent Watermarking for Diffusion Models", "comment": null, "summary": "Modern generative diffusion models rely on vast training datasets, often including images with uncertain ownership or usage rights. Radioactive watermarks -- marks that transfer to a model's outputs -- can help detect when such unauthorized data has been used for training. Moreover, aside from being radioactive, an effective watermark for protecting images from unauthorized training also needs to meet other existing requirements, such as imperceptibility, robustness, and multi-bit capacity. To overcome these challenges, we propose HMARK, a novel multi-bit watermarking scheme, which encodes ownership information as secret bits in the semantic-latent space (h-space) for image diffusion models. By leveraging the interpretability and semantic significance of h-space, ensuring that watermark signals correspond to meaningful semantic attributes, the watermarks embedded by HMARK exhibit radioactivity, robustness to distortions, and minimal impact on perceptual quality. Experimental results demonstrate that HMARK achieves 98.57% watermark detection accuracy, 95.07% bit-level recovery accuracy, 100% recall rate, and 1.0 AUC on images produced by the downstream adversarial model finetuned with LoRA on watermarked data across various types of distortions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u591a\u6bd4\u7279\u6c34\u5370\u65b9\u6848HMARK\uff0c\u7528\u4e8e\u5728\u56fe\u50cf\u6269\u6563\u6a21\u578b\u4e2d\u5d4c\u5165\u6240\u6709\u6743\u4fe1\u606f\u3002\u8be5\u65b9\u6848\u901a\u8fc7\u5728\u8bed\u4e49\u6f5c\u5728\u7a7a\u95f4\uff08h-space\uff09\u4e2d\u7f16\u7801\u6c34\u5370\u4fe1\u606f\uff0c\u4ee5\u5b9e\u73b0\u6c34\u5370\u7684\u653e\u5c04\u6027\u3001\u9c81\u68d2\u6027\u548c\u5bf9\u611f\u77e5\u8d28\u91cf\u7684\u6700\u5c0f\u5f71\u54cd\u3002\u5b9e\u9a8c\u8bc1\u660e\u5728\u5404\u79cd\u5931\u771f\u7c7b\u578b\u4e0b\uff0cHMARK\u5177\u6709\u5f88\u9ad8\u7684\u6c34\u5370\u68c0\u6d4b\u51c6\u786e\u7387\u3001\u6bd4\u7279\u7ea7\u6062\u590d\u51c6\u786e\u7387\u3001\u53ec\u56de\u7387\u548cAUC\u503c\u3002", "motivation": "\u73b0\u4ee3\u7684\u751f\u6210\u6269\u6563\u6a21\u578b\u4f9d\u8d56\u4e8e\u5927\u89c4\u6a21\u8bad\u7ec3\u6570\u636e\u96c6\uff0c\u5176\u4e2d\u53ef\u80fd\u5305\u542b\u672a\u7ecf\u6388\u6743\u6216\u4f7f\u7528\u6743\u4e0d\u786e\u5b9a\u7684\u56fe\u50cf\u3002\u6c34\u5370\u6280\u672f\u53ef\u4ee5\u5e2e\u52a9\u68c0\u6d4b\u6a21\u578b\u8bad\u7ec3\u662f\u5426\u4f7f\u7528\u4e86\u8fd9\u4e9b\u672a\u7ecf\u6388\u6743\u7684\u6570\u636e\u3002\u6709\u6548\u7684\u56fe\u50cf\u4fdd\u62a4\u6c34\u5370\u9664\u4e86\u8981\u5177\u5907\u653e\u5c04\u6027\uff0c\u8fd8\u9700\u6ee1\u8db3\u9690\u5f62\u6027\u3001\u9c81\u68d2\u6027\u548c\u591a\u6bd4\u7279\u5bb9\u91cf\u7b49\u8981\u6c42\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u591a\u6bd4\u7279\u6c34\u5370\u65b9\u6848HMARK\uff0c\u8be5\u65b9\u6848\u5c06\u6240\u6709\u6743\u4fe1\u606f\u4f5c\u4e3a\u79d8\u5bc6\u6bd4\u7279\u7f16\u7801\u5728\u56fe\u50cf\u6269\u6563\u6a21\u578b\u7684\u8bed\u4e49\u6f5c\u5728\u7a7a\u95f4\uff08h-space\uff09\u4e2d\u3002\u901a\u8fc7\u5229\u7528h-space\u7684\u53ef\u89e3\u91ca\u6027\u548c\u8bed\u4e49\u610f\u4e49\uff0c\u786e\u4fdd\u6c34\u5370\u4fe1\u53f7\u5bf9\u5e94\u6709\u610f\u4e49\u7684\u8bed\u4e49\u5c5e\u6027\uff0c\u4ece\u800c\u5b9e\u73b0\u6c34\u5370\u7684\u653e\u5c04\u6027\u3001\u6297\u626d\u66f2\u548c\u51cf\u5c11\u5bf9\u611f\u77e5\u8d28\u91cf\u7684\u5f71\u54cd\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5728\u5404\u79cd\u5931\u771f\u7c7b\u578b\u4e0b\uff0cHMARK\u5728\u901a\u8fc7\u4e0b\u6e38\u5bf9\u6297\u6a21\u578b\uff08\u7528LoRA\u5728\u6c34\u5370\u6570\u636e\u4e0a\u5fae\u8c03\uff09\u751f\u6210\u7684\u56fe\u50cf\u4e0a\u5b9e\u73b0\u4e8698.57%\u7684\u6c34\u5370\u68c0\u6d4b\u51c6\u786e\u7387\u300195.07%\u7684\u6bd4\u7279\u7ea7\u6062\u590d\u51c6\u786e\u7387\u3001100%\u7684\u53ec\u56de\u7387\u548c1.0\u7684AUC\u503c\u3002", "conclusion": "HMARK\u662f\u4e00\u79cd\u6709\u6548\u7684\u591a\u6bd4\u7279\u6c34\u5370\u65b9\u6848\uff0c\u80fd\u591f\u5f88\u597d\u5730\u4fdd\u62a4\u56fe\u50cf\u514d\u53d7\u672a\u7ecf\u6388\u6743\u7684\u8bad\u7ec3\u4f7f\u7528\u3002\u901a\u8fc7\u5728\u8bed\u4e49\u6f5c\u5728\u7a7a\u95f4\u5d4c\u5165\u6c34\u5370\u4fe1\u606f\uff0cHMARK\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u6c34\u5370\u68c0\u6d4b\u3001\u826f\u597d\u7684\u6bd4\u7279\u7ea7\u6062\u590d\u3001\u5b8c\u7f8e\u7684\u53ec\u56de\u7387\u548cAUC\u503c\uff0c\u5728\u5404\u79cd\u5931\u771f\u4e0b\u8868\u73b0\u826f\u597d\u3002"}}
{"id": "2512.00231", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00231", "abs": "https://arxiv.org/abs/2512.00231", "authors": ["Monique Louise Monteiro", "George G. Cabral", "Adriano L. I. OLiveira"], "title": "CodeFlowLM: Incremental Just-In-Time Defect Prediction with Pretrained Language Models and Exploratory Insights into Defect Localization", "comment": null, "summary": "This work introduces CodeFlowLM, an incremental learning framework for Just-In-Time Software Defect Prediction (JIT-SDP) that leverages pre-trained language models (PLMs). Unlike traditional online learners, CodeFlowLM employs continual fine-tuning to address concept drift, class imbalance, and verification latency without retraining from scratch. We evaluated encoder-only and encoder-decoder PLMs (notably CodeT5+ and UniXCoder) in JIT-SDP scenarios within and between projects, comparing them with the incremental baseline BORB. The results show that CodeFlowLM achieves up to 68% G-Mean gains, confirming its superior adaptability and robustness in evolving software environments. We further extend the analysis to Just-in-Time Defect Localization (JIT-DL), benchmarking Large Language Models (LLMs) such as GPT-5, Claude Sonnet 4.5, and Gemini 2.5 Pro against attention-based models. GPT-5 delivers comparable performance for Recall@20% and Effort@20% with higher stability, although attention-based methods retain an advantage in fine-grained ranking metrics (Top-k, IFA). A qualitative error analysis reveals that most false positives arise from (1) human-like conservative bias, (2) insufficient contextual information in diff-based prompts, and (3) potential dataset mislabeling in JIT-Defects4J. These findings highlight both the promise and the current limitations of LLM reasoning in defect localization. False negatives occur in smaller proportions. Overall, CodeFlowLM significantly advances the state of the art in incremental JIT-SDP, demonstrating superior adaptability and robustness in evolving software environments. Furthermore, our exploratory analysis of LLMs in JIT-DL not only benchmarks their performance against established attention-based models but also provides critical insights into the current limitations of prompt-based defect reasoning.", "AI": {"tldr": "\u6b64\u8bba\u6587\u63d0\u51fa\u4e86 CodeFlowLM \uff0c\u4e00\u4e2a\u5229\u7528\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7684\u589e\u91cf\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u5373\u65f6\u8f6f\u4ef6\u7f3a\u9677\u9884\u6d4b\u3002\u4e0e\u4f20\u7edf\u7684\u5728\u7ebf\u5b66\u4e60\u4e0d\u540c\uff0c CodeFlowLM \u4f7f\u7528\u6301\u7eed\u5fae\u8c03\u5904\u7406\u6982\u5ff5\u6f02\u79fb\u3001\u7c7b\u522b\u4e0d\u5e73\u8861\u548c\u9a8c\u8bc1\u5ef6\u8fdf\uff0c\u800c\u65e0\u9700\u4ece\u5934\u5f00\u59cb\u91cd\u65b0\u8bad\u7ec3\u3002\u901a\u8fc7\u5728\u9879\u76ee\u5185\u548c\u8de8\u9879\u76ee\u7684 JIT-SDP \u573a\u666f\u4e2d\u5bf9\u4ec5\u7f16\u7801\u5668\u548c\u7f16\u7801\u5668-\u89e3\u7801\u5668\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\uff08\u5c24\u5176\u662f CodeT5+ \u548c UniXCoder\uff09\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u5e76\u4e0e\u589e\u91cf\u57fa\u7ebf BORB \u8fdb\u884c\u4e86\u6bd4\u8f83\u3002\u7ed3\u679c\u663e\u793a\uff0c CodeFlowLM \u5728\u9002\u5e94\u6027\u3001\u9c81\u68d2\u6027\u4e0a\u8868\u73b0\u51fa\u8272\uff0c G-Mean \u6536\u76ca\u9ad8\u8fbe 68%\u3002\u7814\u7a76\u8fdb\u4e00\u6b65\u5c06\u5206\u6790\u5ef6\u4f38\u81f3\u5373\u65f6\u7f3a\u9677\u5b9a\u4f4d\uff08JIT-DL\uff09\uff0c\u4ee5\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u6a21\u578b\u4e3a\u57fa\u51c6\u8bc4\u6d4b\u4e86 GPT-5 \u3001 Claude Sonnet 4.5 \u548c Gemini 2.5 Pro \u7b49\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u3002\u901a\u8fc7\u5b9a\u6027\u8bef\u5dee\u5206\u6790\u53d1\u73b0\uff0c\u5927\u591a\u6570\u5047\u9633\u6027\u6e90\u4e8e\uff1a1 ) \u7c7b\u4f3c\u4eba\u7c7b\u7684\u4fdd\u5b88\u504f\u89c1\uff0c2 ) \u57fa\u4e8ediff\u7684\u63d0\u793a\u4e2d\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\u4e0d\u8db3\uff0c3 ) JIT-Defects4J \u4e2d\u7684\u6f5c\u5728\u6570\u636e\u96c6\u9519\u8bef\u6807\u8bb0\u3002\u5047\u9634\u6027\u5728\u8f83\u5c0f\u6bd4\u4f8b\u4e2d\u51fa\u73b0\u3002\u603b\u4f53\u800c\u8a00\uff0c CodeFlowLM \u5728\u81ea\u9002\u5e94\u5373\u65f6 SDP \u65b9\u9762\u663e\u8457\u63d0\u9ad8\u4e86\u6280\u672f\u6c34\u5e73\u3002\u6b64\u5916\uff0c\u5bf9JIT-DL \u4e2dLLMs\u7684\u63a2\u7d22\u6027\u5206\u6790\u4e0d\u4ec5\u5c06\u5176\u6027\u80fd\u4e0e\u6210\u719f\u7684\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u6a21\u578b\u8fdb\u884c\u4e86\u5bf9\u6bd4\uff0c\u8fd8\u6df1\u5165\u63ed\u793a\u4e86\u57fa\u4e8e\u63d0\u793a\u63a8\u7406\u7684\u5f53\u524d\u5c40\u9650\u6027\u3002", "motivation": "\u8fd9\u9879\u7814\u7a76\u65e8\u5728\u89e3\u51b3 JIT-SDP \u4e2d\u7684\u6982\u5ff5\u6f02\u79fb\u3001\u7c7b\u522b\u4e0d\u5e73\u8861\u3001\u9a8c\u8bc1\u5ef6\u8fdf\u7684\u95ee\u9898\uff0c\u5e76\u63a2\u7d22LLMs\u5728 JIT-DL \u4e2d\u7684\u5e94\u7528\u3002", "method": "\u7814\u7a76\u65b9\u6cd5\u4e3b\u8981\u5305\u62ec\uff1a1 ) \u63d0\u51fa CodeFlowLM \u6846\u67b6; 2 )\u901a\u8fc7\u4e0d\u65ad\u5fae\u8c03\u7684\u65b9\u6cd5\u6765\u5e94\u5bf9\u4e00\u7cfb\u5217\u6311\u6218;3 ) \u8bc4\u4f30\u591a\u4e2a\u8bed\u8a00\u6a21\u578b\u5e76\u4e0e\u57fa\u7ebf\u65b9\u6cd5\u5bf9\u6bd4;4 ) JIT-DL\u4e2d\u7684LLMs\u5206\u6790\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff1a1 ) CodeFlowLM \u5728 JIT-SDP \u4e2d\u7684\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u57fa\u7ebf\u65b9\u6cd5\uff0cG-Mean\u589e\u76ca\u9ad8\u8fbe68\uff05;2) GPT-5\u5728JIT-DL\u4e2d\u8868\u73b0\u76f8\u5f53;3)\u5047\u9633\u6027\u7684\u4e3b\u8981\u6765\u6e90\u5728\u4e8e\u4eba\u7c7b\u4f3c\u7684\u4fdd\u5b88\u504f\u89c1\u3001\u57fa\u4e8ediff\u7684\u63d0\u793a\u7f3a\u4e4f\u8db3\u591f\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\u3001\u4ee5\u53ca\u6570\u636e\u96c6\u8bef\u6807\u6ce8\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u5b9e\u4e86 CodeFlowLM \u5728\u9762\u5bf9\u52a8\u6001\u53d8\u5316\u7684\u8f6f\u4ef6\u73af\u5883\u65f6\uff0c\u5177\u5907\u66f4\u4f18\u79c0\u7684\u9002\u5e94\u6027\u548c\u7a33\u5065\u6027\uff0c\u660e\u663e\u63d0\u5347\u4e86\u589e\u91cf\u5f0f JIT-SDP \u7684\u6280\u672f\u6807\u51c6\u3002\u540c\u65f6\uff0c\u901a\u8fc7\u5bf9 JIT-DL \u4e2d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a2\u7a76\u6027\u5206\u6790\uff0c\u4e0d\u4ec5\u5c06\u5b83\u4eec\u4e0e\u6210\u719f\u7684\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u6a21\u578b\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8fd8\u6df1\u5165\u63ed\u793a\u4e86\u57fa\u4e8e\u63d0\u793a\u63a8\u7406\u7684\u73b0\u6709\u5c40\u9650\u6027\u3002"}}
{"id": "2512.00398", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.00398", "abs": "https://arxiv.org/abs/2512.00398", "authors": ["Bingzheng Xia", "Zujie Ren", "Kuang Ma", "Xiaoqian Li", "Wenda Li", "Shuibing He"], "title": "Heimdall++: Optimizing GPU Utilization and Pipeline Parallelism for Efficient Single-Pulse Detection", "comment": null, "summary": "With the increasing time and frequency resolution of modern radio telescopes and the exponential growth in observational data volumes, real-time single-pulse detection has become a critical requirement for time-domain radio astronomy. Heimdall, as a representative GPU-accelerated single-pulse search tool, offers substantial performance advantages over CPU-based approaches. However, its sequential execution model and resource contention in intermediate processing stages limit GPU utilization, leading to suboptimal throughput and increased computational latency. To address these limitations, we present Heimdall++, an optimized successor to Heimdall that incorporates fine-grained GPU parallelization, enhanced memory management, and a multi-threaded framework to decouple CPU-bound and GPU-bound processing stages. This design mitigates the GPU stall problem and improves end-to-end efficiency. We evaluated Heimdall++ on a system equipped with NVIDIA RTX 3080 Ti GPUs using both a single large-scale observational file and multiple files. Experimental results demonstrate that Heimdall++ achieves up to 2.66x speedup in single-file processing and 2.05x speedup in multi-file batch processing, while maintaining full consistency with the original Heimdall's search results.", "AI": {"tldr": "Heimdall++\u662f\u4e00\u79cd\u4f18\u5316\u7684GPU\u52a0\u901f\u5355\u8109\u51b2\u641c\u7d22\u5de5\u5177\uff0c\u901a\u8fc7\u7ec6\u7c92\u5ea6GPU\u5e76\u884c\u5316\u3001\u6539\u8fdb\u7684\u5185\u5b58\u7ba1\u7406\u548c\u591a\u7ebf\u7a0b\u6846\u67b6\u6765\u63d0\u9ad8\u6548\u7387\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cHeimdall++\u5728\u5355\u6587\u4ef6\u5904\u7406\u4e0a\u8fbe\u52302.66\u500d\u52a0\u901f\uff0c\u591a\u6587\u4ef6\u6279\u5904\u7406\u4e0a\u8fbe\u52302.05\u500d\u52a0\u901f\uff0c\u4e0e\u539fHeimdall\u7ed3\u679c\u4fdd\u6301\u4e00\u81f4\u3002", "motivation": "\u73b0\u4ee3\u5c04\u7535\u671b\u8fdc\u955c\u5bf9\u5b9e\u65f6\u5355\u8109\u51b2\u68c0\u6d4b\u63d0\u51fa\u4e86\u66f4\u9ad8\u8981\u6c42\uff0c\u800cHeimdall\u5728\u4e2d\u95f4\u5904\u7406\u9636\u6bb5\u5b58\u5728\u8d44\u6e90\u7ade\u4e89\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5176\u6027\u80fd\u3002", "method": "\u901a\u8fc7\u7ec6\u7c92\u5ea6GPU\u5e76\u884c\u5316\u3001\u6539\u8fdb\u7684\u5185\u5b58\u7ba1\u7406\u548c\u591a\u7ebf\u7a0b\u6846\u67b6\u6765\u4f18\u5316Heimdall\uff0c\u5e76\u89e3\u8026CPU\u548cGPU\u7684\u5904\u7406\u9636\u6bb5\u3002", "result": "Heimdall++\u5728\u5355\u6587\u4ef6\u5904\u7406\u4e0a\u8fbe\u52302.66\u500d\u52a0\u901f\uff0c\u591a\u6587\u4ef6\u6279\u5904\u7406\u4e0a\u8fbe\u52302.05\u500d\u52a0\u901f\uff0c\u4fdd\u6301\u4e86\u4e0e\u539fHeimdall\u7684\u7ed3\u679c\u4e00\u81f4\u6027\u3002", "conclusion": "\u4f18\u5316\u540e\u7684Heimdall++\u663e\u8457\u63d0\u9ad8\u4e86GPU\u5229\u7528\u7387\u548c\u6574\u4f53\u541e\u5410\u91cf\uff0c\u89e3\u51b3\u4e86\u539fHeimdall\u7684\u6027\u80fd\u74f6\u9888\u3002"}}
{"id": "2512.00185", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.00185", "abs": "https://arxiv.org/abs/2512.00185", "authors": ["Shashanka B R", "Mohith Charan R", "Seema Banu F"], "title": "Chunking Strategies for Multimodal AI Systems", "comment": "45 pages, 5 figure", "summary": "Our goal is to consolidate the landscape of multimodal chunking strategies, providing researchers and practitioners with a technical foundation and design space for developing more effective and efficient multimodal AI systems. This survey paves the way for innovations in robust chunking pipelines that scale with modality complexity, enhance processing accuracy, and improve generative coherence in real-world applications. This survey provides a comprehensive taxonomy and technical analysis of chunking strategies tailored for each modality: text, images, audio, video, and cross-modal data. We examine classical and modern approaches such as fixed-size token windowing, recursive text splitting, object-centric visual chunking, silence-based audio segmentation, and scene detection in videos. Each approach is analyzed in terms of its underlying methodology, supporting tools (e.g., LangChain, Detectron2, PySceneDetect), benefits, and challenges, particularly those related to granularity-context trade-offs and multimodal alignment. Furthermore, we explore emerging cross-modal chunking strategies that aim to preserve alignment and semantic consistency across disparate data types [4]. We also include comparative insights, highlight open problems such as asynchronous information density and noisy alignment signals, and identify opportunities for future research in adaptive, learning-based, and task-specific chunking.", "AI": {"tldr": "\u672c\u6587\u8c03\u67e5\u5e76\u5206\u7c7b\u4e86\u9002\u7528\u4e8e\u6587\u672c\u3001\u56fe\u50cf\u3001\u97f3\u9891\u3001\u89c6\u9891\u548c\u8de8\u6a21\u6001\u6570\u636e\u7684\u5206\u5757\u7b56\u7565\uff0c\u5206\u6790\u7ecf\u5178\u4e0e\u73b0\u4ee3\u65b9\u6cd5\uff0c\u5e76\u63a2\u8ba8\u4e86\u7c92\u5ea6-\u4e0a\u4e0b\u6587\u6743\u8861\u548c\u591a\u6a21\u6001\u5bf9\u9f50\u7b49\u6311\u6218\uff0c\u7a81\u51fa\u663e\u793a\u4e86\u5f00\u653e\u95ee\u9898\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u4e3a\u7814\u7a76\u4eba\u5458\u548c\u4ece\u4e1a\u8005\u63d0\u4f9b\u591a\u6a21\u6001\u5206\u5757\u7b56\u7565\u7684\u6280\u672f\u57fa\u7840\u548c\u5f00\u53d1\u7a7a\u95f4\uff0c\u4ee5\u5f00\u53d1\u66f4\u6709\u6548\u7684\u591a\u6a21\u6001 AI \u7cfb\u7edf\uff0c\u63a8\u52a8\u9c81\u68d2\u5206\u5757\u7ba1\u9053\u7684\u521b\u65b0\uff0c\u9002\u5e94\u6a21\u6001\u590d\u6742\u6027\uff0c\u63d0\u9ad8\u5904\u7406\u51c6\u786e\u6027\u548c\u751f\u6210\u4e00\u81f4\u6027\u3002", "method": "\u63d0\u4f9b\u4e86\u9488\u5bf9\u6bcf\u79cd\u6a21\u6001\uff08\u6587\u672c\u3001\u56fe\u50cf\u3001\u97f3\u9891\u3001\u89c6\u9891\u548c\u8de8\u6a21\u6001\u6570\u636e\uff09\u7684\u5206\u5757\u7b56\u7565\u5168\u9762\u5206\u7c7b\u548c\u6280\u672f\u5206\u6790\uff0c\u8003\u5bdf\u4e86\u56fa\u5b9a\u5927\u5c0f\u4ee4\u724c\u7a97\u53e3\u3001\u9012\u5f52\u6587\u672c\u5206\u5272\u3001\u4ee5\u5bf9\u8c61\u4e3a\u4e2d\u5fc3\u7684\u89c6\u89c9\u5206\u5757\u3001\u57fa\u4e8e\u9759\u97f3\u7684\u97f3\u9891\u5206\u6bb5\u548c\u89c6\u9891\u573a\u666f\u68c0\u6d4b\u7b49\u7ecf\u5178\u548c\u73b0\u4ee3\u65b9\u6cd5\uff0c\u5206\u6790\u6bcf\u79cd\u65b9\u6cd5\u7684\u57fa\u672c\u65b9\u6cd5\u3001\u652f\u6301\u5de5\u5177\u3001\u4f18\u70b9\u548c\u6311\u6218\uff0c\u7279\u522b\u662f\u7c92\u5ea6-\u4e0a\u4e0b\u6587\u6743\u8861\u548c\u591a\u6a21\u6001\u5bf9\u9f50\u76f8\u5173\u95ee\u9898\uff0c\u5e76\u63a2\u8ba8\u4e86\u65e8\u5728\u4fdd\u6301\u4e0d\u540c\u6570\u636e\u7c7b\u578b\u4e4b\u95f4\u7684\u5bf9\u9f50\u548c\u8bed\u4e49\u4e00\u81f4\u6027\u7684\u65b0\u5174\u8de8\u6a21\u6001\u5206\u5757\u7b56\u7565\u3002", "result": "\u5305\u542b\u6bd4\u8f83\u89c1\u89e3\uff0c\u7a81\u51fa\u5f00\u653e\u95ee\u9898\uff0c\u5982\u5f02\u6b65\u4fe1\u606f\u5bc6\u5ea6\u548c\u566a\u58f0\u5bf9\u9f50\u4fe1\u53f7\uff0c\u5e76\u786e\u5b9a\u4e86\u672a\u6765\u5728\u81ea\u9002\u5e94\u3001\u57fa\u4e8e\u5b66\u4e60\u548c\u7279\u5b9a\u4efb\u52a1\u5206\u5757\u65b9\u9762\u7684\u7814\u7a76\u673a\u4f1a\u3002", "conclusion": "\u8be5\u7efc\u8ff0\u901a\u8fc7\u63d0\u4f9b\u5168\u9762\u7684\u6280\u672f\u5206\u6790\u548c\u5206\u7c7b\u6cd5\uff0c\u4e3a\u66f4\u590d\u6742\u548c\u9ad8\u6548\u7684\u591a\u6a21\u6001 AI \u7cfb\u7edf\u53d1\u5c55\u5960\u5b9a\u4e86\u575a\u5b9e\u7684\u57fa\u7840\uff0c\u5e76\u5f3a\u8c03\u4e86\u7ee7\u7eed\u63a2\u7d22\u5982\u81ea\u9002\u5e94\u548c\u5b66\u4e60\u578b\u5206\u5757\u6280\u672f\u7684\u91cd\u8981\u6027\uff0c\u4ee5\u89e3\u51b3\u5f53\u524d\u65b9\u6cd5\u4e2d\u7684\u7c92\u5ea6-\u4e0a\u4e0b\u6587\u5bf9\u9f50\u6311\u6218\u548c\u5f00\u653e\u95ee\u9898\u3002"}}
{"id": "2512.00595", "categories": ["cs.DC", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2512.00595", "abs": "https://arxiv.org/abs/2512.00595", "authors": ["Bala Siva Sai Akhil Malepati"], "title": "IslandRun: Privacy-Aware Multi-Objective Orchestration for Distributed AI Inference", "comment": "15 pages, 3 figures, 2 tables", "summary": "Modern AI inference faces an irreducible tension: no single computational resource simultaneously maximizes performance, preserves privacy, minimizes cost, and maintains trust. Existing orchestration frameworks optimize single dimensions (Kubernetes prioritizes latency, federated learning preserves privacy, edge computing reduces network distance), creating solutions that struggle under real-world heterogeneity. We present IslandRun, a multi-objective orchestration system that treats computational resources as autonomous \"islands\" spanning personal devices, private edge servers, and public cloud. Our key insights: (1) request-level heterogeneity demands policy-constrained multi-objective optimization, (2) data locality enables routing compute to data rather than data to compute, and (3) typed placeholder sanitization preserves context semantics across trust boundaries. IslandRun introduces agent-based routing, tiered island groups with differential trust, and reversible anonymization. This establishes a new paradigm for privacy-aware, decentralized inference orchestration across heterogeneous personal computing ecosystems.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2512.00110", "categories": ["cs.CR", "cs.DC"], "pdf": "https://arxiv.org/pdf/2512.00110", "abs": "https://arxiv.org/abs/2512.00110", "authors": ["Leo Kao"], "title": "Quantum-Adversary-Resilient Evidence Structures and Migration Strategies for Regulated AI Audit Trails", "comment": "15 pages, 1 table. Technical report for regulated AI audit trails with quantum-adversary security definitions and migration patterns", "summary": "Constant-size cryptographic evidence records are increasingly used to build audit trails for regulated AI workloads in clinical, pharmaceutical, and financial settings, where each execution is summarized by a compact, verifiable record of code identity, model version, data digests, and platform measurements. Existing instantiations, however, typically rely on classical signature schemes whose long-term security is threatened by quantum-capable adversaries. In this paper we formalize security notions for evidence structures in the presence of quantum adversaries and study post-quantum (PQ) instantiations and migration strategies for deployed audit logs. We recall an abstraction of constant-size evidence structures and introduce game-based definitions of Q-Audit Integrity, Q-Non-Equivocation, and Q-Binding, capturing the inability of a quantum adversary to forge, equivocate, or rebind evidence items. We then analyze a hash-and-sign instantiation in the quantum random-oracle model (QROM), assuming an existentially unforgeable PQ signature scheme against quantum adversaries, and show that the resulting evidence structure satisfies these notions under standard assumptions. Building on this, we present three migration patterns for existing evidence logs: hybrid signatures, re-signing of legacy evidence, and Merkle-root anchoring, and analyze their security, storage, and computational trade-offs. A case study based on an industrial constant-size evidence platform for regulated AI at Codebat Technologies Inc. suggests that quantum-safe audit trails are achievable with moderate overhead and that systematic migration can significantly extend the evidentiary lifetime of existing deployments.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u9488\u5bf9\u91cf\u5b50\u654c\u624b\u7684\u5b89\u5168\u8bc1\u636e\u7ed3\u6784\u53ca\u5176\u540e\u91cf\u5b50\u5b9e\u73b0\u4e0e\u8fc1\u79fb\u7b56\u7565\uff0c\u7528\u4e8e\u53d7\u76d1\u7ba1AI\u7684\u53ef\u5ba1\u8ba1\u8ffd\u8e2a\u3002", "motivation": "\u4f20\u7edf\u7b7e\u540d\u65b9\u6848\u6784\u5efa\u7684\u6052\u5b9a\u5927\u5c0f\u52a0\u5bc6\u8bc1\u636e\u8bb0\u5f55\u5728\u4e34\u5e8a\u3001\u5236\u836f\u548c\u91d1\u878d\u9886\u57df\u5b58\u5728\u91cf\u5b50\u653b\u51fb\u4e0b\u7684\u957f\u671f\u5b89\u5168\u9690\u60a3\uff0c\u9700\u8bbe\u8ba1\u540e\u91cf\u5b50\u5b89\u5168\u7684\u8bc1\u636e\u7ed3\u6784\u4e0e\u8fc1\u79fb\u65b9\u6848\u3002", "method": "\u5f15\u5165\u57fa\u4e8e\u6e38\u620f\u7684Q-Audit Integrity\u3001Q-Non-Equivocation\u548cQ-Binding\u5b89\u5168\u5b9a\u4e49\uff0c\u5206\u6790\u57fa\u4e8eQROM\u7684hash-and-sign\u5b9e\u73b0\uff0c\u63d0\u51fa\u6df7\u5408\u7b7e\u540d\u3001\u8bc1\u636e\u91cd\u7b7e\u4e0e\u9ed8\u514b\u5c14\u6839\u7684\u8fc1\u79fb\u6a21\u5f0f\u3002", "result": "QROM\u4e0bhash-and-sign\u65b9\u6848\u6ee1\u8db3\u91cf\u5b50\u5b89\u5168\u8981\u6c42\uff1b\u4e09\u79cd\u8fc1\u79fb\u7b56\u7565\u5728\u5b89\u5168\u3001\u5b58\u50a8\u548c\u8ba1\u7b97\u95f4\u5177\u5907\u53ef\u884c\u6743\u8861\uff1b\u5de5\u4e1a\u6848\u4f8b\u663e\u793a\u91cf\u5b50\u5b89\u5168\u5ba1\u8ba1\u8ffd\u8e2a\u53ef\u8fbe\uff0c\u8fc1\u79fb\u80fd\u663e\u8457\u5ef6\u957f\u73b0\u6709\u8bc1\u636e\u5bff\u547d\u3002", "conclusion": "\u540e\u91cf\u5b50\u5b89\u5168\u5ba1\u8ba1\u8ffd\u8e2a\u53ef\u5728\u9002\u5ea6\u5f00\u9500\u4e0b\u5b9e\u73b0\uff0c\u7cfb\u7edf\u6027\u8fc1\u79fb\u662f\u4fdd\u62a4\u73b0\u6709\u8bc1\u636e\u7ed3\u6784\u7684\u6709\u6548\u8def\u5f84\u3002"}}
{"id": "2512.00193", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00193", "abs": "https://arxiv.org/abs/2512.00193", "authors": ["Anson Ho", "Jean-Stanislas Denain", "David Atanasov", "Samuel Albanie", "Rohin Shah"], "title": "A Rosetta Stone for AI Benchmarks", "comment": null, "summary": "Most AI benchmarks saturate within years or even months after they are introduced, making it hard to study long-run trends in AI capabilities. To address this challenge, we build a statistical framework that stitches benchmarks together, putting model capabilities and benchmark difficulties on a single numerical scale. This acts as a \"Rosetta Stone\", allowing us to compare models across a wide range of abilities and time, even if they are not evaluated on the same benchmarks. Moreover, this works without assuming how capabilities evolve across time or with training compute. We demonstrate three applications of this framework. First, we use it to measure the speed of AI progress over time, and to forecast future AI capabilities. Second, we estimate the rate of improvements in algorithmic efficiency, finding estimates that are higher, but broadly consistent with prior work. Finally, we find that our approach can be used to detect rapid accelerations in AI progress.", "AI": {"tldr": "\u5927\u591a\u6570AI\u57fa\u51c6\u6d4b\u8bd5\u5728\u51e0\u5e74\u751a\u81f3\u51e0\u4e2a\u6708\u540e\u5c31\u5931\u53bb\u65f6\u6548\u6027\uff0c\u5bfc\u81f4\u96be\u4ee5\u7814\u7a76AI\u80fd\u529b\u957f\u671f\u8d8b\u52bf\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u4e0d\u540c\u57fa\u51c6\u6d4b\u8bd5\u8fde\u63a5\u8d77\u6765\u7684\u7edf\u8ba1\u6846\u67b6\uff0c\u5c06\u6a21\u578b\u80fd\u529b\u548c\u57fa\u51c6\u96be\u5ea6\u653e\u5728\u5355\u4e00\u6570\u503c\u5c3a\u5ea6\u4e0a\uff0c\u5f62\u6210\u201c\u7f57\u585e\u5854\u77f3\u7891\u201d\uff0c\u5b9e\u73b0\u8de8\u80fd\u529b\u548c\u8de8\u65f6\u95f4\u7684\u6a21\u578b\u6bd4\u8f83\uff0c\u65e0\u9700\u5047\u8bbe\u80fd\u529b\u968f\u65f6\u95f4\u6216\u8bad\u7ec3\u8ba1\u7b97\u91cf\u7684\u53d8\u5316\u65b9\u5f0f\u3002\u6587\u7ae0\u5c55\u793a\u4e86\u8be5\u6846\u67b6\u7684\u4e09\u79cd\u5e94\u7528\uff1a1\uff09\u6d4b\u91cfAI\u8fdb\u5c55\u901f\u5ea6\u5e76\u9884\u6d4b\u672a\u6765\u80fd\u529b\uff1b2\uff09\u4f30\u7b97\u7b97\u6cd5\u6548\u7387\u63d0\u5347\u901f\u5ea6\uff0c\u7ed3\u679c\u9ad8\u4e8e\u4f46\u57fa\u672c\u7b26\u5408\u5148\u524d\u7814\u7a76\uff1b3\uff09\u68c0\u6d4bAI\u8fdb\u5c55\u7684\u7a81\u7136\u52a0\u901f\u3002", "motivation": "\u57fa\u51c6\u6d4b\u8bd5\u9971\u548c\u8fc7\u5feb\uff0c\u96be\u4ee5\u7814\u7a76\u957f\u671f\u8d8b\u52bf\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u8fde\u63a5\u4e0d\u540c\u57fa\u51c6\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u7edf\u8ba1\u6846\u67b6\uff0c\u5c06\u6a21\u578b\u80fd\u529b\u548c\u57fa\u51c6\u96be\u5ea6\u7edf\u4e00\u5230\u5355\u4e00\u5c3a\u5ea6\u4e0a\uff0c\u65e0\u9700\u9884\u8bbe\u80fd\u529b\u5982\u4f55\u968f\u65f6\u95f4\u548c\u8bad\u7ec3\u8ba1\u7b97\u91cf\u53d8\u5316\u3002", "result": "\u5c55\u793a\u4e86\u4e09\u79cd\u5e94\u7528\uff1a1\uff09\u6d4b\u91cf\u548c\u9884\u6d4bAI\u80fd\u529b\uff1b2\uff09\u4f30\u7b97\u7b97\u6cd5\u6548\u7387\uff0c\u7ed3\u679c\u9ad8\u4e8e\u5148\u524d\u7814\u7a76\u4f46\u57fa\u672c\u4e00\u81f4\uff1b3\uff09\u68c0\u6d4bAI\u8fdb\u5c55\u7684\u7a81\u7136\u52a0\u901f\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u6709\u6548\u8fde\u63a5\u4e0d\u540c\u57fa\u51c6\uff0c\u4e3a\u957f\u671f\u7814\u7a76\u548c\u5206\u6790AI\u80fd\u529b\u63d0\u4f9b\u65b0\u89c6\u89d2\u3002"}}
{"id": "2512.00325", "categories": ["cs.SE", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.00325", "abs": "https://arxiv.org/abs/2512.00325", "authors": ["Shaira Sadia Karim", "Abrar Mahmud Rahim", "Lamia Alam", "Ishmam Tashdeed", "Lutfun Nahar Lota", "Md. Abu Raihan M. Kamal", "Md. Azam Hossain"], "title": "Progressive Code Integration for Abstractive Bug Report Summarization", "comment": null, "summary": "Bug reports are often unstructured and verbose, making it challenging for developers to efficiently comprehend software issues. Existing summarization approaches typically rely on surface-level textual cues, resulting in incomplete or redundant summaries, and they frequently ignore associated code snippets, which are essential for accurate defect diagnosis. To address these limitations, we propose a progressive code-integration framework for LLM-based abstractive bug report summarization. Our approach incrementally incorporates long code snippets alongside textual content, overcoming standard LLM context window constraints and producing semantically rich summaries. Evaluated on four benchmark datasets using eight LLMs, our pipeline outperforms extractive baselines by 7.5%-58.2% and achieves performance comparable to state-of-the-art abstractive methods, highlighting the benefits of jointly leveraging textual and code information for enhanced bug comprehension.", "AI": {"tldr": "Bug reports \u901a\u5e38\u662f\u975e\u7ed3\u6784\u5316\u548c\u5197\u957f\u7684\uff0c\u8fd9\u4f7f\u5f97\u5f00\u53d1\u8005\u96be\u4ee5\u9ad8\u6548\u7406\u89e3\u95ee\u9898\u3002\u73b0\u6709\u6458\u8981\u65b9\u6cd5\u4f9d\u8d56\u8868\u9762\u6587\u672c\u63d0\u793a\uff0c\u5bfc\u81f4\u6458\u8981\u4e0d\u5b8c\u6574\u6216\u5197\u4f59\uff0c\u4e14\u5ffd\u7565\u5173\u952e\u4ee3\u7801\u7247\u6bb5\u3002\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e LLM \u7684\u9010\u6b65\u4ee3\u7801\u6574\u5408\u6846\u67b6\uff0c\u80fd\u591f\u8054\u5408\u5229\u7528\u6587\u672c\u548c\u4ee3\u7801\u7247\u6bb5\uff0c\u751f\u6210\u8bed\u4e49\u4e30\u5bcc\u7684\u6458\u8981\uff0c\u663e\u8457\u4f18\u4e8e\u63d0\u53d6\u57fa\u7ebf\uff0c\u5e76\u4e0e\u6700\u5148\u8fdb\u65b9\u6cd5\u6027\u80fd\u76f8\u5f53\u3002", "motivation": "Bug reports \u7684\u975e\u7ed3\u6784\u5316\u548c\u5197\u957f\u7279\u6027\u4f7f\u5f97\u5f00\u53d1\u8005\u96be\u4ee5\u9ad8\u6548\u7406\u89e3\u8f6f\u4ef6\u7f3a\u9677\uff0c\u7279\u522b\u662f\u5ffd\u7565\u4e86\u4ee3\u7801\u7247\u6bb5\u5bf9\u51c6\u786e\u8bca\u65ad\u7f3a\u9677\u7684\u91cd\u8981\u4f5c\u7528\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e LLM \u7684\u9010\u6b65\u4ee3\u7801\u6574\u5408\u6846\u67b6\uff0c\u4ee5\u8fed\u4ee3\u5730\u5408\u5e76\u957f\u4ee3\u7801\u7247\u6bb5\u4e0e\u6587\u672c\u5185\u5bb9\uff0c\u514b\u670d\u6807\u51c6 LLM \u4e0a\u4e0b\u6587\u7a97\u53e3\u9650\u5236\uff0c\u5e76\u751f\u6210\u8bed\u4e49\u4e30\u5bcc\u7684\u6458\u8981\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u548c\u516b\u4e2a LLM \u7684\u8bc4\u4f30\u4e2d\uff0c\u8be5\u65b9\u6cd5\u8d85\u51fa\u63d0\u53d6\u57fa\u7ebf 7.5%-58.2%\uff0c\u5e76\u4e0e\u6700\u5148\u8fdb\u62bd\u8c61\u65b9\u6cd5\u6027\u80fd\u76f8\u5f53\u3002", "conclusion": "\u8054\u5408\u5229\u7528\u6587\u672c\u548c\u4ee3\u7801\u4fe1\u606f\u4ee5\u589e\u5f3a\u5bf9 bug \u7684\u7406\u89e3\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u6458\u8981\u8d28\u91cf\uff0c\u5e76\u514b\u670d\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2512.00623", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.00623", "abs": "https://arxiv.org/abs/2512.00623", "authors": ["Basilis Mamalis", "Marios Perlitis"], "title": "Steady and Energy-Efficient Multi-Hop Clustering for Flying Ad-Hoc Networks (FANETs)", "comment": "7 pages, 5 figures, Accepted for publication in International Journal of Computer Applications (IJCA) - December 2025 Edition", "summary": "Flying Ad-hoc Networks (FANETs), formed by Unmanned Aerial Vehicles (UAVs), represent an emerging and promising communication paradigm. These networks face unique challenges due to UAVs high mobility, limited energy resources, and dynamic topology. In this work, we propose a novel multi-hop clustering algorithm aimed at creating stable, energy-efficient clusters in FANET environments. The proposed solution enhances cluster longevity and communication efficiency through mobility-aware clustering, energy-centric cluster head (CH) selection, and a ground station(GS)-assisted cluster maintenance management mechanism. First, steady multi-hop clusters are constructed, having CHs with not only high stability and high energy but also with steady and high-energy neighboring areas, and then a proper GS-assisted cluster maintenance mechanism is applied. Experimental results, based on extended simulations, demonstrate that our approach outperforms existing schemes significantly, in terms of cluster stability, communication overhead, and security resilience.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2512.00119", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00119", "abs": "https://arxiv.org/abs/2512.00119", "authors": ["Zeng Wang", "Minghao Shao", "Akashdeep Saha", "Ramesh Karri", "Johann Knechtel", "Muhammad Shafique", "Ozgur Sinanoglu"], "title": "NetDeTox: Adversarial and Efficient Evasion of Hardware-Security GNNs via RL-LLM Orchestration", "comment": null, "summary": "Graph neural networks (GNNs) have shown promise in hardware security by learning structural motifs from netlist graphs. However, this reliance on motifs makes GNNs vulnerable to adversarial netlist rewrites; even small-scale edits can mislead GNN predictions. Existing adversarial approaches, ranging from synthesis-recipe perturbations to gate transformations, come with high design overheads. We present NetDeTox, an automated end-to-end framework that orchestrates large language models (LLMs) with reinforcement learning (RL) in a systematic manner, enabling focused local rewriting. The RL agent identifies netlist components critical for GNN-based reasoning, while the LLM devises rewriting plans to diversify motifs that preserve functionality. Iterative feedback between the RL and LLM stages refines adversarial rewritings to limit overheads. Compared to the SOTA work AttackGNN, NetDeTox successfully degrades the effectiveness of all security schemes with fewer rewrites and substantially lower area overheads (reductions of 54.50% for GNN-RE, 25.44% for GNN4IP, and 41.04% for OMLA, respectively). For GNN4IP, ours can even optimize/reduce the original benchmarks' area, in particular for larger circuits, demonstrating the practicality and scalability of NetDeTox.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2512.00218", "categories": ["cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2512.00218", "abs": "https://arxiv.org/abs/2512.00218", "authors": ["Matt MacDermott", "Qiyao Wei", "Rada Djoneva", "Francis Rhys Ward"], "title": "Reasoning Under Pressure: How do Training Incentives Influence Chain-of-Thought Monitorability?", "comment": null, "summary": "AI systems that output their reasoning in natural language offer an opportunity for safety -- we can \\emph{monitor} their chain of thought (CoT) for undesirable reasoning, such as the pursuit of harmful objectives. However, the extent to which CoT faithfully reflects the underlying reasoning process, and hence the extent to which it can be usefully monitored, may be influenced by certain aspects of training. We investigate how different \\emph{training incentives}, applied to a reasoning model, affect its monitorability. We introduce a novel methodology for measuring monitorability according to whether a monitor can predict a key latent variable using the model's reasoning. When controlling for accuracy, we do not find evidence for consistent effects from commonly used incentives (length penalties and KL regularisation), but we find that adversarial optimisation (penalising monitor accuracy) degrades monitor performance, while direct optimisation for monitorability does not reliably lead to improvements. Our code is available at https://github.com/QiyaoWei/reasoning-under-pressure.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u4e0d\u540c\u8bad\u7ec3\u6fc0\u52b1\u5bf9\u63a8\u7406\u6a21\u578b\u53ef\u76d1\u6d4b\u6027\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u5bf9\u6297\u6027\u4f18\u5316\u4f1a\u964d\u4f4e\u76d1\u6d4b\u5668\u6027\u80fd\uff0c\u800c\u76f4\u63a5\u4f18\u5316\u53ef\u76d1\u6d4b\u6027\u6548\u679c\u4e0d\u663e\u8457\u3002", "motivation": "\u7814\u7a76\u76ee\u6807\u662f\u8bc4\u4f30\u5982\u4f55\u901a\u8fc7\u5bf9\u63a8\u7406\u6a21\u578b\u7684\u8bad\u7ec3\u6fc0\u52b1\uff0c\u4f7f\u5176\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u8fc7\u7a0b\uff08CoT\uff09\u66f4\u771f\u5b9e\u53cd\u6620\u5185\u5728\u903b\u8f91\uff0c\u4ece\u800c\u63d0\u5347\u5b89\u5168\u76d1\u6d4b\u6548\u679c\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u76d1\u6d4b\u5668\u5bf9\u6f5c\u53d8\u91cf\u9884\u6d4b\u80fd\u529b\u7684\u65b0\u53ef\u76d1\u6d4b\u6027\u5ea6\u91cf\u65b9\u6cd5\u3002\u6d4b\u8bd5\u4e86\u957f\u5ea6\u60e9\u7f5a\u3001KL\u6b63\u5219\u5316\u548c\u5bf9\u6297\u6027\u4f18\u5316\u7b49\u6fc0\u52b1\u7684\u5f71\u54cd\u3002", "result": "\u5728\u63a7\u5236\u51c6\u786e\u7387\u65f6\uff0c\u5e38\u7528\u6fc0\u52b1\uff08\u957f\u5ea6\u60e9\u7f5a/KL\u6b63\u5219\uff09\u7f3a\u4e4f\u4e00\u81f4\u6027\u5f71\u54cd\uff0c\u5bf9\u6297\u6027\u4f18\u5316\u4f1a\u635f\u5bb3\u76d1\u6d4b\uff0c\u76f4\u63a5\u4f18\u5316\u53ef\u76d1\u6d4b\u6027\u672a\u80fd\u53ef\u9760\u6539\u8fdb\u6027\u80fd\u3002", "conclusion": "\u5bf9\u6297\u6027\u8bad\u7ec3\u4f1a\u964d\u4f4e\u63a8\u7406\u8fc7\u7a0b\u7684\u53ef\u76d1\u6d4b\u6027\uff0c\u800c\u5e38\u89c4\u8bad\u7ec3\u65b9\u6cd5\u5bf9\u76d1\u6d4b\u7684\u63d0\u5347\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u65b9\u6cd5\u8bbe\u8ba1\u3002"}}
{"id": "2512.00380", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.00380", "abs": "https://arxiv.org/abs/2512.00380", "authors": ["Mingwei Liu", "Zheng Pei", "Yanlin Wang", "Zihao Wang", "Zikang Li", "Enci Lin", "Xin Peng", "Zibin Zheng"], "title": "Framework-Aware Code Generation with API Knowledge Graph-Constructed Data: A Study on HarmonyOS", "comment": null, "summary": "In the context of software frameworks with limited resources (such as HarmonyOS), large language models (LLMs) often exhibit poor code generation performance because they lack sufficient exposure to such environments during pre-training. Although LLMs can usually maintain correct logical structures across programming languages, they frequently struggle when dealing with framework-specific APIs or syntax, resulting in errors. This indicates that while pre-training equips LLMs with general algorithmic capabilities, they remain unfamiliar with the distinctive syntax and API usage of underrepresented frameworks. As a result, even advanced commercial models like GPT-4o cannot reliably generate correct code without prior adaptation. To address this issue, we propose APIKG4SYN, a framework designed to exploit API knowledge graphs for the construction of API-oriented question-code pairs, specifically tailored for low-resource frameworks without requiring executable code. APIKG4SYN integrates both single-API and multi-API knowledge, where the latter is derived through uncertainty estimation (UE)-driven Monte Carlo Tree Search (MCTS), enabling the creation of a diverse and informative dataset for fine-tuning LLMs. Using HarmonyOS as a case study, we build the first benchmark for HarmonyOS code generation. Experimental results show that fine-tuning Qwen with APIKG4SYN raises pass@1 accuracy to 25.00%, compared with 17.59% for the baseline GPT model. These results confirm that API-oriented data significantly enhance LLM performance in low-resource software development scenarios.", "AI": {"tldr": "\u4e3a\u89e3\u51b3LLMs\u5728\u4f4e\u8d44\u6e90\u6846\u67b6 (\u5982HarmonyOS) \u4e0b\u7684\u4ee3\u7801\u751f\u6210\u95ee\u9898\uff0c\u63d0\u51faAPIKG4SYN\u6846\u67b6\uff0c\u901a\u8fc7\u6784\u5efaAPI\u77e5\u8bc6\u56fe\u8c31\u751f\u6210API\u5bfc\u5411\u8bad\u7ec3\u6570\u636e\uff0c\u5b9e\u73b0\u5bf9LLMs\u5fae\u8c03\uff0c\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u4ee3\u7801\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8d44\u6e90\u6709\u9650\u7684\u8f6f\u4ef6\u6846\u67b6\u4e0b\uff0c\u56e0\u7f3a\u4e4f\u5145\u5206\u9884\u8bad\u7ec3\uff0c\u5bfc\u81f4\u9762\u5bf9\u6846\u67b6\u7279\u5b9aAPI\u6216\u8bed\u6cd5\u65f6\u8868\u73b0\u4e0d\u4f73\u3002\u5373\u4f7f\u5728\u4ee3\u7801\u7ed3\u6784\u4e0a\u6b63\u786e\uff0c\u4e5f\u5e38\u56e0\u5bf9\u65b0\u73af\u5883\u4e0d\u719f\u6089\u800c\u51fa\u9519\uff0c\u8fd9\u8868\u660e\u6a21\u578b\u9700\u8981\u8fdb\u4e00\u6b65\u9002\u5e94\u3002", "method": "APIKG4SYN\u5229\u7528API\u77e5\u8bc6\u56fe\u8c31\uff0c\u6784\u5efaAPI\u5bfc\u5411\u7684\u95ee\u7b54\u4ee3\u7801\u5bf9\uff0c\u4ee5\u9002\u914d\u4f4e\u8d44\u6e90\u8f6f\u4ef6\u6846\u67b6\uff0c\u901a\u8fc7\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u9a71\u52a8\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u589e\u5f3aAPI\u6570\u636e\u96c6\uff0c\u4e0d\u4f9d\u8d56\u53ef\u6267\u884c\u4ee3\u7801\u3002", "result": "\u5728HarmonyOS\u4e3a\u6848\u4f8b\u7684\u5b9e\u9a8c\u4e2d\uff0c\u8be5\u65b9\u6cd5\u6784\u5efa\u4e86\u8be5\u6846\u67b6\u4e0b\u9996\u4e2a\u4ee3\u7801\u751f\u6210\u57fa\u51c6\uff0c\u901a\u8fc7\u5fae\u8c03Qwen\u6a21\u578b\uff0cpass@1\u51c6\u786e\u7387\u4ece\u57fa\u7ebfGPT\u6a21\u578b\u768417.59%\u63d0\u9ad8\u5230\u4e8625.00%\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u5728\u4f4e\u8d44\u6e90\u73af\u5883\u4e0b\u7684\u6027\u80fd\u3002", "conclusion": "\u501f\u52a9\u77e5\u8bc6\u56fe\u8c31\u6784\u5efa\u7684API\u5bfc\u5411\u6570\u636e\u5bf9\u63d0\u5347LLMs\u5728\u4f4e\u8d44\u6e90\u8f6f\u4ef6\u6846\u67b6\u4e0b\u7684\u4ee3\u7801\u751f\u6210\u80fd\u529b\u5177\u6709\u663e\u8457\u4f5c\u7528\uff0c\u901a\u8fc7APIKG4SYN\u6846\u67b6\uff0c\u53ef\u4ee5\u5728\u4e0d\u4f9d\u8d56\u53ef\u6267\u884c\u4ee3\u7801\u7684\u60c5\u51b5\u4e0b\u63d0\u9ad8\u751f\u6210\u4ee3\u7801\u7684\u51c6\u786e\u6027\u548c\u9002\u5e94\u6027\u3002"}}
{"id": "2512.00705", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.00705", "abs": "https://arxiv.org/abs/2512.00705", "authors": ["Seongyeon Park", "Jaeyong Song", "Changmin Shin", "Sukjin Kim", "Junguk Hong", "Jinho Lee"], "title": "FlexiWalker: Extensible GPU Framework for Efficient Dynamic Random Walks with Runtime Adaptation", "comment": "To appear at EuroSys 2026", "summary": "Dynamic random walks are fundamental to various graph analysis applications, offering advantages by adapting to evolving graph properties. Their runtime-dependent transition probabilities break down the pre-computation strategy that underpins most existing CPU and GPU static random walk optimizations. This leaves practitioners suffering from suboptimal frameworks and having to write hand-tuned kernels that do not adapt to workload diversity. To handle this issue, we present FlexiWalker, the first GPU framework that delivers efficient, workload-generic support for dynamic random walks. Our design-space study shows that rejection sampling and reservoir sampling are more suitable than other sampling techniques under massive parallelism. Thus, we devise (i) new high-performance kernels for them that eliminate global reductions, redundant memory accesses, and random-number generation. Given the necessity of choosing the best-fitting sampling strategy at runtime, we adopt (ii) a lightweight first-order cost model that selects the faster kernel per node at runtime. To enhance usability, we introduce (iii) a compile-time component that automatically specializes user-supplied walk logic into optimized building blocks. On various dynamic random walk workloads with real-world graphs, FlexiWalker outperforms the best published CPU/GPU baselines by geometric means of 73.44x and 5.91x, respectively, while successfully executing workloads that prior systems cannot support. We open-source FlexiWalker in https://github.com/AIS-SNU/FlexiWalker.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2512.00136", "categories": ["cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2512.00136", "abs": "https://arxiv.org/abs/2512.00136", "authors": ["Tong Wu", "Weibin Wu", "Zibin Zheng"], "title": "An Empirical Study on the Security Vulnerabilities of GPTs", "comment": null, "summary": "Equipped with various tools and knowledge, GPTs, one kind of customized AI agents based on OpenAI's large language models, have illustrated great potential in many fields, such as writing, research, and programming. Today, the number of GPTs has reached three millions, with the range of specific expert domains becoming increasingly diverse. However, given the consistent framework shared among these LLM agent applications, systemic security vulnerabilities may exist and remain underexplored. To fill this gap, we present an empirical study on the security vulnerabilities of GPTs. Building upon prior research on LLM security, we first adopt a platform-user perspective to conduct a comprehensive attack surface analysis across different system components. Then, we design a systematic and multidimensional attack suite with the explicit objectives of information leakage and tool misuse based on the attack surface analysis, thereby concretely demonstrating the security vulnerabilities that various components of GPT-based systems face. Finally, we accordingly propose defense mechanisms to address the aforementioned security vulnerabilities. By increasing the awareness of these vulnerabilities and offering critical insights into their implications, this study seeks to facilitate the secure and responsible application of GPTs while contributing to developing robust defense mechanisms that protect users and systems against malicious attacks.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2512.00267", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00267", "abs": "https://arxiv.org/abs/2512.00267", "authors": ["Anab Maulana Barik", "Shou Ziyi", "Yang Kaiwen", "Yang Qi", "Shen Xin"], "title": "Trification: A Comprehensive Tree-based Strategy Planner and Structural Verification for Fact-Checking", "comment": null, "summary": "Technological advancement allows information to be shared in just a single click, which has enabled the rapid spread of false information. This makes automated fact-checking system necessary to ensure the safety and integrity of our online media ecosystem. Previous methods have demonstrated the effectiveness of decomposing the claim into simpler sub-tasks and utilizing LLM-based multi agent system to execute them. However, those models faces two limitations: they often fail to verify every component in the claim and lack of structured framework to logically connect the results of sub-tasks for a final prediction. In this work, we propose a novel automated fact-checking framework called Trification. Our framework begins by generating a comprehensive set of verification actions to ensure complete coverage of the claim. It then structured these actions into a dependency graph to model the logical interaction between actions. Furthermore, the graph can be dynamically modified, allowing the system to adapt its verification strategy. Experimental results on two challenging benchmarks demonstrate that our framework significantly enhances fact-checking accuracy, thereby advancing current state-of-the-art in automated fact-checking system.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2512.00556", "categories": ["cs.SE", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.00556", "abs": "https://arxiv.org/abs/2512.00556", "authors": ["Sina Salimian", "Gias Uddin", "Sumon Biswas", "Henry Leung"], "title": "Bias Testing and Mitigation in Black Box LLMs using Metamorphic Relations", "comment": null, "summary": "The widespread deployment of Large Language Models (LLMs) has intensified concerns about subtle social biases embedded in their outputs. Existing guardrails often fail when faced with indirect or contextually complex bias-inducing prompts. To address these limitations, we propose a unified framework for both systematic bias evaluation and targeted mitigation. Our approach introduces six novel Metamorphic Relations (MRs) that, based on metamorphic testing principles, transform direct bias-inducing inputs into semantically equivalent yet adversarially challenging variants. These transformations enable an automated method for exposing hidden model biases: when an LLM responds inconsistently or unfairly across MR-generated variants, the underlying bias becomes detectable. We further show that the same MRs can be used to generate diverse bias-inducing samples for fine-tuning, directly linking the testing process to mitigation. Using six state-of-the-art LLMs - spanning open-source and proprietary models - and a representative subset of 385 questions from the 8,978-item BiasAsker benchmark covering seven protected groups, our MRs reveal up to 14% more hidden biases compared to existing tools. Moreover, fine-tuning with both original and MR-mutated samples significantly enhances bias resiliency, increasing safe response rates from 54.7% to over 88.9% across models. These results highlight metamorphic relations as a practical mechanism for improving fairness in conversational AI.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2512.00719", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.00719", "abs": "https://arxiv.org/abs/2512.00719", "authors": ["Bohan Zhao", "Zane Cao", "Yongchao He"], "title": "SIMPLE: Disaggregating Sampling from GPU Inference into a Decision Plane for Faster Distributed LLM Serving", "comment": null, "summary": "As large language models (LLMs) scale out with tensor parallelism (TP) and pipeline parallelism (PP) and production stacks have aggressively optimized the data plane (attention/GEMM and KV cache), sampling, the decision plane that turns logits into tokens, becomes a new bottleneck. This creates a structural holdout: sampling neither expands with TP nor balances across PP stages, so its share of iteration time grows as GPUs get faster and it caps pipeline frequency at the last stage. We present SIMPLE, a stage-agnostic, sequence-parallel, overlappable decision plane that disaggregates sampling into a CPU-side service and shrinks its runtime footprint back to a minor, hidden role. SIMPLE combines: (1) sequence-parallel sampling, which shards work along the batch dimension and removes vocabulary-axis collectives; (2) a CPU-based algorithm with column-wise penalties and truncation-first filtering to realize single-pass, linear-time kernels; and (3) speculative hot-vocab sampling (SHVS), which samples on a small hot set with rejection-correctness and uses a simple sizing model to choose the hot-vocab size that maximizes throughput. In evaluation, SIMPLE improves end-to-end throughput by up to 96% and reduces P95 latency by 20-65%. Crucially, SIMPLE requires no user-side code changes and composes with existing data-plane optimizations, unlocking scaling benefits that compound with future GPU generations.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2512.00142", "categories": ["cs.CR", "cs.AI", "q-fin.CP", "q-fin.GN"], "pdf": "https://arxiv.org/pdf/2512.00142", "abs": "https://arxiv.org/abs/2512.00142", "authors": ["Swati Sachan", "Dale S. Fickett"], "title": "DeFi TrustBoost: Blockchain and AI for Trustworthy Decentralized Financial Decisions", "comment": "19 pages", "summary": "This research introduces the Decentralized Finance (DeFi) TrustBoost Framework, which combines blockchain technology and Explainable AI to address challenges faced by lenders underwriting small business loan applications from low-wealth households. The framework is designed with a strong emphasis on fulfilling four crucial requirements of blockchain and AI systems: confidentiality, compliance with data protection laws, resistance to adversarial attacks, and compliance with regulatory audits. It presents a technique for tamper-proof auditing of automated AI decisions and a strategy for on-chain (inside-blockchain) and off-chain data storage to facilitate collaboration within and across financial organizations.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2512.00305", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00305", "abs": "https://arxiv.org/abs/2512.00305", "authors": ["Zhengzhuo Xu", "SiNan Du", "Yiyan Qi", "SiwenLu", "Chengjin Xu", "Chun Yuan", "Jian Guo"], "title": "ChartPoint: Guiding MLLMs with Grounding Reflection for Chart Reasoning", "comment": "Accepted by ICCV 2025", "summary": "Multimodal Large Language Models (MLLMs) have emerged as powerful tools for chart comprehension. However, they heavily rely on extracted content via OCR, which leads to numerical hallucinations when chart textual annotations are sparse. While existing methods focus on scaling instructions, they fail to address the fundamental challenge, i.e., reasoning with visual perception. In this paper, we identify a critical observation: MLLMs exhibit weak grounding in chart elements and proportional relationships, as evidenced by their inability to localize key positions to match their reasoning. To bridge this gap, we propose PointCoT, which integrates reflective interaction into chain-of-thought reasoning in charts. By prompting MLLMs to generate bounding boxes and re-render charts based on location annotations, we establish connections between textual reasoning steps and visual grounding regions. We further introduce an automated pipeline to construct ChartPoint-SFT-62k, a dataset featuring 19.2K high-quality chart samples with step-by-step CoT, bounding box, and re-rendered visualizations. Leveraging this data, we develop two instruction-tuned models, ChartPointQ2 and ChartPointQ2.5, which outperform state-of-the-art across several chart benchmarks, e.g., +5.04\\% on ChartBench.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2512.00560", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.00560", "abs": "https://arxiv.org/abs/2512.00560", "authors": ["Jinyu Cai", "Jialong Li", "Nianyu Li", "Zhenyu Mao", "Mingyue Zhang", "Kenji Tei"], "title": "SAGE: Semantic-Aware Gray-Box Game Regression Testing with Large Language Models", "comment": "This paper has been submitted to the Automated Software Engineering journal", "summary": "The rapid iteration cycles of modern live-service games make regression testing indispensable for maintaining quality and stability. However, existing regression testing approaches face critical limitations, especially in common gray-box settings where full source code access is unavailable: they heavily rely on manual effort for test case construction, struggle to maintain growing suites plagued by redundancy, and lack efficient mechanisms for prioritizing relevant tests. These challenges result in excessive testing costs, limited automation, and insufficient bug detection. To address these issues, we propose SAGE, a semanticaware regression testing framework for gray-box game environments. SAGE systematically addresses the core challenges of test generation, maintenance, and selection. It employs LLM-guided reinforcement learning for efficient, goal-oriented exploration to automatically generate a diverse foundational test suite. Subsequently, it applies a semantic-based multi-objective optimization to refine this suite into a compact, high-value subset by balancing cost, coverage, and rarity. Finally, it leverages LLM-based semantic analysis of update logs to prioritize test cases most relevant to version changes, enabling efficient adaptation across iterations. We evaluate SAGE on two representative environments, Overcooked Plus and Minecraft, comparing against both automated baselines and human-recorded test cases. Across all environments, SAGE achieves superior bug detection with significantly lower execution cost, while demonstrating strong adaptability to version updates.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2512.00902", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.00902", "abs": "https://arxiv.org/abs/2512.00902", "authors": ["Yebo Wu", "Jingguang Li", "Zhijiang Guo", "Li Li"], "title": "Elastic Mixture of Rank-Wise Experts for Knowledge Reuse in Federated Fine-Tuning", "comment": null, "summary": "Federated fine-tuning offers a promising solution for adapting Large Language Models (LLMs) to downstream tasks while safeguarding data privacy. However, its high computational and communication demands hinder its deployment on resource-constrained devices. In this paper, we propose SmartFed, a resource-efficient federated fine-tuning framework. SmartFed intelligently reuses knowledge embedded in existing LoRA modules, eliminating the need for expensive training from scratch when adapting LLMs to new tasks. To effectively exploit this knowledge and ensure scalability, we introduce the Mixture of Rank-Wise Experts (MoRE). MoRE decomposes LoRA modules into fine-grained rank-level experts. These experts are selectively activated and combined based on input semantics and resource budgets. Moreover, to optimize resource utilization, we present the Elastic Expert Quota Allocation (EEQA). EEQA adaptively allocates expert capacity across parameter matrices based on their contribution to model performance, focusing computing resources on the critical experts. Extensive evaluations across multiple benchmarks demonstrate that SmartFed significantly outperforms existing methods in model performance and training efficiency.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2512.00377", "categories": ["cs.CR", "cs.CY"], "pdf": "https://arxiv.org/pdf/2512.00377", "abs": "https://arxiv.org/abs/2512.00377", "authors": ["Yuexin Xiang", "SM Mahir Shazeed Rish", "Qishuang Fu", "Yuquan Li", "Qin Wang", "Tsz Hon Yuen", "Jiangshan Yu"], "title": "Measuring Memecoin Fragility", "comment": null, "summary": "Memecoins, emerging from internet culture and community-driven narratives, have rapidly evolved into a unique class of crypto assets. Unlike technology-driven cryptocurrencies, their market dynamics are primarily shaped by viral social media diffusion, celebrity influence, and speculative capital inflows.\n  To capture the distinctive vulnerabilities of these ecosystems, we present the first Memecoin Ecosystem Fragility Framework (ME2F). ME2F formalizes memecoin risks in three dimensions: i) Volatility Dynamics Score capturing persistent and extreme price swings together with spillover from base chains; ii) Whale Dominance Score quantifying ownership concentration among top holders; and iii) Sentiment Amplification Score measuring the impact of attention-driven shocks on market stability.\n  We apply ME2F to representative tokens (over 65\\% market share) and show that fragility is not evenly distributed across the ecosystem. Politically themed tokens such as TRUMP, MELANIA, and LIBRA concentrate the highest risks, combining volatility, ownership concentration, and sensitivity to sentiment shocks. Established memecoins such as DOGE, SHIB, and PEPE fall into an intermediate range. Benchmark tokens ETH and SOL remain consistently resilient due to deeper liquidity and institutional participation. Our findings provide the first ecosystem-level evidence of memecoin fragility and highlight governance implications for enhancing market resilience in the Web3 era.", "AI": {"tldr": "\u63d0\u51fa\u5e76\u9a8c\u8bc1\u4e86\u9996\u4e2a\u6a21\u56e0\u5e01\u751f\u6001\u8106\u5f31\u6027\u8bc4\u4f30\u6846\u67b6ME2F\uff0c\u53d1\u73b0\u98ce\u9669\u5448\u4e24\u6781\u5206\u5316\u5206\u5e03", "motivation": "\u6a21\u56e0\u5e01\u4f5c\u4e3a\u65b0\u5174\u52a0\u5bc6\u8d44\u4ea7\u7c7b\u522b\u7f3a\u4e4f\u7cfb\u7edf\u6027\u98ce\u9669\u8bc4\u4f30\u5de5\u5177\uff0c\u9700\u63ed\u793a\u5176\u8106\u5f31\u6027\u673a\u5236", "method": "\u6784\u5efa\u4e09\u56e0\u5b50ME2F\u6846\u67b6\uff08\u6ce2\u52a8\u6027\u52a8\u6001\u8bc4\u5206\u3001\u9cb8\u9c7c\u652f\u914d\u8bc4\u5206\u3001\u60c5\u7eea\u653e\u5927\u8bc4\u5206\uff09\uff0c\u8986\u76d665%\u5e02\u573a\u4efd\u989d\u7684\u4ee3\u8868\u6027\u4ee3\u5e01", "result": "\u653f\u6cbb\u4e3b\u9898\u6a21\u56e0\u5e01\u98ce\u9669\u5f97\u5206\u663e\u8457\u9ad8\u4e8eDOGE\u7b49\u6210\u719f\u5e01\u79cd\uff081.8\u500d\uff09\uff0cETH/SOL\u56e0\u6d41\u52a8\u6027\u4f18\u52bf\u4fdd\u6301\u7a33\u5065", "conclusion": "\u6a21\u56e0\u5e01\u751f\u6001\u7cfb\u7edf\u5b58\u5728\u7ed3\u6784\u6027\u98ce\u9669\u5dee\u5f02\uff0c\u9700\u5efa\u7acb\u9488\u5bf9\u6027\u6cbb\u7406\u673a\u5236\u63d0\u5347Web3\u5e02\u573a\u97e7\u6027"}}
{"id": "2512.00571", "categories": ["cs.SE", "cs.AI", "cs.LG", "cs.NE"], "pdf": "https://arxiv.org/pdf/2512.00571", "abs": "https://arxiv.org/abs/2512.00571", "authors": ["Tarun Chintada", "Uday Kiran Cheera"], "title": "Enhancing Analogy-Based Software Effort Estimation with Firefly Algorithm Optimization", "comment": "12 pages, 3 figures, 2 tables. Research conducted in June 2024", "summary": "Analogy-Based Estimation (ABE) is a popular method for non-algorithmic estimation due to its simplicity and effectiveness. The Analogy-Based Estimation (ABE) model was proposed by researchers, however, no optimal approach for reliable estimation was developed. Achieving high accuracy in the ABE might be challenging for new software projects that differ from previous initiatives. This study (conducted in June 2024) proposes a Firefly Algorithm-guided Analogy-Based Estimation (FAABE) model that combines FA with ABE to improve estimation accuracy. The FAABE model was tested on five publicly accessible datasets: Cocomo81, Desharnais, China, Albrecht, Kemerer and Maxwell. To improve prediction efficiency, feature selection was used. The results were measured using a variety of evaluation metrics; various error measures include MMRE, MAE, MSE, and RMSE. Compared to conventional models, the experimental results show notable increases in prediction precision, demonstrating the efficacy of the Firefly-Analogy ensemble.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2512.00412", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00412", "abs": "https://arxiv.org/abs/2512.00412", "authors": ["Jiawei Chen", "Yang Yang", "Chao Yu", "Yu Tian", "Zhi Cao", "Linghao Li", "Hang Su", "Zhaoxia Yin"], "title": "Red Teaming Large Reasoning Models", "comment": "30 pages, 9 figures", "summary": "Large Reasoning Models (LRMs) have emerged as a powerful advancement in multi-step reasoning tasks, offering enhanced transparency and logical consistency through explicit chains of thought (CoT). However, these models introduce novel safety and reliability risks, such as CoT-hijacking and prompt-induced inefficiencies, which are not fully captured by existing evaluation methods. To address this gap, we propose RT-LRM, a unified benchmark designed to assess the trustworthiness of LRMs. RT-LRM evaluates three core dimensions: truthfulness, safety and efficiency. Beyond metric-based evaluation, we further introduce the training paradigm as a key analytical perspective to investigate the systematic impact of different training strategies on model trustworthiness. We achieve this by designing a curated suite of 30 reasoning tasks from an observational standpoint. We conduct extensive experiments on 26 models and identify several valuable insights into the trustworthiness of LRMs. For example, LRMs generally face trustworthiness challenges and tend to be more fragile than Large Language Models (LLMs) when encountering reasoning-induced risks. These findings uncover previously underexplored vulnerabilities and highlight the need for more targeted evaluations. In addition, we release a scalable toolbox for standardized trustworthiness research to support future advancements in this important field. Our code and datasets will be open-sourced.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2512.00331", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2512.00331", "abs": "https://arxiv.org/abs/2512.00331", "authors": ["Yefeng Wu", "Yuchen Song", "Yecheng Zhao", "Ling Wu", "Shan Wan"], "title": "CogEvo-Edu: Cognitive Evolution Educational Multi-Agent Collaborative System", "comment": null, "summary": "Large language models (LLMs) are increasingly deployed as conversational tutors in STEM education, yet most systems still rely on a single LLM with a static retrieval-augmented generation (RAG) pipeline over course materials. This design struggles in complex domains such as digital signal processing (DSP), where tutors must maintain coherent long-term student models, manage heterogeneous knowledge bases, and adapt teaching strategies over extended interactions. We argue that retrieval, memory, and control should be treated as a coupled cognitive evolution process. We instantiate this view in CogEvo-Edu, a hierarchical educational multi-agent system comprising a Cognitive Perception Layer (CPL), a Knowledge Evolution Layer (KEL), and a Meta-Control Layer (MCL). CPL maintains dual memories and performs confidence-weighted consolidation to build structured, self-correcting student profiles under limited context. KEL assigns each knowledge chunk a spatiotemporal value that drives activation, semantic compression, and forgetting. MCL formulates tutoring as hierarchical sequential decision making, orchestrating specialized agents and jointly adapting CPL/KEL hyperparameters via a dual inner--outer loop. To evaluate CogEvo-Edu, we construct DSP-EduBench, a vertical benchmark for DSP tutoring with heterogeneous resources, simulated student profiles, and long-horizon interaction scripts. Using a three-model LLM-as-a-Judge ensemble, CogEvo-Edu raises the overall score from 5.32 to 9.23 and improves all six indicators over static RAG, simple memory, and a single-agent variant, demonstrating the value of jointly evolving student profiles, knowledge bases, and teaching policies.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2512.00651", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.00651", "abs": "https://arxiv.org/abs/2512.00651", "authors": ["Mohammed Latif Siddiq", "Arvin Islam-Gomes", "Natalie Sekerak", "Joanna C. S. Santos"], "title": "Large Language Models for Software Engineering: A Reproducibility Crisis", "comment": "Submitted to Empirical Software Engineering (EMSE) journal; 112 pages (81 pages of references)", "summary": "Reproducibility is a cornerstone of scientific progress, yet its state in large language model (LLM)-based software engineering (SE) research remains poorly understood. This paper presents the first large-scale, empirical study of reproducibility practices in LLM-for-SE research. We systematically mined and analyzed 640 papers published between 2017 and 2025 across premier software engineering, machine learning, and natural language processing venues, extracting structured metadata from publications, repositories, and documentation. Guided by four research questions, we examine (i) the prevalence of reproducibility smells, (ii) how reproducibility has evolved over time, (iii) whether artifact evaluation badges reliably reflect reproducibility quality, and (iv) how publication venues influence transparency practices. Using a taxonomy of seven smell categories: Code and Execution, Data, Documentation, Environment and Tooling, Versioning, Model, and Access and Legal, we manually annotated all papers and associated artifacts. Our analysis reveals persistent gaps in artifact availability, environment specification, versioning rigor, and documentation clarity, despite modest improvements in recent years and increased adoption of artifact evaluation processes at top SE venues. Notably, we find that badges often signal artifact presence but do not consistently guarantee execution fidelity or long-term reproducibility. Motivated by these findings, we provide actionable recommendations to mitigate reproducibility smells and introduce a Reproducibility Maturity Model (RMM) to move beyond binary artifact certification toward multi-dimensional, progressive evaluation of reproducibility rigor.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2512.01357", "categories": ["cs.DC", "cs.AI", "cs.AR"], "pdf": "https://arxiv.org/pdf/2512.01357", "abs": "https://arxiv.org/abs/2512.01357", "authors": ["Wenbin Zhu", "Zhaoyan Shen", "Zili Shao", "Hongjun Dai", "Feng Chen"], "title": "Tangram: Accelerating Serverless LLM Loading through GPU Memory Reuse and Affinity", "comment": null, "summary": "Serverless Large Language Models (LLMs) have emerged as a cost-effective solution for deploying AI services by enabling a 'pay-as-you-go' pricing model through GPU resource sharing. However, cold-start latency, especially the model loading phase, has become a critical performance bottleneck, as it scales linearly with model size and severely limits the practical deployment of large-scale LLM services. This paper presents Tangram, a novel system that accelerates Serverless LLM loading through efficient GPU memory reuse. By leveraging the unused GPU memory to retain model parameters, Tangram significantly reduces model transfer time and cold-start latency. Its design includes three key components: unified GPU memory pool for tensor-level parameter sharing across models, on-demand KV cache allocation for dynamic memory management, and GPU-affinity-aware scheduling for maximizing resource utilization. These techniques collectively address the critical challenges of inefficient memory usage and the cold-start problem in Serverless LLM platforms. We have implemented a fully functional prototype, and experiments show that Tangram achieves up to 6.2 times faster loading and reduces Time-To-First-Token (TTFT) during cold-start by 23--55% over state-of-the-art methods.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2512.00414", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2512.00414", "abs": "https://arxiv.org/abs/2512.00414", "authors": ["Haney Kang", "Eduard Marin", "Myoungsung You", "Diego Perino", "Seungwon Shin", "Jinwoo Kim"], "title": "BEACON: Automatic Container Policy Generation using Environment-aware Dynamic Analysis", "comment": null, "summary": "This paper introduces BeaCon, a novel tool for the automated generation of adjustable container security policies. Unlike prior approaches, BeaCon leverages dynamic analysis to simulate realistic environments, uncovering container execution paths that may remain hidden during the profiling phase. To address the challenge of exploring vast profiling spaces, we employ efficient heuristics to reveal additional system events with minimal effort. In addition, BeaCon incorporates a security and functionality scoring mechanism to prioritize system calls and capabilities based on their impact on the host OS kernel's security and the functionality of containerized applications. By integrating these scores, BeaCon achieves a customized balance between security and functionality, enabling cloud providers to enforce security measures while maintaining tenant availability. We implemented a prototype of BeaCon using eBPF kernel technology and conducted extensive evaluations. Results from the top 15 containers, which revealed significant improvements, demonstrate that BeaCon identifies an average of 16.5% additional syscalls by applying diverse environments. Furthermore, we evaluated its effectiveness in mitigating risks associated with 45 known vulnerabilities (e.g., CVEs), showcasing its potential to significantly enhance container security. Additionally, we performed proof-of-concept demonstrations for two well-known security vulnerabilities, showing that BeaCon successfully reduces attack surface by blocking these exploits.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2512.00344", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00344", "abs": "https://arxiv.org/abs/2512.00344", "authors": ["Naifan Zhang", "Ruihan Sun", "Ruixi Su", "Shiqi Ma", "Shiya Zhang", "Xianna Weng", "Xiaofan Zhang", "Yuhan Zhan", "Yuyang Xu", "Zhaohan Chen", "Zhengyuan Pan", "Ziyi Song"], "title": "Echo-N1: Affective RL Frontier", "comment": null, "summary": "The LLM field has spent a year perfecting RL for tasks machines already excel at, math, code, and deterministic reasoning, while completely sidestepping the domain that actually defines human intelligence: subjective, emotionally grounded, personality sensitive conversation. This space has often been regarded as inherently subjective and challenging to formalize, making it appear unsuitable for conventional RL pipelines. We show that it is not only possible and it is a solvable and transformative RL problem. We propose the first framework that infers user personality on the fly and optimizes model behavior toward personalized conversational preferences. Contrary to the widespread belief that RL collapses in non-verifiable settings, our method produces consistent, robust, and dramatic improvements in humanlike interaction quality. We also introduce the first dynamic emotional intelligence evaluation suite to quantify these gains. Our model, which is introduced as Echo-N1, behaves far above its base version and outperforming the proprietary Doubao 1.5 Character. This work establishes a new frontier for RL: optimizing models for the deeply subjective, deeply human dimensions of conversation.", "AI": {"tldr": "\u63d0\u51faRL\u53ef\u4f18\u5316\u4e3b\u89c2\u5bf9\u8bdd\uff0c\u6784\u5efa\u9996\u4e2a\u4e2a\u6027\u5316\u53ca\u60c5\u611f\u667a\u80fd\u8bc4\u4f30\u6846\u67b6\u3002", "motivation": "RL\u957f\u671f\u805a\u7126\u673a\u5668\u64c5\u957f\u9886\u57df\uff0c\u5ffd\u7565\u4e3b\u89c2\u5bf9\u8bdd\u8fd9\u4e00\u4eba\u7c7b\u667a\u80fd\u6838\u5fc3\u3002", "method": "1. \u6846\u67b6\u52a8\u6001\u63a8\u65ad\u7528\u6237\u6027\u683c\u5e76\u4f18\u5316\u6a21\u578b\u884c\u4e3a\uff1b2. \u53d1\u5e03\u52a8\u6001\u60c5\u611f\u667a\u80fd\u8bc4\u4f30\u5957\u4ef6\u3002", "result": "Echo-N1\u8fdc\u8d85\u5176\u57fa\u7840\u7248\u672c\uff0c\u6027\u80fd\u4f18\u4e8eDoubao 1.5 Character\uff0c\u9a8c\u8bc1RL\u5728\u4e3b\u89c2\u5bf9\u8bdd\u7684\u6709\u6548\u6027\u3002", "conclusion": "RL\u53ef\u7a81\u7834\u975e\u53ef\u9a8c\u8bc1\u573a\u666f\u9650\u5236\uff0c\u6210\u4e3a\u4f18\u5316\u4eba\u6027\u5316\u5bf9\u8bdd\u7684\u65b0\u524d\u6cbf\u3002"}}
{"id": "2512.00766", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.00766", "abs": "https://arxiv.org/abs/2512.00766", "authors": ["Zenghui Zhou", "Yuechen Li", "Yi Cai", "Jinlong Wen", "Xiaohan Yu", "Zheng Zheng", "Beibei Yin"], "title": "Code Comments for Quantum Software Development Kits: An Empirical Study on Qiskit", "comment": "Zenghui Zhou and Yuechen Li contributed equally to this work. Corresponding author is Zheng Zheng", "summary": "Quantum computing is gaining attention from academia and industry. With the quantum Software Development Kits (SDKs), programmers can develop quantum software to explore the power of quantum computing. However, programmers may face challenges in understanding quantum software due to the non-intuitive quantum mechanics. To facilitate software development and maintenance, code comments offered in quantum SDKs serve as a natural language explanation of program functionalities and logical flows. Despite their importance, scarce research systematically reports their value and provides constructive guidelines for programmers. To address this gap, our paper focuses on Qiskit, one of the most popular quantum SDKs, and presents CC4Q, the first dataset of code comments for quantum computing. CC4Q incorporates 9677 code comment pairs and 21970 sentence-level code comment units, the latter of which involve heavy human annotation. Regarding the annotation, we validate the applicability of the developer-intent taxonomy used in classical programs, and also propose a new taxonomy considering quantum-specific knowledge. We conduct an empirical study comprehensively interpreting code comments from three perspectives: comment structure and coverage, developers' intentions, and associated quantum topics. Our findings uncover key differences in code comments between classical and quantum software, and also outline quantum-specific knowledge relevant to quantum software development.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u9488\u5bf9\u91cf\u5b50\u8ba1\u7b97\u7f16\u7a0b\u7684\u4e00\u4e2a\u4ee3\u7801\u6ce8\u91ca\u6570\u636e\u96c6CC4Q\uff0c\u65e8\u5728\u901a\u8fc7\u5206\u6790Qiskit\u4ee3\u7801\u6ce8\u91ca\u4ee5\u6539\u5584\u5f00\u53d1\u548c\u7ef4\u62a4\u3002", "motivation": "\u91cf\u5b50\u8f6f\u4ef6\u56e0\u5176\u975e\u76f4\u89c2\u91cf\u5b50\u529b\u5b66\u539f\u7406\u4f7f\u5f97\u7a0b\u5e8f\u5458\u5728\u5f00\u53d1\u65f6\u9762\u4e34\u56f0\u96be\uff0c\u73b0\u6709\u7684\u7ecf\u5178\u7f16\u7a0b\u7684\u6ce8\u91ca\u65b9\u6cd5\u4e0d\u80fd\u76f4\u63a5\u8fc1\u79fb\u3002\u56e0\u6b64\uff0c\u6709\u5fc5\u8981\u7814\u7a76\u91cf\u5b50\u4ee3\u7801\u6ce8\u91ca\u5e76\u63d0\u4f9b\u6307\u5bfc\u3002", "method": "\u7814\u7a76\u91c7\u7528\u4e86Qiskit SDK\uff0c\u5e76\u5efa\u7acb\u4e86\u4e00\u4e2a\u540d\u4e3aCC4Q\u7684\u6570\u636e\u96c6\uff0c\u8be5\u6570\u636e\u96c6\u5305\u62ec\u4e869677\u4e2a\u4ee3\u7801\u6ce8\u91ca\u5bf9\u548c21970\u4e2a\u53e5\u5b50\u7ea7\u522b\u7684\u6ce8\u91ca\u5355\u5143\u3002\u901a\u8fc7\u4eba\u5de5\u6807\u6ce8\u5e76\u9a8c\u8bc1\u4e86\u7ecf\u5178\u5f00\u53d1\u8005\u610f\u56fe\u5206\u7c7b\u7684\u9002\u7528\u6027\uff0c\u540c\u65f6\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u9488\u5bf9\u91cf\u5b50\u77e5\u8bc6\u7684\u5206\u7c7b\u65b9\u6cd5\u3002\u4ece\u4ee3\u7801\u6ce8\u91ca\u7684\u7ed3\u6784\u548c\u8986\u76d6\u5ea6\u3001\u5f00\u53d1\u8005\u610f\u56fe\u53ca\u5176\u76f8\u5173\u91cf\u5b50\u4e3b\u9898\u4e09\u4e2a\u89d2\u5ea6\u8fdb\u884c\u4e86\u7efc\u5408\u7ecf\u9a8c\u7814\u7a76\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u7ecf\u5178\u4ee3\u7801\u6ce8\u91ca\u548c\u91cf\u5b50\u4ee3\u7801\u6ce8\u91ca\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u63d0\u51fa\u4e86\u4e0e\u91cf\u5b50\u8f6f\u4ef6\u5f00\u53d1\u76f8\u5173\u7684\u7279\u5b9a\u91cf\u5b50\u77e5\u8bc6\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u9488\u5bf9\u91cf\u5b50\u7f16\u7a0b\u7684\u4ee3\u7801\u6ce8\u91ca\u6570\u636e\u96c6CC4Q\uff0c\u5e76\u901a\u8fc7\u7ecf\u9a8c\u7814\u7a76\u63ed\u793a\u4e86\u7ecf\u5178\u548c\u91cf\u5b50\u6ce8\u91ca\u4e4b\u95f4\u7684\u4e3b\u8981\u5dee\u5f02\uff0c\u4e3a\u91cf\u5b50\u8f6f\u4ef6\u7684\u5f00\u53d1\u548c\u7ef4\u62a4\u63d0\u4f9b\u4e86\u5177\u4f53\u6307\u5bfc\u3002"}}
{"id": "2512.01549", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.01549", "abs": "https://arxiv.org/abs/2512.01549", "authors": ["Tom Goethals", "Merlijn Sebrechts", "Stijn De Schrijver", "Filip De Turck", "Bruno Volckaert"], "title": "Delta Sum Learning: an approach for fast and global convergence in Gossip Learning", "comment": null, "summary": "Federated Learning is a popular approach for distributed learning due to its security and computational benefits. With the advent of powerful devices in the network edge, Gossip Learning further decentralizes Federated Learning by removing centralized integration and relying fully on peer to peer updates. However, the averaging methods generally used in both Federated and Gossip Learning are not ideal for model accuracy and global convergence. Additionally, there are few options to deploy Learning workloads in the edge as part of a larger application using a declarative approach such as Kubernetes manifests. This paper proposes Delta Sum Learning as a method to improve the basic aggregation operation in Gossip Learning, and implements it in a decentralized orchestration framework based on Open Application Model, which allows for dynamic node discovery and intent-driven deployment of multi-workload applications. Evaluation results show that Delta Sum performance is on par with alternative integration methods for 10 node topologies, but results in a 58% lower global accuracy drop when scaling to 50 nodes. Overall, it shows strong global convergence and a logarithmic loss of accuracy with increasing topology size compared to a linear loss for alternatives under limited connectivity.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u7528\u4e8eGossip Learning\u7684Delta Sum Learning\u65b9\u6cd5\uff0c\u5e76\u5b9e\u73b0\u4e86\u57fa\u4e8eOpen Application Model\u7684\u6846\u67b6\uff0c\u4ee5\u63d0\u9ad8\u6a21\u578b\u51c6\u786e\u6027\u548c\u5168\u5c40\u6536\u655b\u6027\u3002", "motivation": "\u4e3a\u89e3\u51b3Federated\u548cGossip Learning\u4e2d\u5e73\u5747\u65b9\u6cd5\u5728\u6a21\u578b\u51c6\u786e\u6027\u548c\u5168\u5c40\u6536\u655b\u6027\u7684\u4e0d\u7406\u60f3\u95ee\u9898\uff0c\u7f3a\u4e4f\u5229\u7528\u5982Kubernetes\u7684\u58f0\u660e\u6027\u65b9\u6cd5\u90e8\u7f72\u8fb9\u7f18\u5b66\u4e60\u8d1f\u8f7d\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u6539\u8fdbGossip Learning\u57fa\u7840\u805a\u5408\u64cd\u4f5c\u7684Delta Sum Learning\u65b9\u6cd5\uff0c\u5e76\u5728\u53bb\u4e2d\u5fc3\u5316\u7684\u7f16\u914d\u6846\u67b6\u4e2d\u5b9e\u73b0\uff0c\u4f7f\u7528Open Application Model\u8fdb\u884c\u52a8\u6001\u8282\u70b9\u53d1\u73b0\u548c\u610f\u56fe\u9a71\u52a8\u7684\u591a\u5de5\u4f5c\u8d1f\u8f7d\u5e94\u7528\u90e8\u7f72\u3002", "result": "Delta Sum\u572810\u8282\u70b9\u62d3\u6251\u4e0a\u6027\u80fd\u4e0e\u66ff\u4ee3\u65b9\u6cd5\u76f8\u5f53\uff0c\u6269\u5c55\u523050\u8282\u70b9\u65f6\u5168\u7403\u7cbe\u5ea6\u4e0b\u964d\u51cf\u5c1158%\uff0c\u5728\u53d7\u9650\u8fde\u63a5\u4e0b\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u5168\u5c40\u6536\u655b\u6027\u548c\u5bf9\u6570\u7cbe\u5ea6\u635f\u5931\uff0c\u800c\u66ff\u4ee3\u54c1\u5b58\u5728\u7ebf\u6027\u635f\u5931\u3002", "conclusion": "Delta Sum Learning\u5728\u6269\u5c55\u6027\u548c\u51c6\u786e\u6027\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u5e76\u5c55\u793a\u4e86\u5728\u8fb9\u7f18\u8ba1\u7b97\u73af\u5883\u4e2d\u90e8\u7f72\u5b66\u4e60\u8d1f\u8f7d\u7684\u65b0\u6846\u67b6\u3002"}}
{"id": "2512.00844", "categories": ["cs.SE", "cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.00844", "abs": "https://arxiv.org/abs/2512.00844", "authors": ["Giles Winchester", "George Parisis", "Luc Berthouze"], "title": "FC-ADL: Efficient Microservice Anomaly Detection and Localisation Through Functional Connectivity", "comment": "13 pages, 6 figures, 2 tables", "summary": "Microservices have transformed software architecture through the creation of modular and independent services. However, they introduce operational complexities in service integration and system management that makes swift and accurate anomaly detection and localisation challenging. Despite the complex, dynamic, and interconnected nature of microservice architectures, prior works that investigate metrics for anomaly detection rarely include explicit information about time-varying interdependencies. And whilst prior works on fault localisation typically do incorporate information about dependencies between microservices, they scale poorly to real world large-scale deployments due to their reliance on computationally expensive causal inference. To address these challenges we propose FC-ADL, an end-to-end scalable approach for detecting and localising anomalous changes from microservice metrics based on the neuroscientific concept of functional connectivity. We show that by efficiently characterising time-varying changes in dependencies between microservice metrics we can both detect anomalies and provide root cause candidates without incurring the significant overheads of causal and multivariate approaches. We demonstrate that our approach can achieve top detection and localisation performance across a wide degree of different fault scenarios when compared to state-of-the-art approaches. Furthermore, we illustrate the scalability of our approach by applying it to Alibaba's extremely large real-world microservice deployment.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86FC-ADL\uff0c\u4e00\u79cd\u57fa\u4e8e\u529f\u80fd\u8fde\u63a5\u6027\u7684\u7aef\u5230\u7aef\u53ef\u6269\u5c55\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece\u5fae\u670d\u52a1\u5ea6\u91cf\u4e2d\u68c0\u6d4b\u548c\u5b9a\u4f4d\u5f02\u5e38\u53d8\u5316\u3002", "motivation": "\u5fae\u670d\u52a1\u67b6\u6784\u7ed9\u8f6f\u4ef6\u67b6\u6784\u5e26\u6765\u4e86\u6a21\u5757\u5316\u548c\u72ec\u7acb\u6027\uff0c\u4f46\u4e5f\u5f15\u5165\u4e86\u8fd0\u8425\u590d\u6742\u6027\uff0c\u4f7f\u5f02\u5e38\u68c0\u6d4b\u548c\u5b9a\u4f4d\u53d8\u5f97\u56f0\u96be\u3002\u4ee5\u5f80\u7684\u65b9\u6cd5\u5728\u8003\u8651\u65f6\u95f4\u53d8\u5316\u4f9d\u8d56\u6027\u65b9\u9762\u4e0d\u8db3\uff0c\u6216\u56e0\u8ba1\u7b97\u590d\u6742\u800c\u5bfc\u81f4\u6269\u5c55\u6027\u5dee\u3002", "method": "\u63d0\u51fa\u7684\u65b9\u6cd5FC-ADL\uff0c\u91c7\u7528\u4e86\u795e\u7ecf\u79d1\u5b66\u4e2d\u7684\u529f\u80fd\u8fde\u63a5\u6027\u6982\u5ff5\uff0c\u6709\u6548\u63cf\u8ff0\u4e86\u5fae\u670d\u52a1\u5ea6\u91cf\u95f4\u4f9d\u8d56\u6027\u7684\u65f6\u53d8\u7279\u6027\uff0c\u5b9e\u73b0\u5f02\u5e38\u68c0\u6d4b\u53ca\u6839\u672c\u539f\u56e0\u5019\u9009\u70b9\u63d0\u4f9b\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u4e0d\u540c\u6545\u969c\u573a\u666f\u4e0b\u5b9e\u73b0\u4e86\u9876\u5c16\u7684\u68c0\u6d4b\u548c\u5b9a\u4f4d\u6027\u80fd\uff0c\u5e76\u5728\u6269\u5c55\u6027\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u53ef\u5e94\u7528\u4e8e\u5927\u578b\u771f\u5b9e\u5fae\u670d\u52a1\u90e8\u7f72\uff0c\u5982\u963f\u91cc\u7684\u5927\u89c4\u6a21\u670d\u52a1\u7cfb\u7edf\u3002", "conclusion": "FC-ADL\u4e3a\u5fae\u670d\u52a1\u73af\u5883\u4e0b\u7684\u5f02\u5e38\u68c0\u6d4b\u548c\u5b9a\u4f4d\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u514b\u670d\u4e86\u4ee5\u5f80\u65b9\u6cd5\u7684\u5c40\u9650\u3002"}}
{"id": "2512.01646", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.01646", "abs": "https://arxiv.org/abs/2512.01646", "authors": ["Barenya Kumar Nandy", "Rupesh Nasre"], "title": "StarDist: A Code Generator for Distributed Graph Algorithms", "comment": null, "summary": "Relational data, occurring in the real world, are often structured as graphs, which provide the logical abstraction required to make analytical derivations simpler. As graphs get larger, the irregular access patterns exhibited in most graph algorithms, hamper performance. This, along with NUMA and physical memory limits, results in scaling complexities with sequential/shared memory frameworks. StarPlat's MPI backend abstracts away the programmatic complexity involved in designing optimal distributed graph algorithms. It provides an instrument for coding graph algorithms that scale over distributed memory. In this work, we provide an analysis-transformation framework that leverages general semantics associated with iterations involving nodes and their neighbors, within StarPlat, to aggregate communication. The framework scans for patterns that warrant re-ordering in neighborhood access patterns, aggregate communication, and avoid communication altogether with opportunistic caching in reduction constructs. We also architect an optimized bulk-reduction substrate using Open MPI's passive Remote Memory Access (RMA) constructs. We applied our optimization logic to StarPlat's distributed backend and outperformed d-Galois by 2.05 and DRONE by 1.44 times in Single Source Shortest Paths across several big data graphs.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8eStarPlat\u7684\u4f18\u5316\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u5927\u578b\u56fe\u5728\u5206\u5e03\u5f0f\u5185\u5b58\u73af\u5883\u4e2d\u7684\u6027\u80fd\u74f6\u9888\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u5173\u7cfb\u6570\u636e\u5e38\u8868\u73b0\u4e3a\u56fe\u5f62\uff0c\u4f46\u56fe\u7b97\u6cd5\u7684\u4e0d\u89c4\u5219\u8bbf\u95ee\u6a21\u5f0f\u3001NUMA\u9650\u5236\u548c\u7269\u7406\u5185\u5b58\u74f6\u9888\u4f1a\u5f71\u54cd\u6027\u80fd\u3002StarPlat\u6846\u67b6\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5206\u5e03\u5f0f\u5185\u5b58\u7f16\u7801\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5206\u6790\u8f6c\u6362\u6846\u67b6\uff0c\u4f18\u5316\u5206\u5e03\u5f0f\u5185\u5b58\u4e2d\u7684\u56fe\u5f62\u7b97\u6cd5\u3002\u8be5\u65b9\u6cd5\u5229\u7528\u4e0e\u8282\u70b9\u53ca\u5176\u90bb\u5c45\u76f8\u5173\u7684\u8fed\u4ee3\u8bed\u4e49\uff0c\u5b9e\u73b0\u901a\u4fe1\u7684\u805a\u96c6\u3002\u8fd8\u901a\u8fc7\u626b\u63cf\u91cd\u6392\u5e8f\u8bbf\u95ee\u6a21\u5f0f\u548c\u51cf\u5c11\u901a\u4fe1\uff0c\u4ee5\u53ca\u901a\u8fc7\u673a\u4f1a\u6027\u7f13\u5b58\u4f18\u5316\u3002\u6b64\u5916\uff0c\u4f7f\u7528Open MPI\u7684\u88ab\u52a8RMA\u6784\u5efa\u4e86\u4f18\u5316\u7684\u6279\u91cf\u5f52\u7ea6\uff08bulk-reduction\uff09\u5b50\u7ed3\u6784", "result": "SSSP\u7b97\u6cd5\u6027\u80fd\u6d4b\u8bd5\u4e2d,\u8be5\u6846\u67b6\u6bd4d-Galois\u548cDRONE\u5206\u522b\u4f18\u5316\u4e862.05\u500d\u548c1.44\u500d\u3002", "conclusion": "\u8be5\u4f18\u5316\u6846\u67b6\u6709\u6548\u63d0\u9ad8\u4e86StarPlat\u5728\u5927\u6570\u636e\u56fe\u4e0a\u7684\u6027\u80fd\u3002"}}
{"id": "2512.00480", "categories": ["cs.CR", "cs.IT"], "pdf": "https://arxiv.org/pdf/2512.00480", "abs": "https://arxiv.org/abs/2512.00480", "authors": ["Liang Feng Zhang"], "title": "A Unified Framework for Constructing Information-Theoretic Private Information Retrieval", "comment": "22 pages", "summary": "Retrieving up-to-date information from a publicly accessible database poses significant threats to the user's privacy. {\\em Private information retrieval} (PIR) protocols allow a user to retrieve any entry from a database, without revealing the identity of the entry being retrieved to the server(s). Such protocols have found numerous applications in both theoretical studies and real-life scenarios. The existing PIR constructions mainly give multi-server {\\em information-theoretic} PIR (IT-PIR) protocols or single-server computational PIR (CPIR) protocols. Compared with CPIR, IT-PIR protocols are computationally more efficient and secure in the presence of unbounded servers. The most classical and challenging problem in the realm of IT-PIR is constructing protocols with lower {\\em communication complexity}. In this review, we introduce a new discrete structure called {\\em families of orthogonal arrays with span capability} (FOASC) and propose a unified framework for constructing IT-PIR protocols. We show how the most influential IT-PIR protocols in the literature can be captured by the framework. We also put forward several interesting open problems concerning FOASC, whose solutions may result in innovative IT-PIR protocols.", "AI": {"tldr": "\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u7684\u79bb\u6563\u7ed3\u6784FOASC\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\u6765\u6784\u5efaIT-PIR\u534f\u8bae\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u5728IT-PIR\u9886\u57df\u4e2d\u6700\u7ecf\u5178\u548c\u6700\u5177\u6311\u6218\u6027\u7684\u95ee\u9898\uff1a\u6784\u5efa\u5177\u6709\u8f83\u4f4e\u901a\u4fe1\u590d\u6742\u5ea6\u7684\u534f\u8bae\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u7ed3\u6784\u53eb\u505a\u5e26\u8de8\u5ea6\u80fd\u529b\u6b63\u4ea4\u9635\u5217\u65cf\uff08FOASC\uff09\uff0c\u5e76\u63d0\u51fa\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\u6784\u5efaIT-PIR\u534f\u8bae\u3002", "result": "\u5c55\u793a\u4e86\u5982\u4f55\u5c06\u6587\u732e\u4e2d\u6700\u6709\u5f71\u54cd\u529b\u7684IT-PIR\u534f\u8bae\u7eb3\u5165\u6b64\u6846\u67b6\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e9b\u672a\u89e3\u7684\u5173\u4e8eFOASC\u7684\u6709\u8da3\u95ee\u9898\u3002", "conclusion": "\u901a\u8fc7FOASC\u7ed3\u6784\u53ca\u5176\u95ee\u9898\u7684\u89e3\u51b3\uff0c\u53ef\u80fd\u5bfc\u81f4\u521b\u65b0\u7684IT-PIR\u534f\u8bae\u7684\u4ea7\u751f\u3002"}}
{"id": "2512.00406", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.00406", "abs": "https://arxiv.org/abs/2512.00406", "authors": ["Pengyu Zeng", "Yuqin Dai", "Jun Yin", "Jing Zhong", "Ziyang Han", "Chaoyang Shi", "ZhanXiang Jin", "Maowei Jiang", "Yuxing Han", "Shuai Lu"], "title": "GreenPlanner: Practical Floorplan Layout Generation via an Energy-Aware and Function-Feasible Generative Framework", "comment": "11 pages, 6 figures", "summary": "Building design directly affects human well-being and carbon emissions, yet generating spatial-functional and energy-compliant floorplans remains manual, costly, and non-scalable. Existing methods produce visually plausible layouts but frequently violate key constraints, yielding invalid results due to the absence of automated evaluation. We present GreenPlanner, an energy- and functionality-aware generative framework that unifies design evaluation and generation. It consists of a labeled Design Feasibility Dataset for learning constraint priors; a fast Practical Design Evaluator (PDE) for predicting energy performance and spatial-functional validity; a Green Plan Dataset (GreenPD) derived from PDE-guided filtering to pair user requirements with regulation-compliant layouts; and a GreenFlow generator trained on GreenPD with PDE feedback for controllable, regulation-aware generation. Experiments show that GreenPlanner accelerates evaluation by over $10^{5}\\times$ with $>$99% accuracy, eliminates invalid samples, and boosts design efficiency by 87% over professional architects.", "AI": {"tldr": "GreenPlanner \u662f\u4e00\u4e2a\u7ed3\u5408\u751f\u6210\u4e0e\u8bc4\u4f30\u7684\u5efa\u7b51\u8bbe\u8ba1\u6846\u67b6\uff0c\u80fd\u540c\u65f6\u5904\u7406\u529f\u80fd\u6027\u548c\u80fd\u6e90\u5408\u89c4\u6027\u7ea6\u675f\u3002", "motivation": "\u4f20\u7edf\u5efa\u7b51\u751f\u6210\u65b9\u6cd5\u7f3a\u4e4f\u81ea\u52a8\u7ea6\u675f\u8bc4\u4f30\u80fd\u529b\uff0c\u5bfc\u81f4\u5e03\u5c40\u867d\u89c6\u89c9\u5408\u7406\u4f46\u5e38\u8fdd\u53cd\u6838\u5fc3\u8bbe\u8ba1\u89c4\u8303\u4e14\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u56db\u9636\u6bb5\u6846\u67b6\uff1a1) \u6784\u5efa\u5e26\u6807\u7b7e\u7684\u8bbe\u8ba1\u53ef\u884c\u6027\u6570\u636e\u96c6\u5b66\u4e60\u7ea6\u675f\u5148\u9a8c\uff1b2) \u5f00\u53d1\u5feb\u901f\u5b9e\u7528\u8bbe\u8ba1\u8bc4\u4f30\u5668(PDE)\uff1b3) \u751f\u6210PDE\u7b5b\u9009\u7684\u5408\u89c4\u6570\u636e\u96c6(GreenPD)\uff1b4) \u57fa\u4e8ePDE\u53cd\u9988\u8bad\u7ec3\u751f\u6210\u6a21\u578b(GreenFlow)\u3002", "result": "\u8bc4\u4f30\u901f\u5ea6\u63d0\u534710\u4e07\u500d\uff08>99%\u51c6\u786e\u7387\uff09\uff0c\u8bbe\u8ba1\u6548\u7387\u6bd4\u4e13\u4e1a\u5efa\u7b51\u5e08\u9ad887%\uff0c\u751f\u6210\u7ed3\u679c100%\u5408\u89c4\u3002", "conclusion": "\u901a\u8fc7\u8054\u5408\u4f18\u5316\u8bc4\u4f30\u4e0e\u751f\u6210\u90fd\u663e\u8457\u63d0\u5347\u5efa\u7b51\u8bbe\u8ba1\u6548\u7387\u53ca\u5408\u89c4\u6027\u3002\u672a\u6765\u53ef\u80fd\u5e94\u7528\u5230\u66f4\u591a\u5efa\u7b51\u548c\u5de5\u7a0b\u9886\u57df\u3002"}}
{"id": "2512.00855", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.00855", "abs": "https://arxiv.org/abs/2512.00855", "authors": ["Miikka Kuutila", "Paul Ralph", "Huilian Sophie Qiu", "Ronnie de Souza Santos", "Morakot Choetkiertikul", "Amin Milani Fard", "Rana Alkadhi", "Xavier Devroey", "Gregorio Robles", "Hideaki Hata", "Sebastian Baltes", "Vladimir Kovalenko", "Shalini Chakraborty", "Eray Tuzun", "Hera Arif", "Gianisa Adisaputri", "Kelly Garc\u00e9s", "Anielle S. L. Andrade", "Eyram Amedzor", "Bimpe Ayoola", "Keisha Gaspard-Chickoree", "Arazoo Hoseyni"], "title": "The Software Infrastructure Attitude Scale (SIAS): A Questionnaire Instrument for Measuring Professionals' Attitudes Toward Technical and Sociotechnical Infrastructure", "comment": "Accepted to ICSE 2026, 11 pages + 2 for references, 1 figure, 7 tables", "summary": "Context: Recent software engineering (SE) research has highlighted the need for sociotechnical research, implying a demand for customized psychometric scales. Objective: We define the concepts of technical and sociotechnical infrastructure in software engineering, and develop and validate a psychometric scale that measures attitudes toward them. Method: Grounded in theories of infrastructure, attitudes, and prior work on psychometric measurement, we defined the target constructs and generated scale items. The scale was administered to 225 software professionals and evaluated using a split sample. We conducted an exploratory factor analysis (EFA) on one half of the sample to uncover the underlying factor structure and performed a confirmatory factor analysis (CFA) on the other half to validate the structure. Further analyses with the whole sample assessed face, criterion-related, and discriminant validity. Results: EFA supported a two-factor structure (technical and sociotechnical infrastructure), accounting for 65% of the total variance with strong loadings. CFA confirmed excellent model fit. Face and content validity were supported by the item content reflecting cognitive, affective, and behavioral components. Both subscales were correlated with job satisfaction, perceived autonomy, and feedback from the job itself, supporting convergent validity. Regression analysis supported criterion-related validity, while the Heterotrait-Monotrait ratio of correlations (HTMT), the Fornell-Larcker criterion, and model comparison all supported discriminant validity. Discussion: The resulting scale is a valid instrument for measuring attitudes toward technical and sociotechnical infrastructure in software engineering research. Our work contributes to ongoing efforts to integrate psychological measurement rigor into empirical and behavioral software engineering research.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5728\u7ed3\u6784\u4e0a\u975e\u5e38\u7b26\u5408\u5178\u578b\u7684\u5b9e\u8bc1\u5fc3\u7406\u5b66\u548c\u5de5\u7a0b\u6280\u672f\u4ea4\u53c9\u9886\u57df\u7684\u7814\u7a76\u683c\u5f0f\u3002\u6bcf\u4e2a\u90e8\u5206\u90fd\u6709\u5176\u7279\u5b9a\u7684\u4efb\u52a1\u548c\u529f\u80fd\uff0c\u4e3a\u6574\u7bc7\u8bba\u6587\u63d0\u4f9b\u4e86\u6e05\u6670\u3001\u6709\u903b\u8f91\u6027\u7684\u6846\u67b6\u3002\u4ece\u7ed9\u51fa\u80cc\u666f\u548c\u7814\u7a76\u52a8\u673a\uff0c\u5230\u5b9a\u4e49\u6982\u5ff5\uff0c\u518d\u5230\u5177\u4f53\u7684\u65b9\u6cd5\u548c\u7ed3\u679c\u8bba\u8bc1\uff0c\u4ee5\u53ca\u6700\u540e\u7684\u8ba8\u8bba\uff0c\u6bcf\u4e2a\u90e8\u5206\u90fd\u76f8\u4e92\u5173\u8054\uff0c\u5f62\u6210\u4e86\u4e00\u4e2a\u5b8c\u6574\u7684\u7814\u7a76\u95ed\u73af\u3002", "motivation": "\u8f6f\u4ef6\u5de5\u7a0b\u7684\u6700\u65b0\u7814\u7a76\u5f3a\u8c03\u4e86\u793e\u4f1a\u6280\u672f\u7814\u7a76\u7684\u91cd\u8981\u6027\uff0c\u8fd9\u6697\u793a\u4e86\u5bf9\u5b9a\u5236\u5fc3\u7406\u6d4b\u91cf\u91cf\u8868\u7684\u9700\u6c42\u3002", "method": "\u57fa\u4e8e\u57fa\u7840\u8bbe\u65bd\u3001\u6001\u5ea6\u548c\u5148\u524d\u5173\u4e8e\u5fc3\u7406\u6d4b\u91cf\u7684\u7406\u8bba\uff0c\u5b9a\u4e49\u4e86\u76ee\u6807\u6982\u5ff5\u5e76\u751f\u6210\u4e86\u91cf\u8868\u9879\u76ee\u3002\u5bf9225\u540d\u8f6f\u4ef6\u4e13\u4e1a\u4eba\u5458\u8fdb\u884c\u4e86\u91cf\u8868\u8c03\u67e5\uff0c\u5e76\u4f7f\u7528\u5206\u5272\u6837\u672c\u8fdb\u884c\u8bc4\u4f30\u3002\u5728\u5176\u4e2d\u4e00\u534a\u7684\u6837\u672c\u4e0a\u8fdb\u884c\u4e86\u63a2\u7d22\u6027\u56e0\u5b50\u5206\u6790\uff08EFA\uff09\uff0c\u63ed\u793a\u4e86\u5176\u6f5c\u5728\u56e0\u5b50\u7ed3\u6784\uff0c\u5e76\u5728\u53e6\u4e00\u534a\u6837\u672c\u4e0a\u8fdb\u884c\u4e86\u786e\u8ba4\u6027\u56e0\u5b50\u5206\u6790\uff08CFA\uff09\u4ee5\u9a8c\u8bc1\u7ed3\u6784\u3002", "result": "EFA\u652f\u6301\u53cc\u56e0\u5b50\u7ed3\u6784\uff08\u6280\u672f\u57fa\u7840\u8bbe\u65bd\u548c\u793e\u4f1a\u6280\u672f\u57fa\u7840\u8bbe\u65bd\uff09\uff0c\u89e3\u91ca\u603b\u65b9\u5dee\u768465%\u4e14\u5177\u6709\u9ad8\u8f7d\u8377\u3002CFA\u8bc1\u5b9e\u4e86\u4f18\u79c0\u7684\u6a21\u578b\u62df\u5408\u3002\u9879\u76ee\u7684\u6587\u672c\u53cd\u6620\u4e86\u8ba4\u77e5\u3001\u60c5\u611f\u548c\u884c\u4e3a\u7ec4\u4ef6\uff0c\u652f\u6301\u4e86\u8868\u9762\u7684\u5185\u5bb9\u6548\u5ea6\u3002\u4e24\u4e2a\u5b50\u5c3a\u5ea6\u90fd\u4e0e\u5de5\u4f5c\u6ee1\u610f\u5ea6\u3001\u611f\u77e5\u7684\u81ea\u4e3b\u6027\u548c\u5de5\u4f5c\u7684\u53cd\u9988\u76f8\u5173\uff0c\u652f\u6301\u6536\u655b\u6548\u5ea6\u3002\u56de\u5f52\u5206\u6790\u652f\u6301\u4e86\u4e0e\u6807\u51c6\u7684\u76f8\u5173\u6027\uff0c\u800c\u5f02\u8d28-\u540c\u6e90\u6bd4\uff08HTMT\uff09\u3001Fornell-Larcker\u6807\u51c6\u548c\u6a21\u578b\u6bd4\u8f83\u90fd\u652f\u6301\u4e86\u533a\u5206\u6548\u5ea6\u3002", "conclusion": "\u6240\u5f97\u51fa\u7684\u91cf\u8868\u662f\u8861\u91cf\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u6280\u672f\u548c\u793e\u4f1a\u6280\u672f\u57fa\u7840\u8bbe\u65bd\u6001\u5ea6\u7684\u6709\u6548\u5de5\u5177\u3002\u8be5\u5de5\u4f5c\u4e3a\u5c06\u5fc3\u7406\u6d4b\u91cf\u4e25\u683c\u6027\u878d\u5165\u5b9e\u8bc1\u548c\u8f6f\u4ef6\u884c\u4e3a\u5b66\u7814\u7a76\u505a\u51fa\u4e86\u4e00\u5b9a\u8d21\u732e\u3002"}}
{"id": "2512.01764", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.01764", "abs": "https://arxiv.org/abs/2512.01764", "authors": ["Kingshuk Haldar"], "title": "Trace-based, time-resolved analysis of MPI application performance using standard metrics", "comment": "Presented at and submitted to the International Parallel Tools Workshop 2025", "summary": "Detailed trace analysis of MPI applications is essential for performance engineering, but growing trace sizes and complex communication behaviour often render comprehensive visual inspection impractical. This work presents a trace-based calculation of time-resolved values of standard MPI performance metrics, load balance, serialisation, and transfer efficiency, by discretising execution traces into fixed or adaptive time segments. The implementation processes Paraver traces postmortem, reconstructing critical execution paths and handling common event anomalies, such as clock inconsistencies and unmatched MPI events, to robustly calculate metrics for each segment. The calculated per-window metric values expose transient performance bottlenecks that the timeaggregated metrics from existing tools may conceal. Evaluations on a synthetic benchmark and real-world applications (LaMEM and ls1-MarDyn) demonstrate how time-resolved metrics reveal localised performance bottlenecks obscured by global aggregates, offering a lightweight and scalable alternative even when trace visualisation is impractical.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u65f6\u95f4\u7a97\u53e3\u7684MPI\u6027\u80fd\u5206\u6790\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u6267\u884c\u8f68\u8ff9\u5206\u89e3\u4e3a\u56fa\u5b9a\u6216\u81ea\u9002\u5e94\u7684\u65f6\u95f4\u6bb5\uff0c\u8ba1\u7b97\u5e76\u5206\u6790\u6bcf\u4e2a\u65f6\u95f4\u6bb5\u5185\u7684\u6807\u51c6MPI\u6027\u80fd\u6307\u6807\uff0c\u4ee5\u63ed\u793a\u77ed\u6682\u7684\u6027\u80fd\u74f6\u9888\u3002", "motivation": "\u968f\u7740MPI\u5e94\u7528\u8f68\u8ff9\u6570\u636e\u7684\u589e\u957f\u548c\u901a\u4fe1\u884c\u4e3a\u7684\u590d\u6742\u5316\uff0c\u73b0\u6709\u7684\u6027\u80fd\u5206\u6790\u5de5\u5177\uff08\u5982PARAVER\u3001Vampir\u548cIntel Trace Analyzer\uff09\u5728\u5904\u7406\u5927\u89c4\u6a21\u6570\u636e\u65f6\u5b58\u5728\u4e00\u5b9a\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u53ef\u89c6\u5316\u4e0a\u3002\u4f20\u7edf\u7684\u5168\u5c40\u805a\u5408\u6307\u6807\u53ef\u80fd\u63a9\u76d6\u4e86\u5c40\u90e8\u6027\u80fd\u74f6\u9888\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5206\u6790\u5404\u65f6\u95f4\u6bb5\u6027\u80fd\u6570\u636e\u7684\u65b9\u6cd5\uff0c\u4ee5\u66f4\u597d\u5730\u53d1\u73b0\u6027\u80fd\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u65f6\u95f4\u6bb5\u7684MPI\u6027\u80fd\u5206\u6790\u65b9\u6cd5\uff0c\u5229\u7528Paraver\u8f68\u8ff9\u6570\u636e\u6784\u5efa\u5173\u952e\u6267\u884c\u8def\u5f84\u5e76\u5904\u7406\u5e38\u89c1\u4e8b\u4ef6\u5f02\u5e38\uff08\u5982\u65f6\u949f\u4e0d\u4e00\u81f4\u548c\u672a\u5339\u914d\u7684MPI\u4e8b\u4ef6\uff09\uff0c\u5e76\u6309\u56fa\u5b9a\u6216\u81ea\u9002\u5e94\u7684\u65f6\u95f4\u7a97\u53e3\u8fdb\u884c\u5206\u6790\uff0c\u8ba1\u7b97\u51fa\u6bcf\u4e2a\u7a97\u53e3\u5185\u7684\u6027\u80fd\u6307\u6807\u3002", "result": "\u5728\u5408\u6210\u57fa\u51c6\u548c\u771f\u5b9e\u5e94\u7528\uff08LaMEM\u548cls1-MarDyn\uff09\u4e0a\u9a8c\u8bc1\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u901a\u8fc7\u8ba1\u7b97\u65f6\u95f4\u5206\u8fa8\u7387\u6027\u80fd\u6307\u6807\u51c6\u786e\u5b9a\u4f4d\u6027\u80fd\u74f6\u9888\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u4e14\u53ef\u6269\u5c55\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u5c24\u5176\u5728\u8f68\u8ff9\u6570\u636e\u8fc7\u4e8e\u5e9e\u5927\u800c\u65e0\u6cd5\u8fdb\u884c\u53ef\u89c6\u5316\u5904\u7406\u65f6\u6548\u679c\u826f\u597d\u3002", "conclusion": "\u901a\u8fc7\u65f6\u95f4\u5206\u8fa8\u7387\u7684\u6027\u80fd\u5206\u6790\u65b9\u6cd5\uff0c\u80fd\u591f\u66f4\u52a0\u7ec6\u81f4\u5730\u5206\u6790\u548c\u53d1\u73b0MPI\u5e94\u7528\u4e2d\u7684\u6027\u80fd\u95ee\u9898\u3002"}}
{"id": "2512.00591", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2512.00591", "abs": "https://arxiv.org/abs/2512.00591", "authors": ["Weihua Xiao", "Zeng Wang", "Minghao Shao", "Raghu Vamshi Hemadri", "Ozgur Sinanoglu", "Muhammad Shafique", "Johann Knechtel", "Siddharth Garg", "Ramesh Karri"], "title": "TrojanLoC: LLM-based Framework for RTL Trojan Localization", "comment": null, "summary": "Hardware Trojans (HT s) are a persistent threat to integrated circuits, especially when inserted at the register-transfer level (RTL). Existing methods typically first convert the design into a graph, such as a gate-level netlist or an RTL-derived dataflow graph (DFG), and then use a graph neural network (GNN ) to obtain an embedding of that graph, which (i) loses compact RTL semantics, (ii) relies on shallow GNNs with limited receptive field, and (iii) is largely restricted to coarse, module-level binary HT detection. We propose TrojanLoC, an LLM-based framework for RTL-level HT localization. We use an RTL-finetuned LLM to derive module-level and line-level embeddings directly from RTL code, capturing both global design context and local semantics. Next, we train task-specific classifiers on these embeddings to perform module-level Trojan detection, type prediction, and fine-grained line-level localization. We also introduce TrojanInS, a large synthetic dataset of RTL designs with systematically injected Trojans from four effect-based categories, each accompanied by precise line-level annotations. Our experiments show that TrojanLoC achieves strong module-level performance, reaching 0.99 F1-score for Trojan detection, up to 0.68 higher than baseline, and 0.84 macro-F1 for Trojan-type classification. At the line level, TrojanLoc further achieves up to 0.93 macro-F1, enabling fine-grained localization of Trojan-relevant RTL lines", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLLM\u7684RTL\u7ea7\u786c\u4ef6\u6728\u9a6c\u5b9a\u4f4d\u6846\u67b6TrojanLoC\uff0c\u80fd\u76f4\u63a5\u5904\u7406\u548c\u6807\u6ce8RTL\u4ee3\u7801\uff0c\u7ed3\u5408\u5168\u5c40\u8bbe\u8ba1\u4e0a\u4e0b\u6587\u548c\u5c40\u90e8\u8bed\u4e49\u6267\u884c\u6a21\u5757\u7ea7\u548c\u884c\u7ea7\u7684\u6728\u9a6c\u68c0\u6d4b\u3001\u7c7b\u578b\u9884\u6d4b\u548c\u7cbe\u7ec6\u5316\u5b9a\u4f4d\u3002", "motivation": " \u786c\u4ef6\u6728\u9a6c\u5bf9\u96c6\u6210\u7535\u8def\u6784\u6210\u5a01\u80c1\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u56fe\u8f6c\u6362\u8fc7\u7a0b\u4e2d\u4e22\u5931RTL\u8bed\u4e49\uff0c\u53d7\u6d45\u5c42GNN\u9650\u5236\u4e14\u4ec5\u80fd\u7c97\u7cd9\u7684\u6a21\u5757\u7ea7\u4e8c\u8fdb\u5236\u68c0\u6d4b\u3002", "method": "\u4f7f\u7528RTL\u5fae\u8c03\u7684LLM\u76f4\u63a5\u8fdb\u884c\u6a21\u5757\u7ea7\u548c\u884c\u7ea7\u5d4c\u5165\uff0c\u6355\u83b7\u5168\u5c40\u8bbe\u8ba1\u6846\u67b6\u548c\u5c40\u90e8\u8bed\u4e49\uff1b\u5728\u8fd9\u4e9b\u5d4c\u5165\u7684\u57fa\u7840\u4e0a\u8bad\u7ec3\u7279\u5b9a\u4efb\u52a1\u5206\u7c7b\u5668\u6267\u884c\u6a21\u5757\u53ca\u884c\u7ea7\u786c\u4ef6\u6728\u9a6c\u68c0\u6d4b\uff0c\u7c7b\u578b\u9884\u6d4b\u548c\u5b9a\u4f4d\u3002\u540c\u65f6\u5f15\u5165\u4e86TrojanInS\u5927\u578b\u5408\u6210\u6570\u636e\u96c6\uff0c\u5305\u542b\u4ece\u56db\u4e2a\u57fa\u7840\u6548\u5e94\u7c7b\u522b\u7cfb\u7edf\u6ce8\u5165\u7684\u6728\u9a6c\uff0c\u6bcf\u4e2a\u90fd\u6709\u7cbe\u786e\u884c\u7ea7\u6ce8\u91ca\u3002", "result": " TrojanLoC\u8fbe\u5230\u4e86\u5f3a\u5927\u7684\u6a21\u5757\u7ea7\u6027\u80fd\uff0c\u5728\u6728\u9a6c\u68c0\u6d4b\u65b9\u9762\u8fbe\u5230\u4e860.99\u7684F1\u5f97\u5206\uff0c\u5728\u6728\u9a6c\u7c7b\u578b\u5206\u7c7b\u4e0a\u8fbe\u52300.84\u7684\u5e73\u5747F1,\u6bd4\u57fa\u7ebf\u9ad8\u51fa\u6700\u591a0.68\u3002\u5728\u884c\u7ea7\uff0cTrojanLoC\u8fdb\u4e00\u6b65\u5b9e\u73b0\u4e86\u6700\u59270.93\u7684\u5e73\u5747F1\uff0c\u4f7f\u5f97\u5bf9\u7279\u6d1b\u4f0a\u9a6c\u76f8\u5173\u7684RTL\u7ebf\u8fdb\u884c\u7cbe\u7ec6\u5316\u5b9a\u4f4d\u6210\u4e3a\u53ef\u80fd\u3002", "conclusion": "TrojanLoC\u662f\u5bf9\u73b0\u6709\u786c\u4ef6\u6728\u9a6c\u68c0\u6d4b\u65b9\u6cd5\u7684\u6709\u6548\u6539\u8fdb\uff0c\u80fd\u591f\u5728\u66f4\u7cbe\u7ec6\u7684\u5c42\u6b21\u4e0a\u8bc6\u522b\u548c\u5b9a\u4f4dRTL\u7ea7\u522b\u7684\u6728\u9a6c\u3002"}}
{"id": "2512.00479", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00479", "abs": "https://arxiv.org/abs/2512.00479", "authors": ["Yuta Kobayashi", "Vincent Jeanselme", "Shalmali Joshi"], "title": "Mind the data gap: Missingness Still Shapes Large Language Model Prognoses", "comment": "Presented at ML4H 2025 - Findings Track", "summary": "Data collection often reflects human decisions. In healthcare, for instance, a referral for a diagnostic test is influenced by the patient's health, their preferences, available resources, and the practitioner's recommendations. Despite the extensive literature on the informativeness of missingness, its implications on the performance of Large Language Models (LLMs) have not been studied. Through a series of experiments on data from Columbia University Medical Center, a large urban academic medical center, and MIMIC-IV, we demonstrate that patterns of missingness significantly impact zero-shot predictive performance. Notably, the explicit inclusion of missingness indicators at prompting benefits some while hurting other LLMs' zero-shot predictive performance and calibration, suggesting an inconsistent impact. The proposed aggregated analysis and theoretical insights suggest that larger models benefit from these interventions, while smaller models can be negatively impacted. The LLM paradigm risks obscuring the impact of missingness, often neglected even in conventional ML, even further. We conclude that there is a need for more transparent accounting and systematic evaluation of the impact of representing (informative) missingness on downstream performance.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\uff0c\u6570\u636e\u7f3a\u5931\u7684\u6a21\u5f0f\u5bf9\u96f6\u6b21\u5b66\u4e60\u6027\u80fd\u6709\u663e\u8457\u5f71\u54cd\uff0c\u663e\u5f0f\u5305\u542b\u7f3a\u5931\u6307\u6807\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u662f\u4e0d\u4e00\u81f4\u7684\u3002\u5efa\u8bae\u5927\u578b\u6a21\u578b\u4e0e\u5c0f\u578b\u6a21\u578b\u5728\u5e94\u5bf9\u7f3a\u5931\u6570\u636e\u65f6\u5e94\u6709\u4e0d\u540c\u7684\u7b56\u7565\u3002", "motivation": "\u5f53\u524d\u6587\u732e\u5bf9\u6570\u636e\u7f3a\u5931\u7684\u4e0d\u786e\u5b9a\u6027\u5e7f\u6cdb\u8ba8\u8bba\uff0c\u4f46\u5176\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5f71\u54cd\u8fd8\u672a\u88ab\u7814\u7a76\u3002", "method": "\u57fa\u4e8e\u54e5\u4f26\u6bd4\u4e9a\u5927\u5b66\u548cMIMIC-IV\u533b\u7597\u4e2d\u5fc3\u7684\u6570\u636e\uff0c\u5f00\u5c55\u4e00\u7cfb\u5217\u5b9e\u9a8c\u9a8c\u8bc1\u7f3a\u5931\u6570\u636e\u6a21\u5f0f\u7684\u5f71\u54cd\uff0c\u901a\u8fc7\u63d0\u793a\u663e\u5f0f\u5305\u542b\u7f3a\u5931\u6307\u6807\u5206\u6790\u3002", "result": "\u6570\u636e\u7f3a\u5931\u6a21\u5f0f\u663e\u8457\u5f71\u54cd\u96f6\u6b21\u5b66\u4e60\u6027\u80fd\uff0c\u663e\u5f0f\u5305\u542b\u7f3a\u5931\u6307\u6807\u5bf9LLMs\u7684\u5f71\u54cd\u4e0d\u4e00\u81f4\uff1b\u5927\u578b\u6a21\u578b\u53d7\u76ca\uff0c\u800c\u5c0f\u578b\u6a21\u578b\u53d7\u5f71\u54cd\u8f83\u5927\u3002", "conclusion": "\u9700\u8981\u5bf9\u7f3a\u5931\u6027\u7684\u5f71\u54cd\u8fdb\u884c\u66f4\u900f\u660e\u7684\u89c4\u5212\u548c\u7cfb\u7edf\u6027\u8bc4\u4f30\uff0c\u7279\u522b\u662f\u5728\u4e0b\u6e38\u6027\u80fd\u7684\u8868\u73b0\u4e0a\u3002"}}
{"id": "2512.00867", "categories": ["cs.SE", "cs.HC"], "pdf": "https://arxiv.org/pdf/2512.00867", "abs": "https://arxiv.org/abs/2512.00867", "authors": ["Obada Kraishan"], "title": "The AI Attribution Paradox: Transparency as Social Strategy in Open-Source Software Development", "comment": "23 pages, 7 figures, 9 tables", "summary": "AI coding assistants have transformed software development, raising questions about transparency and attribution practices. We examine the \"AI attribution paradox\": how developers strategically balance acknowledging AI assistance with managing community scrutiny. Analyzing 14,300 GitHub commits across 7,393 repositories from 2023-2025, we investigated attribution strategies and community responses across eight major AI tools. Results reveal widespread AI usage (95.2% of commits) but strategic attribution: only 29.5% employ explicit disclosure, with dramatic tool variation (Claude 80.5% versus Copilot 9.0%). Explicit attribution triggers modest scrutiny (23% more questions and 21% more comments) but tool choice matters 20-30 times more for predicting reception. Community sentiment remains neutral regardless of attribution type, suggesting curiosity rather than hostility. Temporal analyses show rapid norm evolution: explicit attribution increased from near-zero in early 2024 to 40% by late 2025, indicating community adaptation. These findings illuminate attribution as strategic communication rather than simple transparency, advancing understanding of algorithmic accountability and norm formation during technological transitions. We discuss implications for developers navigating disclosure decisions, platforms designing attribution mechanisms, and researchers studying emergent practices in AI-augmented collaborative work.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86AI\u7f16\u7a0b\u52a9\u624b\u5728\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u7684\u900f\u660e\u5ea6\u548c\u5f52\u5c5e\u5b9e\u8df5\u4e4b\u95f4\u7684\u77db\u76fe\uff0c\u901a\u8fc7\u5206\u67902023-2025\u5e74\u95f4\u768414,300\u4e2aGitHub\u63d0\u4ea4\uff0c\u63ed\u793a\u4e86\u5f00\u53d1\u8005\u5982\u4f55\u6218\u7565\u6027\u5730\u5e73\u8861AI\u8f85\u52a9\u5de5\u5177\u7684\u58f0\u660e\u548c\u793e\u533a\u5ba1\u67e5\u3002", "motivation": "AI\u7f16\u7801\u52a9\u624b\u5bf9\u8f6f\u4ef6\u5f00\u53d1\u7684\u53d8\u9769\u5e26\u6765\u4e86\u5173\u4e8e\u900f\u660e\u5ea6\u548c\u5f52\u5c5e\u5b9e\u8df5\u7684\u7591\u95ee\uff0c\u9700\u8981\u6df1\u5165\u7814\u7a76\u5f00\u53d1\u8005\u5982\u4f55\u7ba1\u7406AI\u4f7f\u7528\u58f0\u660e\uff0c\u4ee5\u53ca\u793e\u533a\u7684\u54cd\u5e94\u3002", "method": "\u5206\u6790\u4e867,393\u4e2a\u4ed3\u5e93\u768414,300\u4e2a\u63d0\u4ea4\uff0c\u8003\u5bdf\u4e86\u516b\u4e2a\u4e3b\u8981AI\u5de5\u5177\u7684\u5f52\u5c5e\u7b56\u7565\u548c\u793e\u533a\u7684\u53cd\u5e94\uff0c\u5e76\u8fdb\u884c\u4e86\u65f6\u95f4\u5206\u6790\u3002", "result": "95.2%\u7684\u63d0\u4ea4\u4e2d\u4f7f\u7528\u4e86AI\uff0c\u4f46\u53ea\u670929.5%\u660e\u786e\u58f0\u660e\u4f7f\u7528\uff0c\u4e0d\u540c\u5de5\u5177\u95f4\u5dee\u5f02\u5927\uff08Claude 80.5% vs Copilot 9.0%)\u3002\u660e\u786e\u58f0\u660e\u4f1a\u5f15\u8d77\u7a0d\u5fae\u591a\u7684\u793e\u533a\u5173\u6ce8\uff0c\u4f46\u5de5\u5177\u9009\u62e9\u5bf9\u9884\u6d4b\u63a5\u53d7\u5ea6\u5f71\u54cd\u66f4\u5927\u3002\u793e\u533a\u60c5\u7eea\u4e2d\u7acb\uff0c\u663e\u793a\u597d\u5947\u5fc3\u5927\u4e8e\u654c\u610f\uff0c\u5e76\u4e14\u5f52\u5c5e\u505a\u6cd5\u5728\u4e09\u5e74\u5185\u8fc5\u901f\u6f14\u53d8\u3002", "conclusion": "\u7814\u7a76\u9610\u660e\u4e86AI\u5de5\u5177\u5f52\u5c5e\u662f\u4e00\u79cd\u6218\u7565\u6c9f\u901a\uff0c\u800c\u975e\u7b80\u5355\u900f\u660e\uff0c\u63a8\u8fdb\u4e86\u5bf9\u7b97\u6cd5\u95ee\u8d23\u5236\u548c\u6280\u672f\u8fc7\u6e21\u65f6\u671f\u89c4\u8303\u5f62\u6210\u7684\u7406\u89e3\uff0c\u5e76\u5bf9\u5f00\u53d1\u8005\u5728\u5982\u4f55\u9009\u62e9\u58f0\u660e\u4f7f\u7528AI\uff0c\u5e73\u53f0\u5982\u4f55\u8bbe\u8ba1\u5f52\u5c5e\u673a\u5236\uff0c\u4ee5\u53ca\u76f8\u5173\u7814\u7a76\u8005\u5982\u4f55\u7814\u7a76AI\u589e\u5f3a\u534f\u4f5c\u5de5\u4f5c\u7684\u65b0\u5b9e\u8df5\u63d0\u4f9b\u4e86\u6307\u5bfc\u610f\u4e49\u3002"}}
{"id": "2512.00635", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2512.00635", "abs": "https://arxiv.org/abs/2512.00635", "authors": ["Archisman Ghosh"], "title": "Extended Abstract: Synthesizable Low-overhead Circuit-level Countermeasures and Pro-Active Detection Techniques for Power and EM SCA", "comment": "This extended abstract is archived for educational purposes as an example for different PhD forum competitions. Total page is 3", "summary": "The gamut of todays internet-connected embedded devices has led to increased concerns regarding the security and confidentiality of data. Most internet-connected embedded devices employ mathematically secure cryptographic algorithms to address security vulnerabilities. Despite such mathematical guarantees, as these algorithms are often implemented in silicon, they leak critical information in terms of power consumption, electromagnetic (EM) radiation, timing, cache hits and misses, photonic emission and so on, leading to side-channel analysis (SCA) attacks. This thesis focuses on low overhead generic circuit-level yet synthesizable countermeasures against power and EM SCA. Existing countermeasures (including proposed) still have relatively high overhead which bars them from being used in energy-constraint IoT devices. We propose a zero-overhead integrated inductive sensor which is able to detect i)EM SCA ii) Clock glitch-based Fault Injection Attack (FIA), and iii) Voltage-glitch based Fault Injection Attack by using a simple ML algorithm. Advent of quantum computer research will open new possibilities for theoretical attacks against existing cryptographic protocols. National Institute of Standard & Technology (NIST) has standardized post-quantum cryptographic algorithms to secure crypto-systems against quantum adversary. I contribute to the standardization procedure by introducing the first silicon-verified Saber (a NIST finalist modulo Learning with Rounding scheme) which consumes lowest energy and area till date amongst all the candidates.", "AI": {"tldr": "\u672c\u6587\u4e3b\u8981\u5173\u6ce8\u5bf9\u5d4c\u5165\u5f0f\u8bbe\u5907\u7684\u65c1\u9053\u653b\u51fb\uff08SCA\uff09\uff0c\u5e76\u63d0\u51fa\u4e86\u4e24\u79cd\u4f4e\u5f00\u9500\u7684\u786c\u4ef6\u7efc\u5408\u5bf9\u6297\u63aa\u65bd\u3002", "motivation": "\u7531\u4e8e\u76ee\u524d\u7684\u4e92\u8054\u7f51\u8fde\u63a5\u5d4c\u5165\u5f0f\u8bbe\u5907\u6570\u636e\u5b89\u5168\u6027\u4e0d\u8db3\uff0c\u5b58\u5728\u65c1\u9053\u653b\u51fb\uff08SCA\uff09\u7684\u5a01\u80c1\uff0c\u7279\u522b\u662f\u5728\u5b9e\u73b0\u5bc6\u7801\u7b97\u6cd5\u65f6\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u79cd\u65b0\u578b\u786c\u4ef6\u7efc\u5408\u5bf9\u6297\u63aa\u65bd\uff1a1) \u91c7\u7528\u96c6\u6210\u7684\u7535\u611f\u4f20\u611f\u5668\u68c0\u6d4b\u7535\u78c1\u6ce2\u65c1\u9053\u5206\u6790\u3001\u65f6\u949f\u6545\u969c\u6ce8\u5165\u653b\u51fb\u4ee5\u53ca\u7535\u538b\u6545\u969c\u6ce8\u5165\u653b\u51fb 2) \u53c2\u4e0e\u4e86NIST\u540e\u91cf\u5b50\u5bc6\u7801\u6807\u51c6\u5316\u7684\u8fc7\u7a0b\uff0c\u8d21\u732e\u4e86\u80fd\u91cf\u548c\u9762\u79ef\u6548\u7387\u6700\u9ad8\u7248\u672c\u7684Saber\u65b9\u6848\u3002", "result": "\u63d0\u51fa\u7684\u7535\u611f\u4f20\u611f\u5668\u5728\u68c0\u6d4bSCA\u4e0a\u51e0\u4e4e\u65e0\u989d\u5916\u5f00\u9500\u3002\u53c2\u4e0e\u5b9e\u73b0\u7684Saber\u65b9\u6848\u662fNIST\u6807\u51c6\u5316\u6700\u7ec8\u5019\u9009\u4e4b\u4e00\uff0c\u5176\u80fd\u91cf\u6d88\u8017\u548c\u82af\u7247\u9762\u79ef\u6700\u4f4e\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u4e86\u65c1\u9053\u653b\u51fb\u7684\u98ce\u9669\uff0c\u4e14\u6613\u4e8e\u5728\u80fd\u91cf\u53d7\u9650\u7684\u7269\u8054\u7f51\u8bbe\u5907\u4e2d\u5e94\u7528\u3002"}}
{"id": "2512.00601", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00601", "abs": "https://arxiv.org/abs/2512.00601", "authors": ["Boyang Gu", "Hongjian Zhou", "Bradley Max Segal", "Jinge Wu", "Zeyu Cao", "Hantao Zhong", "Lei Clifton", "Fenglin Liu", "David A. Clifton"], "title": "Clinical-R1: Empowering Large Language Models for Faithful and Comprehensive Reasoning with Clinical Objective Relative Policy Optimization", "comment": null, "summary": "Recent advances in large language models (LLMs) have shown strong reasoning capabilities through large-scale pretraining and post-training reinforcement learning, demonstrated by DeepSeek-R1. However, current post-training methods, such as Grouped Relative Policy Optimization (GRPO), mainly reward correctness, which is not aligned with the multi-dimensional objectives required in high-stakes fields such as medicine, where reasoning must also be faithful and comprehensive. We introduce Clinical-Objective Relative Policy Optimization (CRPO), a scalable, multi-objective, verifiable reinforcement learning method designed to align LLM post-training with clinical reasoning principles. CRPO integrates rule-based and verifiable reward signals that jointly optimize accuracy, faithfulness, and comprehensiveness without relying on human annotation. To demonstrate its effectiveness, we train Clinical-R1-3B, a 3B-parameter model for clinical reasoning. The experiments on three benchmarks demonstrate that our CRPO substantially improves reasoning on truthfulness and completeness over standard GRPO while maintaining comfortable accuracy enhancements. This framework provides a scalable pathway to align LLM reasoning with clinical objectives, enabling safer and more collaborative AI systems for healthcare while also highlighting the potential of multi-objective, verifiable RL methods in post-training scaling of LLMs for medical domains.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86CRPO\uff0c\u4e00\u79cd\u53ef\u6269\u5c55\u3001\u591a\u76ee\u6807\u3001\u53ef\u9a8c\u8bc1\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u533b\u5b66\u9886\u57df\u4e2d\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u540e\u8bad\u7ec3\uff0c\u4f18\u5316\u51c6\u786e\u6027\u3001\u5fe0\u5b9e\u6027\u548c\u5168\u9762\u6027\u3002\u5b9e\u9a8c\u663e\u793a\uff0c\u76f8\u8f83\u4e8e\u6807\u51c6GRPO\u65b9\u6cd5\uff0cCRPO\u5728\u771f\u5b9e\u6027\u548c\u5b8c\u6574\u6027\u65b9\u9762\u6709\u663e\u8457\u63d0\u5347\uff0c\u540c\u65f6\u4fdd\u6301\u826f\u597d\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u5f53\u524d\u7684\u540e\u8bad\u7ec3\u65b9\u6cd5\uff08\u5982GRPO\uff09\u4e3b\u8981\u5956\u52b1\u6b63\u786e\u6027\uff0c\u800c\u533b\u5b66\u9886\u57df\u9700\u8981\u591a\u7ef4\u5ea6\u76ee\u6807\uff0c\u5305\u62ec\u63a8\u7406\u7684\u5fe0\u5b9e\u6027\u548c\u5168\u9762\u6027\u3002", "method": "CRPO\u7ed3\u5408\u57fa\u4e8e\u89c4\u5219\u548c\u53ef\u9a8c\u8bc1\u7684\u5956\u52b1\u4fe1\u53f7\uff0c\u5171\u540c\u4f18\u5316\u51c6\u786e\u6027\u3001\u5fe0\u5b9e\u6027\u548c\u5168\u9762\u6027\uff0c\u65e0\u9700\u4f9d\u8d56\u4eba\u5de5\u6807\u6ce8\u3002", "result": "\u5728\u4e09\u9879\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCRPO\u663e\u8457\u63d0\u9ad8\u4e86\u771f\u5b9e\u6027\u548c\u5b8c\u6574\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u51c6\u786e\u6027\u589e\u5f3a\u3002", "conclusion": "\u8be5\u6846\u67b6\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u9014\u5f84\uff0c\u4ee5\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u4e0e\u4e34\u5e8a\u76ee\u6807\u5bf9\u9f50\uff0c\u4e3a\u533b\u7597\u9886\u57df\u5e26\u6765\u66f4\u5b89\u5168\u548c\u66f4\u5177\u534f\u4f5c\u6027\u7684\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\uff0c\u7a81\u51fa\u4e86\u591a\u76ee\u6807\u53ef\u9a8c\u8bc1\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u533b\u5b66\u9886\u57df\u540e\u8bad\u7ec3\u6269\u5c55\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2512.00869", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.00869", "abs": "https://arxiv.org/abs/2512.00869", "authors": ["Miikka Kuutila", "Paul Ralph", "Huilian Sophie Qiu", "Ronnie de Souza Santos", "Morakot Choetkiertikul", "Rana Alkadhi", "Xavier Devroey", "Gregorio Robles", "Hideaki Hata", "Sebastian Baltes", "Hera Arif", "Vladimir Kovalenko", "Shalini Chakraborty", "Eray Tuzun", "Gianisa Adisaputri"], "title": "Staying or Leaving? How Job Satisfaction, Embeddedness and Antecedents Predict Turnover Intentions of Software Professionals", "comment": "11 pages, 1 figure, 7 tables. Accepted to ICSE 2026 research track", "summary": "Context: Voluntary turnover is common in the software industry, increasing recruitment and onboarding costs and the risk of losing organizational and tacit knowledge. Objective: This study investigates how job satisfaction, work-life balance, job embeddedness, and their antecedents, including job quality, personality traits, attitudes toward technical and sociotechnical infrastructure, and perceptions of organizational justice, relate to software professionals' turnover intentions. Method: We conducted a geographically diverse cross-sectional survey of software professionals (N = 224) and analyzed the data using partial least squares structural equation modeling (PLS-SEM). Our model includes both reflective and formative constructs and tests 15 hypotheses grounded in occupational psychology and software engineering literature. Results: Job satisfaction and embeddedness were significantly negatively associated with software professionals' turnover intentions, while work-life balance showed no direct effect. The strongest antecedents for job satisfaction were work-life balance and job quality, while organizational justice was the strongest predictor of job embeddedness. Discussion: The resulting PLS-SEM model has considerably higher explanatory power for key outcome variables than previous work conducted in the software development context, highlighting the importance of both psychological (e.g., job satisfaction, job embeddedness) and organizational (e.g., organizational justice, job quality) factors in understanding turnover intentions of software professionals. Our results imply that improving job satisfaction and job embeddedness is the key to retaining software professionals. In turn, enhancing job quality, supporting work-life balance, and ensuring high organizational justice can improve job satisfaction and embeddedness, indirectly reducing turnover intentions.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2512.00645", "categories": ["cs.CR", "cs.DB"], "pdf": "https://arxiv.org/pdf/2512.00645", "abs": "https://arxiv.org/abs/2512.00645", "authors": ["Boyd Franken", "Hong-Hanh Nguyen-Le", "Nhien-An Le-Khac"], "title": "Blockchain-based vs. SQL Database Systems for Digital Twin Evidence Management: A Comparative Forensic Analysis", "comment": "Accepted at EAI International Conference on Digital Forensics & Cyber Crime 2025", "summary": "Digital forensics faces unprecedented challenges with the emergence of digital twins and metaverse technologies. This paper presents the first comparative analysis between blockchain-based and traditional database systems for managing digital twin evidence in forensic investigations. We conducted controlled experiments comparing the Ethereum blockchain with IPFS storage against traditional SQL databases for digital twin evidence management. Our findings reveal that while blockchain provides superior data integrity and immutability, crucial for forensic applications, traditional databases offer better performance consistency. The blockchain implementation showed faster average storage times but higher variability in retrieval operations. Both systems maintained forensic integrity through hash verification, though blockchain's immutable nature provides additional security guarantees essential for legal proceedings. This research contributes to the development of robust digital forensic methodologies for emerging technologies in the metaverse era.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2512.00670", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00670", "abs": "https://arxiv.org/abs/2512.00670", "authors": ["He-Yen Hsieh", "Hong Wang", "H. T. Kung"], "title": "EDIT: Early Diffusion Inference Termination for dLLMs Based on Dynamics of Training Gradients", "comment": "22 pages, 11 figures", "summary": "Diffusion-based large language models (dLLMs) refine token generations through iterative denoising, but answers often stabilize before all steps complete. We propose EDIT (Early Diffusion Inference Termination), an inference-time criterion that adaptively stops denoising once sufficient reasoning stability relative to training-time reasoning is detected. EDIT monitors the alignment between token activations and a reasoning map derived from AdamW-aggregated LoRA updates captured during supervised fine-tuning (SFT). During training, optimization dynamics generate rich metadata about parameter importance that in prior methods is typically discarded upon model release. We preserve this information as a compact representation of learned reasoning pathways. During inference, alignment scores are converted to a distribution over the tokens already unmasked at the current denoising step, and convergence is detected when KL divergence between consecutive steps falls below a threshold on the matched unmasked (visible) tokens. Across reasoning benchmarks, EDIT reduces diffusion steps by 11.8% to 68.3% while preserving or improving accuracy in most settings, with approximately 0.02% storage overhead (about 1.5-2 MB for all QKV modules across 32 blocks in an 8 GB model). By utilizing training-gradient dynamics, our work opens a new research direction for reducing dLLM inference time and cost.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2512.01141", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.01141", "abs": "https://arxiv.org/abs/2512.01141", "authors": ["Muhammad Yousuf", "Akshat Bagade", "Chhittebbayi Penugonda", "Maanas Baraya"], "title": "Neural Variable Name Repair: Learning to Rename Identifiers for Readability", "comment": null, "summary": "Developers routinely work with source files whose variable names are generic or misleading, and with teams moving quickly, many functions are left undocumented. This slows comprehension, increases the risk of subtle bugs, and makes it harder for both humans and large language models (LLMs) to reason about code. We study variable name repair: given a real C++ function where all occurrences of one local or parameter name have been replaced by a placeholder (e.g. ID 1), the goal is to generate a natural, descriptive replacement name. We automatically construct this task from the C++ portion of BigCode's The Stack by parsing functions with Tree-sitter, masking a single identifier, and treating the original name as supervision. On top of Llama 3.1-8B, we build a pipeline with (i) warmup and dropout schedules for more stable fine-tuning, (ii) LoRA adapters for efficient specialization on identifier repair, and (iii) a dual-encoder reranker over top-k generator candidates. We evaluate using exact match, Top-5 Hit, and an embedding-based partial similarity score (0-100) that gives credit for near synonyms and format variants (e.g., jsonValue vs. json). On a held-out set of 200 C++ functions, a zero-shot Llama 3.1 baseline reaches 6.1 percent exact match. Our best LoRA-tuned model (with warmup and dropout) achieves 43.1 percent exact match, 50.2 percent Top-5 Hit, and an 82.03 partial-match score. A dual encoder reranker further improves selection quality without modifying the underlying generator, suggesting that task-specific fine-tuning plus reranking is a promising approach for practical identifier repair tools.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2512.00713", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00713", "abs": "https://arxiv.org/abs/2512.00713", "authors": ["Haoyu Shen", "Weimin Lyu", "Haotian Xu", "Tengfei Ma"], "title": "Concept-Guided Backdoor Attack on Vision Language Models", "comment": null, "summary": "Vision-Language Models (VLMs) have achieved impressive progress in multimodal text generation, yet their rapid adoption raises increasing concerns about security vulnerabilities. Existing backdoor attacks against VLMs primarily rely on explicit pixel-level triggers or imperceptible perturbations injected into images. While effective, these approaches reduce stealthiness and remain vulnerable to image-based defenses. We introduce concept-guided backdoor attacks, a new paradigm that operates at the semantic concept level rather than on raw pixels. We propose two different attacks. The first, Concept-Thresholding Poisoning (CTP), uses explicit concepts in natural images as triggers: only samples containing the target concept are poisoned, causing the model to behave normally in all other cases but consistently inject malicious outputs whenever the concept appears. The second, CBL-Guided Unseen Backdoor (CGUB), leverages a Concept Bottleneck Model (CBM) during training to intervene on internal concept activations, while discarding the CBM branch at inference time to keep the VLM unchanged. This design enables systematic replacement of a targeted label in generated text (for example, replacing \"cat\" with \"dog\"), even when the replacement behavior never appears in the training data. Experiments across multiple VLM architectures and datasets show that both CTP and CGUB achieve high attack success rates while maintaining moderate impact on clean-task performance. These findings highlight concept-level vulnerabilities as a critical new attack surface for VLMs.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2512.00683", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00683", "abs": "https://arxiv.org/abs/2512.00683", "authors": ["Wu Yonggang"], "title": "Model of human cognition", "comment": null, "summary": "The development of large language models (LLMs) is limited by a lack of explainability, the absence of a unifying theory, and prohibitive operational costs. We propose a neuro-theoretical framework for the emergence of intelligence in systems that is both functionally robust and biologically plausible. The model provides theoretical insights into cognitive processes such as decision-making and problem solving, and a computationally efficient approach for the creation of explainable and generalizable artificial intelligence.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2512.01155", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.01155", "abs": "https://arxiv.org/abs/2512.01155", "authors": ["Krishna Kumaar Sharma"], "title": "Beyond Greenfield: AI-Driven Productivity in Documentation and Brownfield Engineering", "comment": "53 pages, 7 figures", "summary": "Brownfield engineering work involving legacy systems, incomplete documentation, and fragmented architectural knowledge poses unique challenges for the effective use of large language models (LLMs). Prior research has largely focused on greenfield or synthetic tasks, leaving a gap in structured workflows for complex, context-heavy environments. This paper introduces the Discover-Define-Deliver (D3) Framework, a disciplined LLM-assisted workflow that combines role-separated prompting strategies with applied best practices for navigating ambiguity in brownfield systems. The framework incorporates a dual-agent prompting architecture in which a Builder model generates candidate outputs and a Reviewer model provides structured critique to improve reliability. I conducted an exploratory survey study with 52 software practitioners who applied the D3 workflow to real-world engineering tasks such as legacy system exploration, documentation reconstruction, and architectural refactoring. Respondents reported perceived improvements in task clarity, documentation quality, and cognitive load, along with self-estimated productivity gains. In this exploratory study, participants reported a weighted average productivity improvement of 26.9%, reduced cognitive load for approximately 77% of participants, and reduced rework for 83% during the Define phase. As these findings are self-reported and not derived from controlled experiments, they should be interpreted as preliminary evidence of practitioner sentiment rather than causal effects. The results highlight both the potential and limitations of structured LLM workflows for legacy engineering systems and motivate future controlled evaluations.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2512.00741", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00741", "abs": "https://arxiv.org/abs/2512.00741", "authors": ["Bojing Li", "Duo Zhong", "Dharani Nadendla", "Gabriel Terceros", "Prajna Bhandar", "Raguvir S", "Charles Nicholas"], "title": "MASCOT: Analyzing Malware Evolution Through A Well-Curated Source Code Dataset", "comment": "11 pages, 6 figures, conference paper; submitted to IEEE BigData 2025 CyberHunt workshop", "summary": "In recent years, the explosion of malware and extensive code reuse have formed complex evolutionary connections among malware specimens. The rapid pace of development makes it challenging for existing studies to characterize recent evolutionary trends. In addition, intuitive tools to untangle these intricate connections between malware specimens or categories are urgently needed. This paper introduces a manually-reviewed malware source code dataset containing 6032 specimens. Building on and extending current research from a software engineering perspective, we systematically evaluate the scale, development costs, code quality, as well as security and dependencies of modern malware. We further introduce a multi-view genealogy analysis to clarify malware connections: at an overall view, this analysis quantifies the strength and direction of connections among specimens and categories; at a detailed view, it traces the evolutionary histories of individual specimens. Experimental results indicate that, despite persistent shortcomings in code quality, malware specimens exhibit an increasing complexity and standardization, in step with the development of mainstream software engineering practices. Meanwhile, our genealogy analysis intuitively reveals lineage expansion and evolution driven by code reuse, providing new evidence and tools for understanding the formation and evolution of the malware ecosystem.", "AI": {"tldr": "\u8fd1\u5e74\u6765\uff0c\u6076\u610f\u8f6f\u4ef6\u5448\u73b0\u7206\u70b8\u6027\u589e\u957f\u4e0e\u5927\u89c4\u6a21\u4ee3\u7801\u590d\u7528\uff0c\u9020\u6210\u6837\u672c\u95f4\u590d\u6742\u7684\u6f14\u5316\u5173\u7cfb\u3002\u73b0\u6709\u7814\u7a76\u96be\u4ee5\u53ca\u65f6\u6355\u6349\u6700\u65b0\u8d8b\u52bf\uff0c\u4e14\u7f3a\u4e4f\u76f4\u89c2\u5de5\u5177\u89e3\u6790\u6837\u672c\u6216\u7c7b\u522b\u95f4\u7684\u5173\u8054\u3002\u672c\u8bba\u6587\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5305\u542b6032\u4e2a\u6837\u672c\u7684\u624b\u52a8\u5ba1\u6838\u4ee3\u7801\u6570\u636e\u96c6\uff0c\u4ece\u8f6f\u4ef6\u5de5\u7a0b\u5b66\u89d2\u5ea6\u5ef6\u4f38\u5e76\u6269\u5c55\u4e86\u73b0\u6709\u7814\u7a76\uff0c\u5bf9\u73b0\u4ee3\u6076\u610f\u8f6f\u4ef6\u7684\u89c4\u6a21\u3001\u5f00\u53d1\u6210\u672c\u3001\u4ee3\u7801\u8d28\u91cf\u3001\u5b89\u5168\u6027\u53ca\u4f9d\u8d56\u6027\u8fdb\u884c\u4e86\u7cfb\u7edf\u8bc4\u4f30\u3002\u5f15\u5165\u4e86\u591a\u89c6\u56fe\u65cf\u8c31\u5206\u6790\u4ee5\u9610\u660e\u6076\u610f\u8f6f\u4ef6\u7684\u5173\u8054\uff1a\u4ece\u6574\u4f53\u89c6\u56fe\u5206\u6790\u4e86\u6837\u672c\u548c\u7c7b\u522b\u95f4\u8fde\u63a5\u5f3a\u5ea6\u548c\u65b9\u5411\uff1b\u4ece\u7ec6\u8282\u89c6\u56fe\u8ffd\u6eaf\u4e86\u5355\u4e2a\u6837\u672c\u7684\u6f14\u5316\u5386\u53f2\u3002\u5b9e\u9a8c\u663e\u793a\uff0c\u5c3d\u7ba1\u4ee3\u7801\u8d28\u91cf\u6301\u7eed\u5b58\u5728\u95ee\u9898\uff0c\u4f46\u6076\u610f\u8f6f\u4ef6\u6837\u672c\u5448\u73b0\u51fa\u65e5\u76ca\u589e\u957f\u7684\u590d\u6742\u6027\u548c\u6807\u51c6\u5316\uff0c\u4e0e\u4e3b\u6d41\u8f6f\u4ef6\u5de5\u7a0b\u7684\u53d1\u5c55\u4fdd\u6301\u540c\u6b65\u3002\u65cf\u8c31\u5206\u6790\u76f4\u89c2\u63ed\u793a\u4e86\u7531\u4ee3\u7801\u590d\u7528\u9a71\u52a8\u7684\u8c31\u7cfb\u6269\u5f20\u548c\u6f14\u5316\uff0c\u4e3a\u7406\u89e3\u6076\u610f\u8f6f\u4ef6\u751f\u6001\u7cfb\u7edf\u7684\u5f62\u6210\u548c\u8fdb\u5316\u63d0\u4f9b\u4e86\u65b0\u8bc1\u636e\u548c\u5de5\u5177\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u96be\u4ee5\u53ca\u65f6\u6355\u6349\u6700\u65b0\u8d8b\u52bf\uff0c\u4e14\u7f3a\u4e4f\u76f4\u89c2\u5de5\u5177\u89e3\u6790\u6837\u672c\u6216\u7c7b\u522b\u95f4\u7684\u5173\u8054\u3002", "method": "\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5305\u542b6032\u4e2a\u6837\u672c\u7684\u624b\u52a8\u5ba1\u6838\u4ee3\u7801\u6570\u636e\u96c6\uff0c\u5e76\u5f15\u5165\u4e86\u591a\u89c6\u56fe\u65cf\u8c31\u5206\u6790\u4ee5\u9610\u660e\u6076\u610f\u8f6f\u4ef6\u7684\u5173\u8054\u3002", "result": "\u6076\u610f\u8f6f\u4ef6\u6837\u672c\u5448\u73b0\u51fa\u65e5\u76ca\u589e\u957f\u7684\u590d\u6742\u6027\u548c\u6807\u51c6\u5316\uff0c\u65cf\u8c31\u5206\u6790\u63ed\u793a\u7531\u4ee3\u7801\u590d\u7528\u9a71\u52a8\u7684\u8c31\u7cfb\u6269\u5f20\u548c\u6f14\u5316\uff0c\u4e3a\u7406\u89e3\u6076\u610f\u8f6f\u4ef6\u751f\u6001\u7cfb\u7edf\u7684\u5f62\u6210\u548c\u8fdb\u5316\u63d0\u4f9b\u4e86\u65b0\u8bc1\u636e\u548c\u5de5\u5177\u3002", "conclusion": "\u4ece\u8f6f\u4ef6\u5de5\u7a0b\u7684\u89d2\u5ea6\u5206\u6790\uff0c\u672c\u6587\u63d0\u4f9b\u4e86\u5bf9\u73b0\u4ee3\u6076\u610f\u8f6f\u4ef6\u8fdb\u5316\u5386\u7a0b\u7684\u91cd\u8981\u89c1\u89e3\u548c\u5de5\u5177\u3002"}}
{"id": "2512.00804", "categories": ["cs.CR", "cs.AI", "cs.DB"], "pdf": "https://arxiv.org/pdf/2512.00804", "abs": "https://arxiv.org/abs/2512.00804", "authors": ["Hao Wu", "Prateek Saxena"], "title": "Bias Injection Attacks on RAG Databases and Sanitization Defenses", "comment": null, "summary": "This paper explores attacks and defenses on vector databases in retrieval-augmented generation (RAG) systems. Prior work on knowledge poisoning attacks primarily inject false or toxic content, which fact-checking or linguistic analysis easily detects. We reveal a new and subtle threat: bias injection attacks, which insert factually correct yet semantically biased passages into the knowledge base to covertly influence the ideological framing of answers generated by large language models (LLMs). We demonstrate that these adversarial passages, though linguistically coherent and truthful, can systematically crowd out opposing views from the retrieved context and steer LLM answers toward the attacker's intended perspective.\n  We precisely characterize this class of attacks and then develop a post-retrieval filtering defense, BiasDef. We construct a comprehensive benchmark based on public question answering datasets to evaluate them. Our results show that: (1) the proposed attack induces significant perspective shifts in LLM answers, effectively evading existing retrieval-based sanitization defenses; and (2) BiasDef outperforms existing methods by reducing adversarial passages retrieved by 15\\% which mitigates perspective shift by 6.2\\times in answers, while enabling the retrieval of 62\\% more benign passages.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86RAG\u7cfb\u7edf\u4e2d\u9488\u5bf9\u5411\u91cf\u6570\u636e\u5e93\u7684\u653b\u51fb\u548c\u9632\u5fa1\u3002", "motivation": "\u77e5\u8bc6\u5e93\u4e2d\u6bd2\u653b\u51fb\u4e3b\u8981\u662f\u6ce8\u5165\u5047\u6216\u6709\u5bb3\u5185\u5bb9\uff0c\u4f46\u8be5\u8bba\u6587\u63ed\u793a\u4e86\u4e00\u79cd\u65b0\u7684\u548c\u5fae\u5999\u5a01\u80c1\uff1a\u504f\u89c1\u6ce8\u5165\u653b\u51fb\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u653b\u51fb\u65b9\u5f0f\uff1a\u504f\u89c1\u6ce8\u5165\u653b\u51fb\uff1b\u5e76\u5f00\u53d1\u4e86\u4e00\u79cd\u65b0\u7684\u9632\u5fa1\u65b9\u5f0f\uff1aBiasDef\u3002", "result": "\u8be5\u8bba\u6587\u63d0\u51fa\u7684\u653b\u51fb\u65b9\u5f0f\u6709\u6548\u56de\u907f\u4e86\u73b0\u6709\u9632\u5fa1\u63aa\u65bd\uff0c\u5e76\u4e14\u65b0\u5f00\u53d1\u7684\u9632\u5fa1\u65b9\u5f0fBiasDef\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u7684\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u8bba\u6587\u63ed\u793a\u4e86\u4e00\u79cd\u65b0\u7684\u504f\u89c1\u6ce8\u5165\u653b\u51fb\uff0c\u8be5\u653b\u51fb\u4e0d\u6613\u88ab\u5bdf\u89c9\u5e76\u4e14\u80fd\u591f\u5f71\u54cd\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u7684\u56de\u7b54\uff0c\u5e76\u5c55\u793a\u4e86\u65b0\u5f00\u53d1\u7684\u9632\u5fa1\u65b9\u5f0fBiasDef\u7684\u6709\u6548\u3002"}}
{"id": "2512.00722", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00722", "abs": "https://arxiv.org/abs/2512.00722", "authors": ["Jiaming Xu", "Jiayi Pan", "Hanzhen Wang", "Yongkang Zhou", "Jiancai Ye", "Yu Wang", "Guohao Dai"], "title": "SpeContext: Enabling Efficient Long-context Reasoning with Speculative Context Sparsity in LLMs", "comment": "Accepted by ASPLOS 2026", "summary": "In this paper, we point out that the objective of the retrieval algorithms is to align with the LLM, which is similar to the objective of knowledge distillation in LLMs. We analyze the similarity in information focus between the distilled language model(DLM) and the original LLM from the perspective of information theory, and thus propose a novel paradigm that leverages a DLM as the retrieval algorithm. Based on the insight, we present SpeContext, an algorithm and system co-design for long-context reasoning. (1) At the algorithm level, SpeContext proposes lightweight retrieval head based on the head-level attention weights of DLM, achieving > 90% parameters reduction by pruning the redundancy. (2) At the system level, SpeContext designs an asynchronous prefetch dataflow via the elastic loading strategy, effectively overlapping KV cache retrieval with the LLM computation. (3) At the compilation level, SpeContext constructs the theoretical memory model and implements an adaptive memory management system to achieve acceleration by maximizing GPU memory utilization. We deploy and evaluate SpeContext in two resourceconstrained environments, cloud and edge. Extensive experiments show that, compared with the Huggingface framework, SpeContext achieves up to 24.89x throughput improvement in cloud and 10.06x speedup in edge with negligible accuracy loss, pushing the Pareto frontier of accuracy and throughput.", "AI": {"tldr": "\u5728\u4fe1\u606f\u8bba\u89c6\u89d2\u4e0b\uff0c\u63d0\u51fa\u5229\u7528\u84b8\u998f\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u68c0\u7d22\u7b97\u6cd5\u7684\u65b0\u8303\u5f0f\u3002", "motivation": "\u68c0\u7d22\u7b97\u6cd5\u7684\u76ee\u6807\u662f\u4e0eLLM\u5bf9\u9f50\uff0c\u7c7b\u4f3c\u4e8eLLM\u7684\u77e5\u8bc6\u84b8\u998f\u76ee\u6807\u3002", "method": "\u63d0\u51faSpeContext\uff0c\u901a\u8fc7\u4e09\u4e2a\u5c42\u6b21\u51cf\u8f7b\u68c0\u7d22\u7684\u8d1f\u62c5\uff1a1) \u5229\u7528DLM\u5934\u7ea7\u6ce8\u610f\u529b\u6743\u91cd\u7684\u8f7b\u91cf\u7ea7\u68c0\u7d22\u5934\uff1b2) \u7cfb\u7edf\u7ea7\u5f02\u6b65\u9884\u53d6\u6570\u636e\u6d41\uff1b3) \u7f16\u8bd1\u7ea7\u7684\u81ea\u9002\u5e94\u5185\u5b58\u7ba1\u7406\u3002", "result": "\u76f8\u6bd4Huggingface\u6846\u67b6\uff0cSpeContext\u5728\u4e91\u7aef\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u63d0\u5347\u4e8624.89\u500d\u541e\u5410\u91cf\uff0c\u5728\u8fb9\u7f18\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u63d0\u5347\u4e8610.06\u500d\u901f\u5ea6\uff0c\u7cbe\u5ea6\u635f\u5931\u53ef\u5ffd\u7565\u4e0d\u8ba1\u3002", "conclusion": "SpeContext\u63a8\u8fdb\u4e86\u7cbe\u5ea6\u4e0e\u541e\u5410\u91cf\u7684\u5e15\u7d2f\u6258\u8fb9\u754c\u3002"}}
{"id": "2512.01356", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.01356", "abs": "https://arxiv.org/abs/2512.01356", "authors": ["Yuxin Zhang", "Yuxia Zhang", "Zeyu Sun", "Yanjie Jiang", "Hui Liu"], "title": "LAURA: Enhancing Code Review Generation with Context-Enriched Retrieval-Augmented LLM", "comment": "Accepted by the 2025 40th IEEE/ACM International Conference on Automated Software Engineering (ASE). Copyright 2025 IEEE. This is the author's accepted manuscript. The final published version may differ and will be available from IEEE Xplore", "summary": "Code review is critical for ensuring software quality and maintainability. With the rapid growth in software scale and complexity, code review has become a bottleneck in the development process because of its time-consuming and knowledge-intensive nature and the shortage of experienced developers willing to review code. Several approaches have been proposed for automatically generating code reviews based on retrieval, neural machine translation, pre-trained models, or large language models (LLMs). These approaches mainly leverage historical code changes and review comments. However, a large amount of crucial information for code review, such as the context of code changes and prior review knowledge, has been overlooked. This paper proposes an LLM-based review knowledge-augmented, context-aware framework for code review generation, named LAURA. The framework integrates review exemplar retrieval, context augmentation, and systematic guidance to enhance the performance of ChatGPT-4o and DeepSeek v3 in generating code review comments. Besides, given the extensive low-quality reviews in existing datasets, we also constructed a high-quality dataset. Experimental results show that for both models, LAURA generates review comments that are either completely correct or at least helpful to developers in 42.2% and 40.4% of cases, respectively, significantly outperforming SOTA baselines. Furthermore, our ablation studies demonstrate that all components of LAURA contribute positively to improving comment quality.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eLLM\u7684\u4ee3\u7801\u5ba1\u67e5\u6846\u67b6LAURA\uff0c\u6574\u5408\u8303\u4f8b\u68c0\u7d22\u3001\u4e0a\u4e0b\u6587\u589e\u5f3a\u548c\u7cfb\u7edf\u5f15\u5bfc\uff0c\u663e\u8457\u63d0\u5347ChatGPT-4o\u548cDeepSeek v3\u7684\u5ba1\u67e5\u8bc4\u8bba\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u81ea\u52a8\u5316\u4ee3\u7801\u5ba1\u67e5\u65b9\u6cd5\u5ffd\u7565\u4e86\u4ee3\u7801\u53d8\u66f4\u4e0a\u4e0b\u6587\u548c\u5ba1\u67e5\u5148\u9a8c\u77e5\u8bc6\u7b49\u5173\u952e\u4fe1\u606f\uff0c\u4e14\u6570\u636e\u96c6\u8d28\u91cf\u666e\u904d\u8f83\u4f4e\uff0c\u5bfc\u81f4\u5ba1\u67e5\u6548\u7387\u548c\u8d28\u91cf\u53d7\u9650\u3002", "method": "1) \u901a\u8fc7\u8303\u4f8b\u68c0\u7d22\u5f15\u5165\u5386\u53f2\u5ba1\u67e5\u77e5\u8bc6\uff1b2) \u6784\u5efa\u4ee3\u7801\u53d8\u66f4\u4e0a\u4e0b\u6587\uff1b3) \u96c6\u6210\u7cfb\u7edf\u5f15\u5bfc\u7684LLM\u751f\u6210\u6846\u67b6\u3002\u540c\u65f6\u6784\u5efa\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u3002", "result": "LAURA\u751f\u6210\u7684\u5ba1\u67e5\u8bc4\u8bba\u4e2d42.2%\uff08ChatGPT-4o\uff09\u548c40.4%\uff08DeepSeek v3\uff09\u5b8c\u5168\u6b63\u786e\u6216\u5177\u9ad8\u5b9e\u7528\u6027\uff0c\u6027\u80fd\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002\u6d88\u878d\u5b9e\u9a8c\u9a8c\u8bc1\u5404\u7ec4\u4ef6\u6b63\u5411\u8d21\u732e\u3002", "conclusion": "LAURA\u901a\u8fc7\u589e\u5f3a\u4e0a\u4e0b\u6587\u611f\u77e5\u548c\u77e5\u8bc6\u6574\u5408\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u81ea\u52a8\u5316\u4ee3\u7801\u5ba1\u67e5\u4e2d\u7684\u5173\u952e\u4fe1\u606f\u7f3a\u5931\u95ee\u9898\uff0c\u4e3a\u5f00\u53d1\u6d41\u7a0b\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u8f85\u52a9\u5de5\u5177\u3002"}}
{"id": "2512.00833", "categories": ["cs.CR", "cs.AR"], "pdf": "https://arxiv.org/pdf/2512.00833", "abs": "https://arxiv.org/abs/2512.00833", "authors": ["Rupesh Raj Karn", "Lakshmi Likhitha Mankali", "Zeng Wang", "Saideep Sreekumar", "Prithwish Basu Roy", "Ozgur Sinanoglu", "Lilas Alrahis", "Johann Knechtel"], "title": "Logic Encryption: This Time for Real", "comment": null, "summary": "Modern circuits face various threats like reverse engineering, theft of intellectual property (IP), side-channel attacks, etc. Here, we present a novel approach for IP protection based on logic encryption (LE). Unlike established schemes for logic locking, our work obfuscates the circuit's structure and functionality by encoding and encrypting the logic itself. We devise an end-to-end method for practical LE implementation based on standard cryptographic algorithms, key-bit randomization, simple circuit design techniques, and system-level synthesis operations, all in a correct-by-construction manner. Our extensive analysis demonstrates the remarkable efficacy of our scheme, outperforming prior art against a range of oracle-less attacks covering crucial threat vectors, all with lower design overheads. We provide a full open-source release.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u903b\u8f91\u52a0\u5bc6\uff08LE\uff09\u7684\u65b0\u578bIP\u4fdd\u62a4\u65b9\u6cd5\uff0c\u901a\u8fc7\u7f16\u7801\u548c\u52a0\u5bc6\u903b\u8f91\u672c\u8eab\u6df7\u6dc6\u7535\u8def\u7ed3\u6784\u548c\u529f\u80fd\u3002\u91c7\u7528\u6807\u51c6\u5bc6\u7801\u5b66\u7b97\u6cd5\u3001\u5bc6\u94a5\u4f4d\u968f\u673a\u5316\u3001\u7b80\u5355\u7535\u8def\u8bbe\u8ba1\u6280\u672f\u548c\u7cfb\u7edf\u7ea7\u7efc\u5408\u64cd\u4f5c\uff0c\u4ee5\"\u6b63\u786e\u6784\u5efa\"\u65b9\u5f0f\u5b9e\u73b0\u7aef\u5230\u7aefLE\u65b9\u6848\u3002\u5b9e\u9a8c\u8bc1\u660e\u5176\u5728\u591a\u79cd\u65e0\u9884\u8a00\u653b\u51fb\u4e0b\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u8bbe\u8ba1\u5f00\u9500\u66f4\u4f4e\uff0c\u5e76\u63d0\u4f9b\u4e86\u5f00\u6e90\u5b9e\u73b0\u3002", "motivation": "\u73b0\u4ee3\u7535\u8def\u9762\u4e34\u9006\u5411\u5de5\u7a0b\u3001IP\u76d7\u7a83\u3001\u4fa7\u4fe1\u9053\u653b\u51fb\u7b49\u591a\u79cd\u5a01\u80c1\uff0c\u73b0\u6709\u903b\u8f91\u9501\u5b9a\u65b9\u6848\u5b58\u5728\u5b89\u5168\u6027\u6216\u6548\u7387\u7f3a\u9677\u3002", "method": "\u57fa\u4e8e\u6807\u51c6\u5bc6\u7801\u5b66\u7b97\u6cd5\u548c\u7cfb\u7edf\u7ea7\u8bbe\u8ba1\u6280\u672f\uff0c\u63d0\u51fa\u4e00\u79cd\"\u6b63\u786e\u6784\u5efa\"\u7684\u7aef\u5230\u7aef\u903b\u8f91\u52a0\u5bc6\u65b9\u6cd5\uff1a1\uff09\u5bf9\u7535\u8def\u903b\u8f91\u8fdb\u884c\u7f16\u7801\u548c\u52a0\u5bc6\u5b9e\u73b0\u7ed3\u6784/\u529f\u80fd\u53cc\u91cd\u6df7\u6dc6 2\uff09\u5f15\u5165\u5bc6\u94a5\u4f4d\u968f\u673a\u5316 3\uff09\u91c7\u7528\u4e13\u7528\u7535\u8def\u8bbe\u8ba1\u6280\u672f 4\uff09\u7cfb\u7edf\u7ea7\u7efc\u5408\u4f18\u5316\u3002", "result": "\u5728\u591a\u79cd\u65e0\u9884\u8a00\u653b\u51fb\u6a21\u578b\u4e0b\u7684\u5bf9\u6bd4\u5b9e\u9a8c\u663e\u793a\uff1a1\uff09\u8986\u76d6\u5173\u952e\u5a01\u80c1\u573a\u666f\u7684\u6709\u6548\u6027 2\uff09\u8bbe\u8ba1\u5f00\u9500\u663e\u8457\u4f4e\u4e8e\u73b0\u6709\u65b9\u6848 3\uff09\u5df2\u53d1\u5e03\u5f00\u6e90\u5b9e\u73b0\u3002\u9a8c\u8bc1\u4e86\u65b9\u6848\u7684\u9ad8\u6548\u5b89\u5168\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u903b\u8f91\u7ea7\"\u6df7\u6dc6\"\u7a81\u7834\u4e86\u4f20\u7edf\u4fdd\u62a4\u8303\u5f0f\uff0c\u5728\u4fdd\u6301\u4f4e\u5f00\u9500\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u6297\u653b\u51fb\u80fd\u529b\uff0c\u4e3aIP\u4fdd\u62a4\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002\u4f46\u4e5f\u9700\u5173\u6ce8\u5bc6\u7801\u5b66\u7b97\u6cd5\u5728\u786c\u4ef6\u5c42\u9762\u7684\u5177\u4f53\u5b9e\u73b0\u95ee\u9898\u3002"}}
{"id": "2512.00729", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.00729", "abs": "https://arxiv.org/abs/2512.00729", "authors": ["Yuxiang Chen", "Zuohan Wu", "Ziwei Wang", "Xiangning Yu", "Xujia Li", "Linyi Yang", "Mengyue Yang", "Jun Wang", "Lei Chen"], "title": "Probing the \"Psyche'' of Large Reasoning Models: Understanding Through a Human Lens", "comment": "13 pages", "summary": "Large reasoning models (LRMs) have garnered significant attention from researchers owing to their exceptional capability in addressing complex tasks. Motivated by the observed human-like behaviors in their reasoning processes, this paper introduces a comprehensive taxonomy to characterize atomic reasoning steps and probe the ``psyche'' of LRM intelligence. Specifically, it comprises five groups and seventeen categories derived from human mental processes, thereby grounding the understanding of LRMs in an interdisciplinary perspective. The taxonomy is then applied for an in-depth understanding of current LRMs, resulting in a distinct labeled dataset that comprises 277,534 atomic reasoning steps. Using this resource, we analyze contemporary LRMs and distill several actionable takeaways for improving training and post-training of reasoning models. Notably, our analysis reveals that prevailing post-answer ``double-checks'' (self-monitoring evaluations) are largely superficial and rarely yield substantive revisions. Thus, incentivizing comprehensive multi-step reflection, rather than simple self-monitoring, may offer a more effective path forward. To complement the taxonomy, an automatic annotation framework, named CAPO, is proposed to leverage large language models (LLMs) for generating the taxonomy-based annotations. Experimental results demonstrate that CAPO achieves higher consistency with human experts compared to baselines, facilitating a scalable and comprehensive analysis of LRMs from a human cognitive perspective. Together, the taxonomy, CAPO, and the derived insights provide a principled, scalable path toward understanding and advancing LRM reasoning.", "AI": {"tldr": "\u672c\u6587\u5f15\u5165\u7efc\u5408\u5206\u7c7b\u6cd5\uff0c\u4ece\u8de8\u5b66\u79d1\u7684\u89d2\u5ea6\u5206\u6790\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u7684\u63a8\u7406\u6b65\u9aa4\uff0c\u5e76\u63d0\u51fa\u6539\u8fdb\u8bad\u7ec3\u548c\u63a8\u7406\u7b56\u7565\u7684\u65b9\u6cd5\u3002", "motivation": "LRMs\u5728\u89e3\u51b3\u590d\u6742\u4efb\u52a1\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u7814\u7a76\u8005\u5e0c\u671b\u7406\u89e3\u5176\u7c7b\u4eba\u884c\u4e3a\u7684\u63a8\u7406\u8fc7\u7a0b\u3002", "method": "\u63d0\u51fa\u5305\u62ec\u4e94\u4e2a\u7c7b\u522b\u548c\u5341\u4e03\u4e2a\u5b50\u7c7b\u522b\u7684\u5206\u7c7b\u6cd5\uff0c\u5e76\u5e94\u7528\u8be5\u5206\u7c7b\u6cd5\u5206\u6790\u4e86\u5f53\u524dLRMs\uff0c\u751f\u6210\u4e86\u5305\u542b277,534\u4e2a\u539f\u5b50\u63a8\u7406\u6b65\u9aa4\u6807\u8bb0\u7684\u6570\u636e\u96c6\u3002\u8fd8\u63d0\u51fa\u4e86CAPO\u81ea\u52a8\u5316\u6807\u6ce8\u6846\u67b6\u3002", "result": "CAPO\u5728\u4eba\u7c7b\u4e13\u5bb6\u4e00\u81f4\u6027\u65b9\u9762\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u63ed\u793a\u4e86\u5f53\u524dLRMs\u4e2d\u7684\u201c\u81ea\u6211\u76d1\u6d4b\u201d\u901a\u5e38\u8f83\u4e3a\u7b80\u5355\uff0c\u9700\u8981\u591a\u6b65\u9aa4\u6df1\u5c42\u6b21\u7684\u63a8\u7406\u3002", "conclusion": "\u8be5\u5206\u7c7b\u6cd5\u3001CAPO\u548c\u76f8\u5173\u89c1\u89e3\u63d0\u4f9b\u4e86\u4e00\u6761\u7cfb\u7edf\u6027\u3001\u53ef\u6269\u5c55\u7684\u8def\u5f84\uff0c\u5e2e\u52a9\u7406\u89e3\u548c\u63a8\u8fdbLRM\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2512.01396", "categories": ["cs.SE", "cs.CL", "cs.CR"], "pdf": "https://arxiv.org/pdf/2512.01396", "abs": "https://arxiv.org/abs/2512.01396", "authors": ["Zhiqing Zhong", "Jiaming Huang", "Pinjia He"], "title": "BackportBench: A Multilingual Benchmark for Automated Backporting of Patches", "comment": "Under review", "summary": "Many modern software projects evolve rapidly to incorporate new features and security patches. It is important for users to update their dependencies to safer versions, but many still use older, vulnerable package versions because upgrading can be difficult and may break their existing codebase. Software developers can mitigate this problem by backporting security patches to older releases. However, manually backporting is time-consuming and error-prone. The effectiveness of existing automated backporting techniques on general software remains unclear since they typically target only code-hunk or function-level patch porting scenarios and are evaluated with imperfect metrics.\n  To facilitate the development and evaluation of automated backporting techniques, we introduce BackportBench, the first comprehensive benchmark suite for patch backporting problem. BackportBench is a multilingual benchmark that contains 202 patch backporting problems from PyPI, Maven, and npm, each with executable Docker environments and relevant test cases. We evaluated existing patch porting methods and LLM-based techniques that have the potential to adapt to this task using BackportBench. The results show that the agentic method has outperformed traditional patch porting methods, especially on cases that require logical and structural changes. However, the performance varies across different programming languages. Based on the findings, we draw several implications for researchers and software practitioners in future work on automated backporting.", "AI": {"tldr": "\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u4f9d\u8d56\u9879\u5347\u7ea7\u7684\u95ee\u9898\uff0c\u4ecb\u7ecd\u4e86\u5f15\u5165\u7684BackportBench\uff0c\u4e00\u4e2a\u7528\u4e8e\u81ea\u52a8\u8865\u4e01\u56de\u79fb\u690d\u6280\u672f\u7efc\u5408\u8bc4\u4ef7\u7684\u591a\u8bed\u8a00\u57fa\u51c6\u6d4b\u8bd5\u5957\u4ef6\u3002\u901a\u8fc7BackportBench\u8bc4\u4f30\u73b0\u6709\u65b9\u6cd5\uff0c\u663e\u793a\u51faagentic\u65b9\u6cd5\u5728\u903b\u8f91\u548c\u7ed3\u6784\u53d8\u66f4\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u5e76\u5206\u6790\u4e86\u4e0d\u540c\u7f16\u7a0b\u8bed\u8a00\u7684\u6267\u884c\u6548\u679c\u3002", "motivation": "\u7528\u6237\u5728\u4f7f\u7528\u73b0\u4ee3\u8f6f\u4ef6\u9879\u76ee\u65f6\uff0c\u9700\u9891\u7e41\u66f4\u65b0\u5176\u4f9d\u8d56\u9879\u5230\u5b89\u5168\u7248\u672c\uff0c\u4f46\u5347\u7ea7\u53ef\u80fd\u5bfc\u81f4\u4ee3\u7801\u5e93\u635f\u574f\u3002\u73b0\u6709\u81ea\u52a8\u56de\u79fb\u690d\u6280\u672f\u672a\u80fd\u5f88\u597d\u5730\u5e94\u5bf9\u8fd9\u4e00\u95ee\u9898\uff0c\u56e0\u4e3a\u5b83\u4eec\u901a\u5e38\u53ea\u5728\u4ee3\u7801\u5757\u6216\u51fd\u6570\u7ea7\u522b\u8fdb\u884c\u64cd\u4f5c\uff0c\u4e14\u8bc4\u4ef7\u6307\u6807\u4e0d\u5168\u9762\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u5168\u9762\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002", "method": "\u5f15\u5165\u4e86BackportBench\uff0c\u4e00\u5957\u7528\u4e8e\u8865\u4e01\u56de\u79fb\u690d\u95ee\u9898\u7684\u591a\u8bed\u8a00\u57fa\u51c6\u6d4b\u8bd5\u5957\u4ef6\u3002\u5b83\u5305\u62ec202\u4e2a\u8865\u4e01\u56de\u79fb\u690d\u95ee\u9898\uff0c\u6db5\u76d6PyPI\u3001Maven\u548cnpm\uff0c\u6bcf\u4e2a\u95ee\u9898\u5305\u542b\u53ef\u6267\u884c\u7684Docker\u73af\u5883\u548c\u76f8\u5173\u6d4b\u8bd5\u6848\u4f8b\u3002\u8bc4\u4f30\u4e86\u73b0\u6709\u8865\u4e01\u79fb\u690d\u65b9\u6cd5\u4ee5\u53caLLM\u57fa\u7840\u7684\u6280\u672f\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cagentic\u65b9\u6cd5\u5728\u9700\u8981\u903b\u8f91\u548c\u7ed3\u6784\u53d8\u5316\u7684\u6848\u4f8b\u4e0a\u4f18\u4e8e\u4f20\u7edf\u8865\u4e01\u79fb\u690d\u65b9\u6cd5\uff0c\u4f46\u4e0d\u540c\u7f16\u7a0b\u8bed\u8a00\u7684\u6548\u679c\u6709\u6240\u5dee\u5f02\u3002\u4e3a\u7814\u7a76\u8005\u548c\u8f6f\u4ef6\u5f00\u53d1\u8005\u5728\u81ea\u52a8\u56de\u79fb\u690d\u6280\u672f\u65b9\u9762\u63d0\u4f9b\u4e86\u82e5\u5e72\u542f\u793a\u3002", "conclusion": "BackportBench\u662f\u4e00\u4e2a\u7528\u4e8e\u81ea\u52a8\u8865\u4e01\u56de\u79fb\u690d\u6280\u672f\u7684\u5168\u9762\u57fa\u51c6\u6d4b\u8bd5\u5957\u4ef6\uff0c\u901a\u8fc7\u5b9e\u9a8c\u53d1\u73b0agentic\u65b9\u6cd5\u66f4\u52a0\u6709\u6548\u3002\u672a\u6765\u7684\u7814\u7a76\u9700\u5173\u6ce8\u4e0d\u540c\u8bed\u8a00\u7684\u8865\u4e01\u79fb\u690d\u6548\u679c\u6539\u8fdb\uff0c\u5e76\u4f18\u5316\u76f8\u5173\u65b9\u6cd5\u3002"}}
{"id": "2512.00857", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2512.00857", "abs": "https://arxiv.org/abs/2512.00857", "authors": ["Gonzalo Roa", "Manuel Suarez-Roman", "Juan Tapiador"], "title": "Hesperus is Phosphorus: Mapping Threat Actor Naming Taxonomies at Scale", "comment": null, "summary": "This paper studies the problem of Threat Actor (TA) naming convention inconsistency across leading Cyber Threat Intelligence (CTI) vendors. The current decentralized and proprietary nomenclature creates confusion and significant obstacles for researchers, including difficulties in integrating and correlating disparate CTI reports and TA profiles. This paper introduces HiP (Hesperus is Phosphorus, a reference to the classic question about the Morning and the Evening Star), a methodology for normalizing, integrating, and clustering TA names presumably corresponding to the same entity. Using HiP, we analyze a large dataset collected from 15 sources and spanning 13,371 CTI reports, 17 vendor taxonomies, 3,287 TA names, and 8 mappings between them. Our analysis of the resulting name graph provides insights on key features of the problem, such as the concentration of aliases on a relatively small subset of TAs, the evolution of this phenomenon over the years, and the factors that could explain TA name proliferation. We also report errors in the mappings and methodological pitfalls that contribute to make certain TA name clusters larger than they should be, including the use of temporary names for activity clusters, the existence of common tools and infrastructure, and overlapping operations. We conclude with a discussion on the inherent difficulties to adopt a TA naming standard, a quest fundamentally hampered by the need to share highly-sensitive telemetry that is private to each CTI vendor.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faHiP\u65b9\u6cd5\u4ee5\u89e3\u51b3CTI\u4f9b\u5e94\u5546\u95f4TA\u547d\u540d\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u901a\u8fc7\u6807\u51c6\u5316\u3001\u6574\u5408\u548c\u805a\u7c7bTA\u540d\u79f0\u63d0\u5347\u60c5\u62a5\u6574\u5408\u6548\u7387\u3002", "motivation": "\u5f53\u524d\u5206\u6563\u7684\u4e13\u6709TA\u547d\u540d\u5bfc\u81f4\u7814\u7a76\u4eba\u5458\u5728\u6574\u5408\u548c\u5173\u8054\u4e0d\u540cCTI\u62a5\u544a\u53caTA\u6863\u6848\u65f6\u9762\u4e34\u663e\u8457\u969c\u788d\u3002", "method": "HiP\u65b9\u6cd5\u5bf9\u6765\u81ea15\u4e2a\u6570\u636e\u6e90\u7684TA\u540d\u79f0\u8fdb\u884c\u6807\u51c6\u5316\u3001\u6574\u5408\u548c\u805a\u7c7b\uff0c\u6784\u5efa\u540d\u79f0\u56fe\u8c31\u5e76\u5206\u6790\u6570\u636e\u3002", "result": "\u5206\u6790\u4e8613,371\u4efd\u62a5\u544a\u53ca3,287\u4e2aTA\u540d\u79f0\uff0c\u53d1\u73b0\u522b\u540d\u96c6\u4e2d\u5728\u5c11\u6570TA\u3001\u540d\u79f0\u968f\u5e74\u4efd\u6f14\u53d8\uff0c\u63ed\u793a\u4e86\u547d\u540d\u6269\u6563\u7684\u5f71\u54cd\u56e0\u7d20\u3002\u62a5\u544a\u8fd8\u6307\u51fa\u4e86\u6620\u5c04\u9519\u8bef\u548c\u65b9\u6cd5\u9677\u9631\uff0c\u8ba8\u8bba\u4e86\u91c7\u7528TA\u547d\u540d\u6807\u51c6\u7684\u56f0\u96be\u3002", "conclusion": "TA\u547d\u540d\u6807\u51c6\u96be\u4ee5\u63a8\u5e7f\uff0c\u4e3b\u8981\u56e0\u4e3a\u5404CTI\u4f9b\u5e94\u5546\u7684\u79c1\u5bc6\u654f\u611f\u9065\u6d4b\u6570\u636e\u96be\u4ee5\u5171\u4eab\u3002"}}
{"id": "2512.00756", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00756", "abs": "https://arxiv.org/abs/2512.00756", "authors": ["Ruihan Chen", "Qiming Li", "Xiaocheng Feng", "Xiaoliang Yang", "Weihong Zhong", "Yuxuan Gu", "Zekun Zhou", "Bing Qin"], "title": "MPR-GUI: Benchmarking and Enhancing Multilingual Perception and Reasoning in GUI Agents", "comment": "27pages, 12figures", "summary": "With the advancement of computational resources, Large Vision-Language Models (LVLMs) exhibit impressive Perception and Reasoning (P&R) performance on Graphical User Interface (GUI) tasks. However, although they demonstrate strong P&R capabilities in English GUI scenarios, their performance in multilingual settings has received little attention, which limits their global applications. Moreover, existing studies on GUI tasks lack fine-grained analyses, including widget functions and elements' spatial relationships, which are fundamental for more targeted improvements. To tackle these issues, we propose MPR-GUI-Bench, a Multilingual fine-grained Perception and Reasoning GUI Benchmark to evaluate GUI agents' P&R capabilities. Evaluation results demonstrate that LVLMs exhibit significantly worse P&R performance in non-English languages than in English. To address these gaps, we propose GUI-XLI, a GUI Cross-Lingual Intervention method that applies interventions to the hidden states at P&R capability-related layers to mitigate the gaps between English and other languages, building on previous research showing that the hidden states of different language inputs exhibit significant differences in the latent space. Experimental results indicate that our method improves GUI agents' multilingual P&R capability by 6.5% on average.", "AI": {"tldr": "\u63d0\u51fa\u4e86MPR-GUI-Bench\uff0c\u4e00\u4e2a\u591a\u8bed\u8a00\u7ec6\u7c92\u5ea6GUI\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u8bbe\u8ba1\u4e86GUI-XLI\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u611f\u77e5\u548c\u63a8\u7406\u76f8\u5173\u7684\u5c42\u9762\u5bf9\u9690\u85cf\u72b6\u6001\u8fdb\u884c\u5e72\u9884\uff0c\u7f29\u5c0f\u82f1\u8bed\u548c\u5176\u4ed6\u8bed\u8a00\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "motivation": "\u5f53\u524dLVLMs\u5728\u591a\u8bed\u8a00GUI\u73af\u5883\u4e0b\u7684\u8868\u73b0\u7f3a\u4e4f\u5173\u6ce8\uff0c\u4e14\u5bf9GUI\u4efb\u52a1\u7684\u7ec6\u7c92\u5ea6\u5206\u6790\u4e0d\u8db3\uff0c\u5f71\u54cd\u4e86\u6a21\u578b\u7684\u5168\u7403\u5e94\u7528\u3002", "method": "\u63d0\u51faMPR-GUI-Bench\u8fdb\u884c\u591a\u8bed\u8a00\u7ec6\u7c92\u5ea6\u8bc4\u4f30\u3002\u4e3a\u89e3\u51b3\u6027\u80fd\u5dee\u8ddd\uff0c\u63d0\u51faGUI-XLI\uff0c\u4e00\u79cd\u901a\u8fc7\u5728\u611f\u77e5\u548c\u63a8\u7406\u76f8\u5173\u5c42\u9762\u5bf9LVLMs\u9690\u85cf\u72b6\u6001\u8fdb\u884c\u5e72\u9884\u7684\u65b9\u6cd5\uff0c\u7f29\u5c0f\u82f1\u8bed\u548c\u5176\u4ed6\u8bed\u8a00\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u7684\u5dee\u5f02\u3002", "result": "GUI-XLI\u65b9\u6cd5\u5e73\u5747\u63d0\u5347\u4e866.5%\u7684\u591a\u8bed\u8a00\u611f\u77e5\u548c\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "GUI-XLI\u65b9\u6cd5\u6709\u6548\u63d0\u9ad8\u4e86LVLMs\u5728\u591a\u8bed\u8a00GUI\u73af\u5883\u4e2d\u7684\u6027\u80fd\uff0c\u6539\u5584\u4e86\u5168\u7403\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2512.01523", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.01523", "abs": "https://arxiv.org/abs/2512.01523", "authors": ["Pankaj Jalore", "Y. Raghu Reddy", "Vasudeva Varma"], "title": "Teaching an Online Multi-Institutional Research Level Software Engineering Course with Industry - an Experience Report", "comment": "7 pages", "summary": "Covid has made online teaching and learning acceptable and students, faculty, and industry professionals are all comfortable with this mode. This comfort can be leveraged to offer an online multi-institutional research-level course in an area where individual institutions may not have the requisite faculty to teach and/or research students to enroll. If the subject is of interest to industry, online offering also allows industry experts to contribute and participate with ease. Advanced topics in Software Engineering are ideally suited for experimenting with this approach as industry, which is often looking to incorporate advances in software engineering in their practices, is likely to agree to contribute and participate. In this paper we describe an experiment in teaching a course titled \"AI in Software Engineering\" jointly between two institutions with active industry participation, and share our and student's experience. We believe this collaborative teaching approach can be used for offering research level courses in any applied area of computer science by institutions who are small and find it difficult to offer research level courses on their own.", "AI": {"tldr": "Covid\u52a0\u901f\u4e86\u7ebf\u4e0a\u6559\u5b66\u7684\u666e\u53ca\uff0c\u4f7f\u5f97\u591a\u673a\u6784\u8054\u5408\u5f00\u8bbe\u7814\u7a76\u7ea7\u8bfe\u7a0b\u6210\u4e3a\u53ef\u80fd\u3002\u672c\u6587\u5206\u4eab\u4e86\"AI\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u7684\u5e94\u7528\"\u8054\u5408\u6559\u5b66\u5b9e\u9a8c\u53ca\u7ecf\u9a8c\u3002", "motivation": "\u4e3a\u5f25\u8865\u5c0f\u578b\u673a\u6784\u5e08\u8d44\u4e0d\u8db3\uff0c\u5229\u7528\u7ebf\u4e0a\u5f62\u5f0f\u8054\u5408\u591a\u5bb6\u673a\u6784\u5f00\u8bbe\u7814\u7a76\u7ea7\u8bfe\u7a0b\uff0c\u5e76\u5f15\u5165\u4ea7\u4e1a\u754c\u7684\u53c2\u4e0e\u3002", "method": "\u4e24\u6240\u673a\u6784\u901a\u8fc7\u7ebf\u4e0a\u8054\u5408\u5f00\u8bbe\u201cAI in\u8f6f\u4ef6\u5de5\u7a0b\u201d\u8bfe\u7a0b\uff0c\u5e76\u5438\u7eb3\u884c\u4e1a\u4e13\u5bb6\u53c2\u4e0e\u6559\u5b66\uff0c\u5b9e\u9a8c\u6b64\u5408\u4f5c\u6559\u5b66\u6a21\u5f0f\u7684\u6548\u679c\u3002", "result": "\u8054\u5408\u6559\u5b66\u548c\u884c\u4e1a\u53c2\u4e0e\u7684\u6a21\u5f0f\u83b7\u5f97\u5b66\u751f\u548c\u6559\u804c\u5458\u5de5\u7684\u5e7f\u6cdb\u8ba4\u53ef\uff0c\u88ab\u8ba4\u4e3a\u5177\u6709\u63a8\u5e7f\u5230\u5176\u4ed6\u8ba1\u7b97\u673a\u5b66\u79d1\u9886\u57df\u7684\u6f5c\u529b\u3002", "conclusion": "\u8fd9\u79cd\u5408\u4f5c\u6559\u5b66\u6a21\u5f0f\u4e3a\u5c0f\u578b\u673a\u6784\u5f00\u8bbe\u7814\u7a76\u7ea7\u8bfe\u7a0b\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2512.00807", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00807", "abs": "https://arxiv.org/abs/2512.00807", "authors": ["Yujie Lin", "Jiayao Ma", "Qingguo Hu", "Derek F. Wong", "Jinsong Su"], "title": "BioPro: On Difference-Aware Gender Fairness for Vision-Language Models", "comment": null, "summary": "Vision-Language Models (VLMs) inherit significant social biases from their training data, notably in gender representation. Current fairness interventions often adopt a difference-unaware perspective that enforces uniform treatment across demographic groups. These approaches, however, fail to distinguish between contexts where neutrality is required and those where group-specific attributes are legitimate and must be preserved. Building upon recent advances in difference-aware fairness for text-only models, we extend this concept to the multimodal domain and formalize the problem of difference-aware gender fairness for image captioning and text-to-image generation. We advocate for selective debiasing, which aims to mitigate unwanted bias in neutral contexts while preserving valid distinctions in explicit ones. To achieve this, we propose BioPro (Bias Orthogonal Projection), an entirely training-free framework. BioPro identifies a low-dimensional gender-variation subspace through counterfactual embeddings and applies projection to selectively neutralize gender-related information. Experiments show that BioPro effectively reduces gender bias in neutral cases while maintaining gender faithfulness in explicit ones, thus providing a promising direction toward achieving selective fairness in VLMs. Beyond gender bias, we further demonstrate that BioPro can effectively generalize to continuous bias variables, such as scene brightness, highlighting its broader applicability.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86BioPro\u6846\u67b6\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u53bb\u504f\u6280\u672f\u89e3\u51b3\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b(VLMs)\u4e2d\u7684\u6027\u522b\u504f\u89c1\u95ee\u9898\uff0c\u5e76\u5728\u4e2d\u7acb\u573a\u666f\u4fdd\u6301\u6027\u522b\u76f8\u5173\u6027\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u516c\u5e73\u6027\u5e72\u9884\u65b9\u6cd5\uff08\u5dee\u5f02\u5ffd\u7565\uff09\u7684\u5c40\u9650\u6027\uff1a\u65e0\u6cd5\u533a\u5206\u9700\u8981\u4e2d\u7acb\u6027\u7684\u573a\u666f\u548c\u9700\u4fdd\u7559\u7fa4\u4f53\u5dee\u5f02\u6027\u7684\u573a\u666f\u3002", "method": "\u63d0\u51faBioPro\u2014\u2014\u4e00\u79cd\u8bad\u7ec3\u65e0\u5173\u7684\u5dee\u5f02\u611f\u77e5\u53bb\u504f\u6846\u67b6\uff0c\u901a\u8fc7\u53cd\u4e8b\u5b9e\u5d4c\u5165\u8bc6\u522b\u6027\u522b\u53d8\u5316\u5b50\u7a7a\u95f4\u5e76\u9009\u62e9\u6027\u53bb\u9664\u6027\u522b\u5173\u8054\u4fe1\u606f\u3002", "result": "\u5b9e\u9a8c\u663e\u793aBioPro\u5728\u4e2d\u7acb\u573a\u666f\u964d\u4f4e\u6027\u522b\u504f\u89c1\u7684\u540c\u65f6\uff0c\u80fd\u5728\u660e\u786e\u573a\u666f\u4fdd\u6301\u6027\u522b\u53ef\u4fe1\u5ea6\uff1b\u4e14\u80fd\u6cdb\u5316\u5230\u573a\u666f\u4eae\u5ea6\u7b49\u8fde\u7eed\u504f\u7f6e\u53d8\u91cf\u3002", "conclusion": "BioPro\u4e3aVLMs\u4e2d\u7684\u9009\u62e9\u6027\u516c\u5e73\u6027\u63d0\u4f9b\u65b0\u65b9\u5411\uff0c\u5e76\u5177\u6709\u66f4\u5e7f\u7684\u9002\u7528\u6027\u3002"}}
{"id": "2512.01570", "categories": ["cs.SE", "cs.DL"], "pdf": "https://arxiv.org/pdf/2512.01570", "abs": "https://arxiv.org/abs/2512.01570", "authors": ["Stephan Druskat", "Lars Grunske"], "title": "OpenDORS: A dataset of openly referenced open research software", "comment": "5 pages, 3 figures, 1 table", "summary": "In many academic disciplines, software is created during the research process or for a research purpose. The crucial role of software for research is increasingly acknowledged. The application of software engineering to research software has been formalized as research software engineering, to create better software that enables better research. Despite this, large-scale studies of research software and its development are still lacking. To enable such studies, we present a dataset of 134,352 unique open research software projects and 134,154 source code repositories referenced in open access literature. Each dataset record identifies the referencing publication and lists source code repositories of the software project. For 122,425 source code repositories, the dataset provides metadata on latest versions, license information, programming languages and descriptive metadata files. We summarize the distributions of these features in the dataset and describe additional software metadata that extends the dataset in future work. Finally, we suggest examples of research that could use the dataset to develop a better understanding of research software practice in RSE research.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2512.01115", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2512.01115", "abs": "https://arxiv.org/abs/2512.01115", "authors": ["Tao Zhang", "Yevgeniy Vorobeychik"], "title": "Sliced R\u00e9nyi Pufferfish Privacy: Directional Additive Noise Mechanism and Private Learning with Gradient Clipping", "comment": null, "summary": "We study privatization mechanism design and privacy accounting in the Pufferfish family, addressing two practical gaps of Renyi Pufferfish Privacy (RPP): high-dimensional optimal transport (OT) calibration and the absence of a general, mechanism-agnostic composition rule for iterative learning. We introduce Sliced Renyi Pufferfish Privacy (SRPP), which replaces high-dimensional comparisons by directional ones over a set of unit vectors, enabling geometry-aware and tractable guarantees. To calibrate noise without high-dimensional OT, we propose sliced Wasserstein mechanisms that compute per-direction (1-D) sensitivities, yielding closed-form, statistically stable, and anisotropic calibrations. We further define SRPP Envelope (SRPE) as computable upper bounds that are tightly implementable by these sliced Wasserstein mechanisms. For iterative deep learning algorithms, we develop a decompose-then-compose SRPP-SGD scheme with gradient clipping based on a History-Uniform Cap (HUC), a pathwise bound on one-step directional changes that is uniform over optimization history, and a mean-square variant (ms-HUC) that leverages subsampling randomness to obtain on-average SRPP guarantees with improved utility. The resulting HUC and ms-HUC accountants aggregate per-iteration, per-direction Renyi costs and integrate naturally with moments-accountant style analyses. Finally, when multiple mechanisms are trained and privatized independently under a common slicing geometry, our analysis yields graceful additive composition in both worst-case and mean-square regimes. Our experiments indicate that the proposed SRPP-based methods achieve favorable privacy-utility trade-offs in both static and iterative settings.", "AI": {"tldr": "\u4e3a\u89e3\u51b3Renyi Pufferfish Privacy (RPP)\u5728\u9ad8\u7ef4\u4f20\u8f93\u6821\u51c6\u548c\u8fed\u4ee3\u5b66\u4e60\u4e2d\u7684\u7ec4\u5408\u89c4\u5219\u7f3a\u5931\u95ee\u9898\uff0c\u63d0\u51faSliced Renyi Pufferfish Privacy (SRPP)\u53ca\u5176\u76f8\u5173\u673a\u5236\u3002", "motivation": "\u586b\u8865RPP\u7684\u4e24\u4e2a\u5b9e\u9645\u7a7a\u767d\uff1a\u9ad8\u7ef4\u6700\u4f18\u4f20\u8f93\u6821\u51c6\u53ca\u7f3a\u4e4f\u901a\u7528\u7684\u3001\u673a\u5236\u65e0\u5173\u7684\u8fed\u4ee3\u5b66\u4e60\u7ec4\u5408\u89c4\u5219\u3002", "method": "\u5f15\u5165SRPP\uff0c\u901a\u8fc7\u5355\u4f4d\u5411\u91cf\u5b9a\u5411\u6bd4\u8f83\u66ff\u4ee3\u9ad8\u7ef4\u6bd4\u8f83\uff1b\u63d0\u51fa\u5206\u7247Wasserstein\u673a\u5236\u8ba1\u7b97\u5355\u7ef4\u654f\u611f\u6027\u4ee5\u5b9e\u73b0\u566a\u58f0\u6821\u51c6\uff1b\u5b9a\u4e49SRPE\u4f5c\u4e3a\u53ef\u8ba1\u7b97\u4e0a\u754c\uff1b\u8bbe\u8ba1SRPP-SGD\u65b9\u6848\u4e0eHUC/ms-HUC\u4f1a\u8ba1\u7b56\u7565\u3002", "result": "SRPP\u65b9\u6cd5\u5728\u9759\u6001\u548c\u8fed\u4ee3\u73af\u5883\u4e2d\u5747\u5b9e\u73b0\u4e86\u6709\u5229\u7684\u9690\u79c1-\u6548\u7528\u6743\u8861\uff0c\u4e14\u591a\u673a\u5236\u72ec\u7acb\u8bad\u7ec3\u65f6\u53ef\u8fdb\u884c\u6e29\u548c\u7684\u52a0\u6027\u7ec4\u5408\u3002", "conclusion": "SRPP\u89e3\u51b3\u4e86RPP\u7684\u5c40\u9650\uff0c\u63d0\u4f9b\u4e86\u8ba1\u7b97\u9ad8\u6548\u3001\u51e0\u4f55\u611f\u77e5\u4e14\u5b9e\u7528\u7684\u9690\u79c1\u4fdd\u8bc1\u7b56\u7565\u3002"}}
{"id": "2512.00818", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.00818", "abs": "https://arxiv.org/abs/2512.00818", "authors": ["Haozhen Gong", "Xiaozhong Ji", "Yuansen Liu", "Wenbin Wu", "Xiaoxiao Yan", "Jingjing Liu", "Kai Wu", "Jiazhen Pan", "Bailiang Jian", "Jiangning Zhang", "Xiaobin Hu", "Hongwei Bran Li"], "title": "Med-CMR: A Fine-Grained Benchmark Integrating Visual Evidence and Clinical Logic for Medical Complex Multimodal Reasoning", "comment": null, "summary": "MLLMs MLLMs are beginning to appear in clinical workflows, but their ability to perform complex medical reasoning remains unclear. We present Med-CMR, a fine-grained Medical Complex Multimodal Reasoning benchmark. Med-CMR distinguishes from existing counterparts by three core features: 1) Systematic capability decomposition, splitting medical multimodal reasoning into fine-grained visual understanding and multi-step reasoning to enable targeted evaluation; 2) Challenging task design, with visual understanding across three key dimensions (small-object detection, fine-detail discrimination, spatial understanding) and reasoning covering four clinically relevant scenarios (temporal prediction, causal reasoning, long-tail generalization, multi-source integration); 3) Broad, high-quality data coverage, comprising 20,653 Visual Question Answering (VQA) pairs spanning 11 organ systems and 12 imaging modalities, validated via a rigorous two-stage (human expert + model-assisted) review to ensure clinical authenticity. We evaluate 18 state-of-the-art MLLMs with Med-CMR, revealing GPT-5 as the top-performing commercial model: 57.81 accuracy on multiple-choice questions (MCQs) and a 48.70 open-ended score, outperforming Gemini 2.5 Pro (49.87 MCQ accuracy, 45.98 open-ended score) and leading open-source model Qwen3-VL-235B-A22B (49.34 MCQ accuracy, 42.62 open-ended score). However, specialized medical MLLMs do not reliably outperform strong general models, and long-tail generalization emerges as the dominant failure mode. Med-CMR thus provides a stress test for visual-reasoning integration and rare-case robustness in medical MLLMs, and a rigorous yardstick for future clinical systems.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2512.01609", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.01609", "abs": "https://arxiv.org/abs/2512.01609", "authors": ["Patrick Herter", "Vincent Ahlrichs", "Ridvan A\u00e7ilan", "Julian Horsch"], "title": "GPTrace: Effective Crash Deduplication Using LLM Embeddings", "comment": "Original publication in 2026 IEEE/ACM 48th International Conference on Software Engineering (ICSE '26), April 12-18, 2026, Rio de Janeiro, Brazil. ACM, New York, NY, USA, 12 pages", "summary": "Fuzzing is a highly effective method for uncovering software vulnerabilities, but analyzing the resulting data typically requires substantial manual effort. This is amplified by the fact that fuzzing campaigns often find a large number of crashing inputs, many of which share the same underlying bug. Crash deduplication is the task of finding such duplicate crashing inputs and thereby reducing the data that needs to be examined. Many existing deduplication approaches rely on comparing stack traces or other information that is collected when a program crashes. Although various metrics for measuring the similarity of such pieces of information have been proposed, many do not yield satisfactory deduplication results. In this work, we present GPTrace, a deduplication workflow that leverages a large language model to evaluate the similarity of various data sources associated with crashes by computing embedding vectors and supplying those as input to a clustering algorithm. We evaluate our approach on over 300 000 crashing inputs belonging to 50 ground truth labels from 14 different targets. The deduplication results produced by GPTrace show a noticeable improvement over hand-crafted stack trace comparison methods and even more complex state-of-the-art approaches that are less flexible.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2512.01164", "categories": ["cs.CR", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.01164", "abs": "https://arxiv.org/abs/2512.01164", "authors": ["Yasaswini Konapalli", "Lotfi Ben Othmane", "Cihan Tunc", "Feras Benchellal", "Likhita Mudagere"], "title": "Reverse Engineering and Control-Aware Security Analysis of the ArduPilot UAV Framework", "comment": null, "summary": "Unmanned Aerial Vehicle (UAV) technologies are gaining high interest for many domains, which makes UAV security of utmost importance. ArduPilot is among the most widely used open-source autopilot UAV frameworks; yet, many studies demonstrate the vulnerabilities affecting such systems. Vulnerabilities within its communication subsystems (including WiFi, telemetry, or GPS) expose critical entry points, and vulnerabilities in Ardupilot can affect the control procedure. In this paper, we reconstruct the software architecture and the control models implemented by ArduPilot and then examine how these control models could potentially misused to induce malicious behaviors while relying on legitimate inputs.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2512.00834", "categories": ["cs.AI", "cs.NI"], "pdf": "https://arxiv.org/pdf/2512.00834", "abs": "https://arxiv.org/abs/2512.00834", "authors": ["Lin Zhu", "Kezhi Wang", "Luping Xiang", "Kun Yang"], "title": "SemAgent: Semantic-Driven Agentic AI Empowered Trajectory Prediction in Vehicular Networks", "comment": "Submitted for possible journal publication", "summary": "Efficient information exchange and reliable contextual reasoning are essential for vehicle-to-everything (V2X) networks. Conventional communication schemes often incur significant transmission overhead and latency, while existing trajectory prediction models generally lack environmental perception and logical inference capabilities. This paper presents a trajectory prediction framework that integrates semantic communication with Agentic AI to enhance predictive performance in vehicular environments. In vehicle-to-infrastructure (V2I) communication, a feature-extraction agent at the Roadside Unit (RSU) derives compact representations from historical vehicle trajectories, followed by semantic reasoning performed by a semantic-analysis agent. The RSU then transmits both feature representations and semantic insights to the target vehicle via semantic communication, enabling the vehicle to predict future trajectories by combining received semantics with its own historical data. In vehicle-to-vehicle (V2V) communication, each vehicle performs local feature extraction and semantic analysis while receiving predicted trajectories from neighboring vehicles, and jointly utilizes this information for its own trajectory prediction. Extensive experiments across diverse communication conditions demonstrate that the proposed method significantly outperforms baseline schemes, achieving up to a 47.5% improvement in prediction accuracy under low signal-to-noise ratio (SNR) conditions.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2512.01617", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.01617", "abs": "https://arxiv.org/abs/2512.01617", "authors": ["Pierciro Caliandro", "Matteo Ciccaglione", "Alessandro Pellegrini"], "title": "When High-Performance Computing Meets Software Testing: Distributed Fuzzing using MPI", "comment": null, "summary": "This paper explores the integration of MPI-based synchronization techniques into distributed fuzzing frameworks, highlighting possible substantial performance improvements compared to traditional filesystem-based synchronization methods. By employing lightweight MPI primitives, reductions in communication latency are achieved, facilitating more efficient data exchanges across distributed fuzzing nodes. Experimental results obtained over standard benchmarks demonstrate enhanced coverage progression from the early stages of the fuzzing process, which could be beneficial if fuzzing is employed in CI/CD pipelines at any stage of software development. Furthermore, the coordinated exchange of input corpora among clusters of fuzzers effectively addresses coverage stagnation, enabling a sustained exploration of complex and deep execution paths. Overall, the adoption of MPI-based synchronization approaches shows promising potential for significantly enhancing the scalability and efficacy of distributed fuzz testing.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u5728\u5206\u5e03\u5f0f\u6a21\u7cca\u6d4b\u8bd5\u6846\u67b6\u4e2d\u96c6\u6210\u57fa\u4e8eMPI\u7684\u540c\u6b65\u6280\u672f\uff0c\u76f8\u6bd4\u4f20\u7edf\u7684\u57fa\u4e8e\u6587\u4ef6\u7cfb\u7edf\u7684\u540c\u6b65\u65b9\u6cd5\uff0c\u6027\u80fd\u6709\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u4f20\u7edf\u57fa\u4e8e\u6587\u4ef6\u7cfb\u7edf\u7684\u5206\u5e03\u5f0f\u6a21\u7cca\u6d4b\u8bd5\u540c\u6b65\u65b9\u6cd5\u5b58\u5728\u901a\u4fe1\u5ef6\u8fdf\u8f83\u9ad8\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86\u6d4b\u8bd5\u6548\u7387\u3002", "method": "\u901a\u8fc7\u91c7\u7528\u8f7b\u91cf\u7ea7\u7684MPI\u539f\u8bed\uff0c\u964d\u4f4e\u901a\u4fe1\u5ef6\u8fdf\uff0c\u63d0\u9ad8\u6570\u636e\u4ea4\u6362\u6548\u7387\uff0c\u5e76\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u65e9\u671f\u8986\u76d6\u8fdb\u5ea6\u63d0\u5347\u660e\u663e\uff0c\u4e14MPI\u65b9\u6cd5\u80fd\u6709\u6548\u89e3\u51b3\u8986\u76d6\u505c\u6eef\u95ee\u9898\uff0c\u63d0\u9ad8\u5bf9\u590d\u6742\u6267\u884c\u8def\u5f84\u7684\u63a2\u7d22\u80fd\u529b\u3002", "conclusion": "\u91c7\u7528\u57fa\u4e8eMPI\u7684\u540c\u6b65\u65b9\u6cd5\u5728\u63d0\u9ad8\u5206\u5e03\u5f0f\u6a21\u7cca\u6d4b\u8bd5\u7684\u53ef\u6269\u5c55\u6027\u548c\u6709\u6548\u6027\u65b9\u9762\u5177\u6709\u5f88\u5927\u6f5c\u529b\u3002"}}
{"id": "2512.01185", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2512.01185", "abs": "https://arxiv.org/abs/2512.01185", "authors": ["Zihao Wang", "Kar Wai Fok", "Vrizlynn L. L. Thing"], "title": "DefenSee: Dissecting Threat from Sight and Text - A Multi-View Defensive Pipeline for Multi-modal Jailbreaks", "comment": null, "summary": "Multi-modal large language models (MLLMs), capable of processing text, images, and audio, have been widely adopted in various AI applications. However, recent MLLMs integrating images and text remain highly vulnerable to coordinated jailbreaks. Existing defenses primarily focus on the text, lacking robust multi-modal protection. As a result, studies indicate that MLLMs are more susceptible to malicious or unsafe instructions, unlike their text-only counterparts. In this paper, we proposed DefenSee, a robust and lightweight multi-modal black-box defense technique that leverages image variants transcription and cross-modal consistency checks, mimicking human judgment. Experiments on popular multi-modal jailbreak and benign datasets show that DefenSee consistently enhances MLLM robustness while better preserving performance on benign tasks compared to SOTA defenses. It reduces the ASR of jailbreak attacks to below 1.70% on MiniGPT4 using the MM-SafetyBench benchmark, significantly outperforming prior methods under the same conditions.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2512.00836", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00836", "abs": "https://arxiv.org/abs/2512.00836", "authors": ["Emily Howerton", "Justin Lessler"], "title": "Assessing model error in counterfactual worlds", "comment": null, "summary": "Counterfactual scenario modeling exercises that ask \"what would happen if?\" are one of the most common ways we plan for the future. Despite their ubiquity in planning and decision making, scenario projections are rarely evaluated retrospectively. Differences between projections and observations come from two sources: scenario deviation and model miscalibration. We argue the latter is most important for assessing the value of models in decision making, but requires estimating model error in counterfactual worlds. Here we present and contrast three approaches for estimating this error, and demonstrate the benefits and limitations of each in a simulation experiment. We provide recommendations for the estimation of counterfactual error and discuss the components of scenario design that are required to make scenario projections evaluable.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2512.01630", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.01630", "abs": "https://arxiv.org/abs/2512.01630", "authors": ["Ziheng Liu", "Runzhi He", "Minghui Zhou"], "title": "Package Dashboard: A Cross-Ecosystem Framework for Dual-Perspective Analysis of Software Packages", "comment": null, "summary": "Software supply chain attacks have revealed blind spots in existing SCA tools, which are often limited to a single ecosystem and assess either software artifacts or community activity in isolation. This fragmentation across tools and ecosystems forces developers to manually reconcile scattered data, undermining risk assessments. We present Package Dashboard, a cross-ecosystem framework that provides a unified platform for supply chain analysis, enabling a holistic, dual-perspective risk assessment by integrating package metadata, vulnerability information, and upstream community health metrics. By combining dependency resolution with repository analysis, it reduces cognitive load and improves traceability. Demonstrating the framework's versatility, a large-scale study of 374,000 packages across five Linux distributions shows its ability to uncover not only conventional vulnerabilities and license conflicts but also overlooked risks such as archived or inaccessible repositories. Ultimately, Package Dashboard provides a unified view of risk, equipping developers and DevSecOps engineers with actionable insights to strengthen the transparency, trustworthiness, and traceability of open-source ecosystems. Package Dashboard is publicly available at https://github.com/n19htfall/PackageDashboard, and a demonstration video can be found at https://youtu.be/y9ncftP8KPQ. Besides, the online version is available at https://pkgdash.osslab-pku.org.", "AI": {"tldr": "Package Dashboard \u662f\u4e00\u4e2a\u8de8\u751f\u6001\u7cfb\u7edf\u7684\u8f6f\u4ef6\u4f9b\u5e94\u94fe\u5206\u6790\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u5305\u5143\u6570\u636e\u3001\u6f0f\u6d1e\u4fe1\u606f\u548c\u4e0a\u6e38\u793e\u533a\u5065\u5eb7\u6307\u6807\uff0c\u63d0\u4f9b\u4e00\u4e2a\u7edf\u4e00\u7684\u98ce\u9669\u8bc4\u4f30\u5e73\u53f0\uff0c\u4ee5\u51cf\u5c11\u5f00\u53d1\u8005\u7684\u8ba4\u77e5\u8d1f\u62c5\u5e76\u63d0\u9ad8\u53ef\u8ffd\u8e2a\u6027\u3002", "motivation": "\u73b0\u6709SCA\u5de5\u5177\u5b58\u5728\u76f2\u70b9\uff0c\u5c40\u9650\u4e8e\u5355\u4e00\u751f\u6001\u7cfb\u7edf\uff0c\u5355\u72ec\u8bc4\u4f30\u8f6f\u4ef6\u5de5\u4ef6\u6216\u793e\u533a\u6d3b\u52a8\uff0c\u5bfc\u81f4\u6570\u636e\u5206\u6563\uff0c\u98ce\u9669\u5224\u65ad\u56f0\u96be\u3002", "method": "Package Dashboard\u7ed3\u5408\u4e86\u4f9d\u8d56\u5173\u7cfb\u89e3\u6790\u548c\u4ed3\u5e93\u5206\u6790\uff0c\u7efc\u5408\u5305\u5143\u6570\u636e\u3001\u6f0f\u6d1e\u4fe1\u606f\u548c\u793e\u533a\u5065\u5eb7\u6307\u6807\u4ee5\u63d0\u4f9b\u66f4\u5168\u9762\u7684\u98ce\u9669\u8bc4\u4f30\u3002", "result": "\u5bf9\u4e94\u4e2aLinux\u53d1\u884c\u7248\u7684374,000\u4e2a\u5305\u8fdb\u884c\u5927\u89c4\u6a21\u7814\u7a76\u8868\u660e\uff0cPackage Dashboard \u80fd\u591f\u53d1\u73b0\u4f20\u7edf\u6f0f\u6d1e\u3001\u8bb8\u53ef\u8bc1\u51b2\u7a81\u4ee5\u53ca\u88ab\u5ffd\u89c6\u7684\u98ce\u9669\uff0c\u6bd4\u5982\u5b58\u6863\u6216\u65e0\u6cd5\u8bbf\u95ee\u7684\u4ed3\u5e93\u3002", "conclusion": "Package Dashboard\u589e\u5f3a\u4e86\u5f00\u6e90\u751f\u6001\u7cfb\u7edf\u7684\u900f\u660e\u5ea6\u3001\u53ef\u4fe1\u5ea6\u548c\u53ef\u8ffd\u8e2a\u6027\uff0c\u4e3a\u5f00\u53d1\u8005\u548cDevSecOps\u5de5\u7a0b\u5e08\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u98ce\u9669\u6d1e\u5bdf\u3002"}}
{"id": "2512.01233", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2512.01233", "abs": "https://arxiv.org/abs/2512.01233", "authors": ["Pratham Gupta", "Aditya Gabani", "Connor Nelson", "Yan Shoshitaishvili"], "title": "CTF Archive: Capture, Curate, Learn Forever", "comment": null, "summary": "Capture the Flag (CTF) competitions represent a powerful experiential learning approach within cybersecurity education, blending diverse concepts into interactive challenges. However, the short duration (typically 24-48 hours) and ephemeral infrastructure of these events often impede sustained educational benefit. Learners face substantial barriers in revisiting unsolved challenges, primarily due to the cumbersome process of manually reconstructing and rehosting the challenges without comprehensive documentation or guidance. To address this critical gap, we introduce CTF Archive, a platform designed to preserve the educational value of CTF competitions by centralizing and archiving hundreds of challenges spanning over a decade in fully configured, ready-to-use environments. By removing the complexity of environment setup, CTF Archive allows learners to focus directly on conceptual understanding rather than technical troubleshooting. The availability of these preserved challenges encourages in-depth research and exploration at the learner's pace, significantly enhancing conceptual comprehension without the pressures of live competition. Additionally, public accessibility lowers entry barriers, promoting an inclusive educational experience. Overall, CTF Archive provides a scalable solution to integrate persistent, practical cybersecurity learning into academic curricula.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2512.01649", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.01649", "abs": "https://arxiv.org/abs/2512.01649", "authors": ["Daniel Strassler", "Gabe Elkin", "Curran Schiefelbein", "Daniel Herring", "Ian Jessen", "David Johnson", "Santiago A. Paredes", "Tod Shannon", "Jim Flavin"], "title": "MIT Lincoln Laboratory: A Case Study on Improving Software Support for Research Projects", "comment": "Strassler, D., et al. MIT Lincoln Laboratory: A Case Study on Improving Software Support for Research Projects. Zenodo, 2025, https://doi.org/10.5281/zenodo.16878161", "summary": "Software plays an ever increasing role in complex system development and prototyping, and in recent years, MIT Lincoln Laboratory has sought to improve both the effectiveness and culture surrounding software engineering in execution of its mission. The Homeland Protection and Air Traffic Control Division conducted an internal study to examine challenges to effective and efficient research software development, and to identify ways to strengthen both the culture and execution for greater impact on our mission. Key findings of this study fell into three main categories: project attributes that influence how software development activities must be conducted and managed, potential efficiencies from centralization, opportunities to improve staffing and culture with respect to software practitioners. The study delivered actionable recommendations, including centralizing and standardizing software support tooling, developing a common database to help match the right software talent and needs to projects, and creating a software stakeholder panel to assist with continued improvement.", "AI": {"tldr": "\u8bba\u6587\u901a\u8fc7\u5185\u90e8\u7814\u7a76\u63a2\u8ba8\u4e86\u7814\u7a76\u9879\u76ee\u4e2d\u7684\u8f6f\u4ef6\u5f00\u53d1\u6311\u6218\uff0c\u5e76\u63d0\u51fa\u4e86\u6539\u5584\u6587\u5316\u3001\u7ba1\u7406\u548c\u6267\u884c\u7684\u5efa\u8bae\u3002", "motivation": "MIT\u6797\u80af\u5b9e\u9a8c\u5ba4\u5bfb\u6c42\u6539\u8fdb\u548c\u589e\u5f3a\u56f4\u7ed5\u8f6f\u4ef6\u5de5\u7a0b\u7684\u6709\u6548\u6027\uff0c\u5e76\u6539\u5584\u5176\u6587\u5316\u6c1b\u56f4\u3002", "method": "\u8fdb\u884c\u5185\u90e8\u7814\u7a76\uff0c\u5206\u6790\u6709\u6548\u7814\u7a76\u8f6f\u4ef6\u5f00\u53d1\u6240\u9762\u4e34\u7684\u6311\u6218\uff0c\u5e76\u63a2\u8ba8\u52a0\u5f3a\u6587\u5316\u548c\u6267\u884c\u7684\u9014\u5f84\u3002", "result": "\u7814\u7a76\u5f97\u51fa\u4e86\u4e09\u4e2a\u4e3b\u8981\u65b9\u9762\u7684\u5173\u952e\u53d1\u73b0\uff1a\u9879\u76ee\u7ba1\u7406\u3001\u96c6\u4e2d\u5316\u7684\u6f5c\u5728\u6548\u7387\u3001\u6539\u5584\u8f6f\u4ef6\u4eba\u5458\u914d\u5907\u548c\u6587\u5316\uff0c\u5e76\u63d0\u51fa\u4e86\u5305\u62ec\u96c6\u4e2d\u548c\u6807\u51c6\u5316\u5de5\u4ef6\u3001\u5efa\u7acb\u4eba\u624d\u6570\u636e\u5e93\u3001\u521b\u5efa\u8f6f\u4ef6\u5229\u76ca\u76f8\u5173\u8005\u5c0f\u7ec4\u7b49\u53ef\u64cd\u4f5c\u7684\u5efa\u8bae\u3002", "conclusion": "\u901a\u8fc7\u96c6\u4e2d\u3001\u6807\u51c6\u5316\u53ca\u521b\u5efa\u5171\u540c\u7684\u8f6f\u4ef6\u5e73\u53f0\u4ee5\u66f4\u597d\u5730\u5339\u914d\u4eba\u624d\u4e0e\u9879\u76ee\u9700\u6c42\uff0c\u53ef\u4ee5\u63d0\u9ad8\u8f6f\u4ef6\u5de5\u7a0b\u5bf9\u4efb\u52a1\u7684\u603b\u4f53\u5f71\u54cd\u3002"}}
{"id": "2512.01247", "categories": ["cs.CR", "cs.CY", "cs.HC"], "pdf": "https://arxiv.org/pdf/2512.01247", "abs": "https://arxiv.org/abs/2512.01247", "authors": ["Yiluo Wei", "Peixian Zhang", "Gareth Tyson"], "title": "Benchmarking and Understanding Safety Risks in AI Character Platforms", "comment": "Accepted to NDSS '26: The Network and Distributed System Security Symposium 2026", "summary": "AI character platforms, which allow users to engage in conversations with AI personas, are a rapidly growing application domain. However, their immersive and personalized nature, combined with technical vulnerabilities, raises significant safety concerns. Despite their popularity, a systematic evaluation of their safety has been notably absent. To address this gap, we conduct the first large-scale safety study of AI character platforms, evaluating 16 popular platforms using a benchmark set of 5,000 questions across 16 safety categories. Our findings reveal a critical safety deficit: AI character platforms exhibit an average unsafe response rate of 65.1%, substantially higher than the 17.7% average rate of the baselines. We further discover that safety performance varies significantly across different characters and is strongly correlated with character features such as demographics and personality. Leveraging these insights, we demonstrate that our machine learning model is able identify less safe characters with an F1-score of 0.81. This predictive capability can be beneficial for platforms, enabling improved mechanisms for safer interactions, character search/recommendations, and character creation. Overall, the results and findings offer valuable insights for enhancing platform governance and content moderation for safer AI character platforms.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2512.00852", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.00852", "abs": "https://arxiv.org/abs/2512.00852", "authors": ["Yandong Sun", "Qiang Huang", "Ziwei Xu", "Yiqun Sun", "Yixuan Tang", "Anthony K. H. Tung"], "title": "One Swallow Does Not Make a Summer: Understanding Semantic Structures in Embedding Spaces", "comment": null, "summary": "Embedding spaces are fundamental to modern AI, translating raw data into high-dimensional vectors that encode rich semantic relationships. Yet, their internal structures remain opaque, with existing approaches often sacrificing semantic coherence for structural regularity or incurring high computational overhead to improve interpretability. To address these challenges, we introduce the Semantic Field Subspace (SFS), a geometry-preserving, context-aware representation that captures local semantic neighborhoods within the embedding space. We also propose SAFARI (SemAntic Field subspAce deteRmInation), an unsupervised, modality-agnostic algorithm that uncovers hierarchical semantic structures using a novel metric called Semantic Shift, which quantifies how semantics evolve as SFSes evolve. To ensure scalability, we develop an efficient approximation of Semantic Shift that replaces costly SVD computations, achieving a 15~30x speedup with average errors below 0.01. Extensive evaluations across six real-world text and image datasets show that SFSes outperform standard classifiers not only in classification but also in nuanced tasks such as political bias detection, while SAFARI consistently reveals interpretable and generalizable semantic hierarchies. This work presents a unified framework for structuring, analyzing, and scaling semantic understanding in embedding spaces.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2512.01690", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.01690", "abs": "https://arxiv.org/abs/2512.01690", "authors": ["Philip Garrett", "Juan P. Galeotti", "Andrea Arcuri", "Alexander Poth", "Olsi Rrjolli"], "title": "Generating REST API Tests With Descriptive Names", "comment": null, "summary": "Automated test generation has become a key technique for ensuring software quality, particularly in modern API-based architectures. However, automatically generated test cases are typically assigned non-descriptive names (e.g., test0, test1), which reduces their readability and hinders their usefulness during comprehension and maintenance. In this work, we present three novel deterministic techniques to generate REST API test names. We then compare eight techniques in total for generating descriptive names for REST API tests automatically produced by the fuzzer EvoMaster, using 10 test cases generated for 9 different open-source APIs. The eight techniques include rule-based heuristics and large language model (LLM)-based approaches. Their effectiveness was empirically evaluated through two surveys (involving up to 39 people recruited via LinkedIn). Our results show that a rule-based approach achieves the highest clarity ratings among deterministic methods, performs on par with state-of-the-art LLM-based models such as Gemini and GPT-4o, and significantly outperforms GPT-3.5.\n  To further evaluate the practical impact of our results, an industrial case study was carried out with practitioners who actively use EvoMaster at Volkswagen AG. A developer questionnaire was then carried out based on the use of EvoMaster on four different APIs by four different users, for a total of 74 evaluated test cases. Feedback from practitioners further confirms that descriptive names produced by this approach improve test suite readability.\n  These findings highlight that lightweight, deterministic techniques can serve as effective alternatives to computationally expensive and security-sensitive LLM-based approaches for automated system-level test naming, providing a practical step toward more developer-friendly API test generation.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e09\u79cd\u786e\u5b9a\u6027\u7684\u6280\u672f\u6765\u751f\u6210REST API\u6d4b\u8bd5\u540d\u79f0\uff0c\u5e76\u901a\u8fc7\u6bd4\u8f83\u7814\u7a76\u663e\u793a\uff0c\u4e00\u79cd\u89c4\u5219\u57fa\u7840\u65b9\u6cd5\u5728\u786e\u5b9a\u6027\u548c\u6e05\u6670\u5ea6\u4e0a\u5747\u4f18\u4e8e\u6216\u4e0e\u5148\u8fdb\u7684LLM\u57fa\u7840\u65b9\u6cd5\u76f8\u5f53\u3002", "motivation": "\u81ea\u52a8\u751f\u6210\u6d4b\u8bd5\u7528\u4f8b\u5f80\u5f80\u4f7f\u7528\u65e0\u610f\u4e49\u7684\u540d\u79f0\uff0c\u964d\u4f4e\u4e86\u53ef\u8bfb\u6027\u548c\u7ef4\u62a4\u6027\u3002\u8be5\u7814\u7a76\u65e8\u5728\u6539\u8fdbREST API\u6d4b\u8bd5\u7684\u81ea\u52a8\u547d\u540d\u6280\u672f\uff0c\u63d0\u9ad8\u5f00\u53d1\u8005\u5728\u7406\u89e3\u548c\u7ef4\u62a4\u6d4b\u8bd5\u7528\u4f8b\u65f6\u7684\u6548\u7387\u3002", "method": "\u672c\u6587\u63d0\u51fa\u5e76\u6bd4\u8f83\u4e86\u5305\u62ec\u4e09\u79cd\u65b0\u578b\u786e\u5b9a\u6027\u53ca\u5176\u4ed6\u4e94\u79cd\u89c4\u5219\u57fa\u7840\u548cLLM\u57fa\u7840\u7684\u65b9\u6cd5\u7528\u4e8e\u751f\u6210REST API\u6d4b\u8bd5\u540d\u79f0\u3002\u8bc4\u4ef7\u901a\u8fc7\u4e24\u4e2a\u8c03\u67e5\uff08\u6700\u591a39\u4eba\uff09\u548c\u4e00\u4e2a\u5de5\u4e1a\u6848\u4f8b\u7814\u7a76\uff08Volkswagen AG\uff09\u8fdb\u884c\uff0c\u5206\u6790\u4e868\u79cd\u4e0d\u540cAPI\u768474\u4e2a\u6d4b\u8bd5\u7528\u4f8b\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u89c4\u5219\u57fa\u7840\u65b9\u6cd5\u5728\u786e\u5b9a\u6027\u65b9\u6cd5\u4e2d\u83b7\u5f97\u4e86\u6700\u9ad8\u7684\u6e05\u6670\u5ea6\u8bc4\u5206\uff0c\u5e76\u4e14\u4e0e\u5148\u8fdb\u7684LLM\u6a21\u578b\u5982Gemini\u548cGPT-4o\u6027\u80fd\u76f8\u5f53\uff0c\u663e\u8457\u4f18\u4e8eGPT-3.5\u3002\u6b64\u5916\uff0c\u5de5\u4e1a\u6848\u4f8b\u7814\u7a76\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86\u8fd9\u4e9b\u63cf\u8ff0\u6027\u540d\u79f0\u5bf9\u63d0\u9ad8\u6d4b\u8bd5\u5957\u4ef6\u53ef\u8bfb\u6027\u7684\u5b9e\u9645\u5f71\u54cd\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u8868\u660e\uff0c\u8f7b\u91cf\u7ea7\u548c\u786e\u5b9a\u6027\u7684\u6280\u672f\u53ef\u4ee5\u6709\u6548\u5730\u4f5c\u4e3a\u8ba1\u7b97\u6210\u672c\u9ad8\u548c\u5b89\u5168\u6027\u654f\u611f\u7684LLM\u57fa\u7840\u65b9\u6cd5\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u5b9e\u73b0\u66f4\u53cb\u597d\u548c\u66f4\u5177\u53ef\u8bfb\u6027\u7684API\u6d4b\u8bd5\u751f\u6210\u3002"}}
{"id": "2512.00881", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00881", "abs": "https://arxiv.org/abs/2512.00881", "authors": ["Li Yuan", "Qingfei Huang", "Bingshan Zhu", "Yi Cai", "Qingbao Huang", "Changmeng Zheng", "Zikun Deng", "Tao Wang"], "title": "Hybrid-DMKG: A Hybrid Reasoning Framework over Dynamic Multimodal Knowledge Graphs for Multimodal Multihop QA with Knowledge Editing", "comment": "Accepted by AAAI 2026", "summary": "Multimodal Knowledge Editing (MKE) extends traditional knowledge editing to settings involving both textual and visual modalities. However, existing MKE benchmarks primarily assess final answer correctness while neglecting the quality of intermediate reasoning and robustness to visually rephrased inputs. To address this limitation, we introduce MMQAKE, the first benchmark for multimodal multihop question answering with knowledge editing. MMQAKE evaluates (1) a model's ability to reason over 2-5-hop factual chains that span both text and images, including performance at each intermediate step, and (2) robustness to visually rephrased inputs in multihop questions. Our evaluation shows that current MKE methods often struggle to consistently update and reason over multimodal reasoning chains after knowledge edits. To overcome these challenges, we propose Hybrid-DMKG, a hybrid reasoning framework built on a dynamic multimodal knowledge graph (DMKG) to enable accurate multihop reasoning over updated multimodal knowledge. Hybrid-DMKG first uses a large language model to decompose multimodal multihop questions into sequential sub-questions, then applies a multimodal retrieval model to locate updated facts by jointly encoding each sub-question with candidate entities and their associated images. For answer inference, a hybrid reasoning module operates over the DMKG via two parallel paths: (1) relation linking prediction, and (2) RAG reasoning with large vision-language models. A decision module aggregates evidence from both paths to select the most credible answer. Experimental results on MMQAKE show that Hybrid-DMKG significantly outperforms existing MKE approaches, achieving higher accuracy and improved robustness to knowledge updates.", "AI": {"tldr": "MMQAKE\uff1a\u591a\u6a21\u6001\u591a\u8df3\u95ee\u7b54\u77e5\u8bc6\u7f16\u8f91\u7684\u65b0\u57fa\u51c6\u53ca\u5176\u6df7\u5408\u63a8\u7406\u6846\u67b6Hybrid-DMKG\uff0c\u901a\u8fc7\u52a8\u6001\u77e5\u8bc6\u56fe\u548c\u53cc\u8def\u5f84\u63a8\u7406\u663e\u8457\u63d0\u5347\u51c6\u786e\u6027\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u89e3\u51b3\u73b0\u6709MKE\u57fa\u51c6\u7f3a\u4e4f\u5bf9\u4e2d\u95f4\u63a8\u7406\u8d28\u91cf\u548c\u89c6\u89c9\u91cd\u8ff0\u8f93\u5165\u9c81\u68d2\u6027\u7684\u8bc4\u4f30\u95ee\u9898\uff0c\u65e8\u5728\u63d0\u5347\u591a\u6a21\u6001\u77e5\u8bc6\u7f16\u8f91\u7684\u7efc\u5408\u80fd\u529b\u3002", "method": "\u5f15\u5165MMQAKE\u57fa\u51c6\u5e76\u6784\u5efaHybrid-DMKG\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u7ed3\u5408\u8bed\u8a00\u6a21\u578b\u5206\u89e3\u95ee\u9898\u3001\u591a\u6a21\u6001\u68c0\u7d22\u6a21\u578b\u5b9a\u4f4d\u4e8b\u5b9e\u3001\u53cc\u8def\u5f84\u6df7\u5408\u63a8\u7406\uff08\u5173\u7cfb\u94fe\u63a5/RAG\u63a8\u7406\uff09\u4e0e\u52a8\u6001\u77e5\u8bc6\u56fe\u6280\u672f\u3002", "result": "\u5b9e\u9a8c\u663e\u793aHybrid-DMKG\u5728MMQAKE\u4e0a\u4f18\u4e8e\u73b0\u6709MKE\u65b9\u6cd5\uff0c\u51c6\u786e\u6027\u548c\u5bf9\u77e5\u8bc6\u66f4\u65b0\u7684\u9c81\u68d2\u6027\u663e\u8457\u63d0\u5347\u3002", "conclusion": "Hybrid-DMKG\u8bc1\u660e\u4e86\u53cc\u8def\u5f84\u63a8\u7406\u548c\u52a8\u6001\u77e5\u8bc6\u56fe\u5728\u591a\u6a21\u6001\u77e5\u8bc6\u7f16\u8f91\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5e76\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u8bc4\u4f30\u57fa\u51c6\u3002"}}
{"id": "2512.01939", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.01939", "abs": "https://arxiv.org/abs/2512.01939", "authors": ["Yanlin Wang", "Xinyi Xu", "Jiachi Chen", "Tingting Bi", "Wenchao Gu", "Zibin Zheng"], "title": "An Empirical Study of Agent Developer Practices in AI Agent Frameworks", "comment": null, "summary": "The rise of large language models (LLMs) has sparked a surge of interest in agents, leading to the rapid growth of agent frameworks. Agent frameworks are software toolkits and libraries that provide standardized components, abstractions, and orchestration mechanisms to simplify agent development. Despite widespread use of agent frameworks, their practical applications and how they influence the agent development process remain underexplored. Different agent frameworks encounter similar problems during use, indicating that these recurring issues deserve greater attention and call for further improvements in agent framework design. Meanwhile, as the number of agent frameworks continues to grow and evolve, more than 80% of developers report difficulties in identifying the frameworks that best meet their specific development requirements. In this paper, we conduct the first empirical study of LLM-based agent frameworks, exploring real-world experiences of developers in building AI agents. To compare how well the agent frameworks meet developer needs, we further collect developer discussions for the ten previously identified agent frameworks, resulting in a total of 11,910 discussions. Finally, by analyzing these discussions, we compare the frameworks across five dimensions: development efficiency, functional abstraction, learning cost, performance optimization, and maintainability, which refers to how easily developers can update and extend both the framework itself and the agents built upon it over time. Our comparative analysis reveals significant differences among frameworks in how they meet the needs of agent developers. Overall, we provide a set of findings and implications for the LLM-driven AI agent framework ecosystem and offer insights for the design of future LLM-based agent frameworks and agent developers.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2512.01295", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2512.01295", "abs": "https://arxiv.org/abs/2512.01295", "authors": ["Mihai Christodorescu", "Earlence Fernandes", "Ashish Hooda", "Somesh Jha", "Johann Rehberger", "Khawaja Shams"], "title": "Systems Security Foundations for Agentic Computing", "comment": null, "summary": "This paper articulates short- and long-term research problems in AI agent security and privacy, using the lens of computer systems security. This approach examines end-to-end security properties of entire systems, rather than AI models in isolation. While we recognize that hardening a single model is useful, it is important to realize that it is often insufficient. By way of an analogy, creating a model that is always helpful and harmless is akin to creating software that is always helpful and harmless. The collective experience of decades of cybersecurity research and practice shows that this is insufficient. Rather, constructing an informed and realistic attacker model before building a system, applying hard-earned lessons from software security, and continuous improvement of security posture is a tried-and-tested approach to securing real computer systems. A key goal is to examine where research challenges arise when applying traditional security principles in the context of AI agents. A secondary goal of this report is to distill these ideas for AI and ML practitioners and researchers. We discuss the challenges of applying security principles to agentic computing, present 11 case studies of real attacks on agentic systems, and define a series of new research problems specific to the security of agentic systems.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2512.00918", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00918", "abs": "https://arxiv.org/abs/2512.00918", "authors": ["Cen Lu", "Yung-Chen Tang", "Andrea Cavallaro"], "title": "Minimal neuron ablation triggers catastrophic collapse in the language core of Large Vision-Language Models", "comment": "15 pages, 6 figures,", "summary": "Large Vision-Language Models (LVLMs) have shown impressive multimodal understanding capabilities, yet their robustness is poorly understood. In this paper, we investigate the structural vulnerabilities of LVLMs to identify any critical neurons whose removal triggers catastrophic collapse. In this context, we propose CAN, a method to detect Consistently Activated Neurons and to locate critical neurons by progressive masking. Experiments on LLaVA-1.5-7b-hf and InstructBLIP-Vicuna-7b reveal that masking only a tiny portion of the language model's feed-forward networks (just as few as four neurons in extreme cases) suffices to trigger catastrophic collapse. Notably, critical neurons are predominantly localized in the language model rather than in the vision components, and the down-projection layer is a particularly vulnerable structure. We also observe a consistent two-stage collapse pattern: initial expressive degradation followed by sudden, complete collapse. Our findings provide important insights for safety research in LVLMs.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2512.01326", "categories": ["cs.CR", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.01326", "abs": "https://arxiv.org/abs/2512.01326", "authors": ["Omar Farooq Khan Suri", "John McCrae"], "title": "Securing Large Language Models (LLMs) from Prompt Injection Attacks", "comment": "10 pages, 1 figure, 1 table", "summary": "Large Language Models (LLMs) are increasingly being deployed in real-world applications, but their flexibility exposes them to prompt injection attacks. These attacks leverage the model's instruction-following ability to make it perform malicious tasks. Recent work has proposed JATMO, a task-specific fine-tuning approach that trains non-instruction-tuned base models to perform a single function, thereby reducing susceptibility to adversarial instructions. In this study, we evaluate the robustness of JATMO against HOUYI, a genetic attack framework that systematically mutates and optimizes adversarial prompts. We adapt HOUYI by introducing custom fitness scoring, modified mutation logic, and a new harness for local model testing, enabling a more accurate assessment of defense effectiveness. We fine-tuned LLaMA 2-7B, Qwen1.5-4B, and Qwen1.5-0.5B models under the JATMO methodology and compared them with a fine-tuned GPT-3.5-Turbo baseline. Results show that while JATMO reduces attack success rates relative to instruction-tuned models, it does not fully prevent injections; adversaries exploiting multilingual cues or code-related disruptors still bypass defenses. We also observe a trade-off between generation quality and injection vulnerability, suggesting that better task performance often correlates with increased susceptibility. Our results highlight both the promise and limitations of fine-tuning-based defenses and point toward the need for layered, adversarially informed mitigation strategies.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2512.00969", "categories": ["cs.AI", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.00969", "abs": "https://arxiv.org/abs/2512.00969", "authors": ["Felix Saretzky", "Lucas Andersen", "Thomas Engel", "Fazel Ansari"], "title": "Integrating Causal Foundation Model in Prescriptive Maintenance Framework for Optimizing Production Line OEE", "comment": "9 pages, 3 images, 1 table, conference paper", "summary": "The transition to prescriptive maintenance in manufacturing is critically constrained by a dependence on predictive models. These models tend to rely on spurious correlations rather than identifying the true causal drivers of failures, often leading to costly misdiagnoses and ineffective interventions. This fundamental limitation results in a key-challenge: while we can predict that a failure may occur, we lack a systematic method to understand why a failure occurs, thereby providing the basis for identifying the most effective intervention. This paper proposes a model based on causal machine learning to bridge this gap. Our objective is to move beyond diagnosis to active prescription by simulating and evaluating potential fixes toward optimizing KPIs such as Overall Equipment Effectiveness (OEE). For this purpose a pre-trained causal foundation model is used as a \"what-if\" model to estimate the effects of potential fixes. By measuring the causal effect of each intervention on system-level KPIs, it provides a data-driven ranking of actions to recommend at the production line. This process not only identifies root causes but also quantifies their operational impact. The model is evaluated using semi-synthetic manufacturing data and compared with a baseline machine learning model. This paper sets the technical basis for a robust prescriptive maintenance framework, allowing engineers to test potential solutions in a causal environment to make more effective operational decisions and reduce costly downtimes.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2512.01335", "categories": ["cs.CR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.01335", "abs": "https://arxiv.org/abs/2512.01335", "authors": ["Xinyun Zhou", "Xinfeng Li", "Yinan Peng", "Ming Xu", "Xuanwang Zhang", "Miao Yu", "Yidong Wang", "Xiaojun Jia", "Kun Wang", "Qingsong Wen", "XiaoFeng Wang", "Wei Dong"], "title": "EmoRAG: Evaluating RAG Robustness to Symbolic Perturbations", "comment": "Accepted to ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD) 2026", "summary": "Retrieval-Augmented Generation (RAG) systems are increasingly central to robust AI, enhancing large language model (LLM) faithfulness by incorporating external knowledge. However, our study unveils a critical, overlooked vulnerability: their profound susceptibility to subtle symbolic perturbations, particularly through near-imperceptible emoticon tokens such as \"(@_@)\" that can catastrophically mislead retrieval, termed EmoRAG. We demonstrate that injecting a single emoticon into a query makes it nearly 100% likely to retrieve semantically unrelated texts that contain a matching emoticon. Our extensive experiment across general question-answering and code domains, using a range of state-of-the-art retrievers and generators, reveals three key findings: (I) Single-Emoticon Disaster: Minimal emoticon injections cause maximal disruptions, with a single emoticon almost 100% dominating RAG output. (II) Positional Sensitivity: Placing an emoticon at the beginning of a query can cause severe perturbation, with F1-Scores exceeding 0.92 across all datasets. (III) Parameter-Scale Vulnerability: Counterintuitively, models with larger parameters exhibit greater vulnerability to the interference. We provide an in-depth analysis to uncover the underlying mechanisms of these phenomena. Furthermore, we raise a critical concern regarding the robustness assumption of current RAG systems, envisioning a threat scenario where an adversary exploits this vulnerability to manipulate the RAG system. We evaluate standard defenses and find them insufficient against EmoRAG. To address this, we propose targeted defenses, analyzing their strengths and limitations in mitigating emoticon-based perturbations. Finally, we outline future directions for building robust RAG systems.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2512.00997", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00997", "abs": "https://arxiv.org/abs/2512.00997", "authors": ["Param Biyani", "Shashank Kirtania", "Yasharth Bajpai", "Sumit Gulwani", "Ashish Tiwari"], "title": "IndiMathBench: Autoformalizing Mathematical Reasoning Problems with a Human Touch", "comment": null, "summary": "We introduce IndiMathBench, a human-verified benchmark designed to evaluate mathematical theorem proving, curated using an AI-powered human-assisted pipeline for formalizing natural language problems in Lean. IndiMathBench is composed of 312 formal Lean 4 theorems paired with their corresponding informal problem statements, sourced from Indian Mathematics Olympiads. Through category-based retrieval, iterative compiler feedback, and multi-model ensembles, our pipeline generates candidate formalizations that experts efficiently validate via an interactive dashboard with automated quality summaries. Evaluation across multiple frontier models demonstrates that autoformalization remains challenging, with substantial gaps between syntactic validity and semantic correctness, while theorem proving success rates remain low even with iterative refinement, demonstrating that \\benchmark~presents a challenging testbed for mathematical reasoning. IndiMathBench is available at https://github.com/prmbiy/IndiMathBench.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86IndiMathBench\uff0c\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u6570\u5b66\u5b9a\u7406\u8bc1\u660e\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u91c7\u7528AI\u4e0e\u4eba\u5de5\u534f\u4f5c\u7684\u65b9\u5f0f\u6784\u5efa\uff0c\u5305\u542b\u5927\u91cf\u6765\u81ea\u5370\u5ea6\u6570\u5b66\u7ade\u8d5b\u7684\u5f62\u5f0f\u5316\u95ee\u9898\u3002", "motivation": "\u6570\u5b66\u5b9a\u7406\u7684\u81ea\u52a8\u5f62\u5f0f\u5316\u4e0e\u8bc1\u660e\u662f\u4e00\u9879\u5173\u952e\u6311\u6218\uff0c\u73b0\u6709\u57fa\u51c6\u5728\u8bed\u4e49\u6b63\u786e\u6027\u548c\u591a\u6837\u6027\u4e0a\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u66f4\u4e25\u683c\u7684\u6d4b\u8bd5\u5de5\u5177\u3002", "method": "\u901a\u8fc7\u7c7b\u522b\u4e3a\u57fa\u7840\u7684\u68c0\u7d22\u3001\u8fed\u4ee3\u7f16\u8bd1\u5668\u53cd\u9988\u3001\u591a\u6a21\u578b\u96c6\u6210\u751f\u6210\u5019\u9009\u5f62\u5f0f\u5316\u95ee\u9898\uff0c\u5229\u7528\u4ea4\u4e92\u5f0f\u4eea\u8868\u76d8\u548c\u81ea\u52a8\u5316\u8d28\u91cf\u6458\u8981\u5e2e\u52a9\u4e13\u5bb6\u9a8c\u8bc1\u3002", "result": "\u8bc4\u4f30\u663e\u793a\u81ea\u52a8\u5f62\u5f0f\u5316\u4ecd\u7136\u5b58\u5728\u663e\u8457\u6311\u6218\uff0c\u8bed\u6cd5\u6709\u6548\u6027\u548c\u8bed\u4e49\u6b63\u786e\u6027\u4e4b\u95f4\u5dee\u8ddd\u5927\uff0c\u5b9a\u7406\u8bc1\u660e\u7684\u6210\u529f\u7387\u8f83\u4f4e\uff0c\u5373\u4f7f\u5728\u8fed\u4ee3\u4f18\u5316\u540e\u4ecd\u4e0d\u7406\u60f3\u3002", "conclusion": "IndiMathBench\u4e3a\u6570\u5b66\u63a8\u7406\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u6d4b\u8bd5\u5e73\u53f0\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u6a21\u578b\u5728\u590d\u6742\u5f62\u5f0f\u5316\u4efb\u52a1\u4e2d\u7684\u4e0d\u8db3\u3002"}}
{"id": "2512.01017", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.01017", "abs": "https://arxiv.org/abs/2512.01017", "authors": ["Xinhang Li", "Jingbo Zhou", "Pengfei Luo", "Yixiong Xiao", "Tong Xu"], "title": "ChartAnchor: Chart Grounding with Structural-Semantic Fidelity", "comment": null, "summary": "Recent advances in multimodal large language models (MLLMs) highlight the need for benchmarks that rigorously evaluate structured chart comprehension.Chart grounding refers to the bidirectional alignment between a chart's visual appearance and the structured semantics. This task requires models to produce a symbolic specification that faithfully captures the chart's visual and structural intent, while also recovering the underlying tabular data with precise values and relationships. Chart grounding directly reflects a model's capabilities in numerical reasoning, multimodal alignment, and structural reconstruction, and has several important applications in real-world scenarios.Existing benchmarks, constrained by narrow chart diversity, isolated tasks, and incomplete evaluation frameworks, fail to holistically assess grounding. To address this, we propose ChartAnchor, a comprehensive benchmark of 8k+ chart-table-code triples spanning 30 chart types drawn from diverse real-world and augmented sources. ChartAnchor introduces two complementary tasks: chart-to-code generation (synthesizing executable code to replicate charts) and controlled chart-to-table reconstruction (extracting exact data with predefined headers), enabling cross-validation of visual and numerical fidelity. A multi-level evaluation framework integrates semantic validation, stylistic analysis, and perceptual metrics to assess both structural and content-level correctness. Extensive experiments on MLLMs reveal critical limitations in numerical precision and code synthesis, emphasizing the need for structured reasoning beyond surface-level perception. By unifying symbolic and data-driven grounding, ChartAnchor establishes a rigorous foundation for chart grounding, offering meaningful insights for advancing MLLMs in scientific, financial, and industrial domains.", "AI": {"tldr": "\u63d0\u51fa ChartAnchor\uff0c\u4e00\u4e2a\u7efc\u5408\u6027\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u7ed3\u6784\u5316\u56fe\u8868\u7406\u89e3\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u53d7\u9650\u4e8e\u56fe\u8868\u591a\u6837\u6027\u72ed\u7a84\u3001\u4efb\u52a1\u5b64\u7acb\u548c\u4e0d\u5b8c\u6574\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u65e0\u6cd5\u5168\u9762\u8bc4\u4f30\u6a21\u578b\u80fd\u529b\u3002", "method": "\u5f15\u5165 ChartAnchor \u57fa\u51c6\uff0c\u5305\u62ec 8k+ \u56fe\u8868-\u8868\u683c-\u4ee3\u7801\u4e09\u5143\u7ec4\uff0c\u6db5\u76d630\u79cd\u56fe\u8868\u7c7b\u578b\uff0c\u5e76\u63d0\u51fa\u4e86\u4e24\u4e2a\u4e92\u8865\u4efb\u52a1\uff1a\u56fe\u8868\u5230\u4ee3\u7801\u751f\u6210\u548c\u53d7\u63a7\u56fe\u8868\u5230\u8868\u683c\u91cd\u5efa\uff0c\u7ed3\u5408\u591a\u5c42\u8bc4\u4f30\u6846\u67b6\u3002", "result": "\u5e7f\u6cdb\u5b9e\u9a8c\u63ed\u793a\u4e86 MLLMs \u5728\u6570\u5b57\u7cbe\u5ea6\u548c\u4ee3\u7801\u5408\u6210\u65b9\u9762\u7684\u91cd\u8981\u5c40\u9650\u6027\uff0c\u5f3a\u8c03\u4e86\u7ed3\u6784\u5316\u63a8\u7406\u7684\u5fc5\u8981\u6027\u3002", "conclusion": "ChartAnchor \u901a\u8fc7\u7edf\u4e00\u7b26\u53f7\u548c\u6570\u636e\u9a71\u52a8\u7684\u63a5\u5730\uff0c\u4e3a\u56fe\u8868\u63a5\u5730\u5efa\u7acb\u4e86\u4e25\u683c\u7684\u57fa\u7840\uff0c\u63a8\u52a8\u4e86 MLLMs \u5728\u591a\u4e2a\u9886\u57df\u7684\u5e94\u7528\u3002"}}
{"id": "2512.01391", "categories": ["cs.CR", "cs.NI"], "pdf": "https://arxiv.org/pdf/2512.01391", "abs": "https://arxiv.org/abs/2512.01391", "authors": ["Yevheniya Nosyk", "Maciej Korczy\u0144ski", "Carlos Ga\u00f1\u00e1n", "Sourena Maroofi", "Jan Bayer", "Zul Odgerel", "Samaneh Tajalizadehkhoob", "Andrzej Duda"], "title": "INFERMAL: Inferential analysis of maliciously registered domains", "comment": null, "summary": "Cybercriminals have long depended on domain names for phishing, spam, malware distribution, and botnet operation. To facilitate the malicious activities, they continually register new domain names for exploitation. Previous work revealed an abnormally high concentration of malicious registrations in a handful of domain name registrars and top-level domains (TLDs). Anecdotal evidence suggests that low registration prices attract cybercriminals, implying that higher costs may potentially discourage them. However, no existing study has systematically analyzed the factors driving abuse, leaving a critical gap in understanding how different variables influence malicious registrations. In this report, we carefully distill the inclinations and aversions of malicious actors during the registration of new phishing domain names. We compile a comprehensive list of 73 features encompassing three main latent factors: registration attributes, proactive verification, and reactive security practices. Through a GLM regression analysis, we find that each dollar reduction in registration fees corresponds to a 49% increase in malicious domains. The availability of free services, such as web hosting, drives an 88% surge in phishing activities. Conversely, stringent restrictions cut down abuse by 63%, while registrars providing API access for domain registration or account creation experience a staggering 401% rise in malicious domains. This exploration may assist intermediaries involved in domain registration to develop tailored anti-abuse practices, yet aligning them with their economic incentives.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u6076\u610f\u57df\u540d\u6ce8\u518c\u7684\u9a71\u52a8\u56e0\u7d20\uff0c\u901a\u8fc7\u5206\u679073\u4e2a\u7279\u5f81\uff0c\u53d1\u73b0\u4f4e\u4ef7\u6ce8\u518c\u8d39\u3001\u514d\u8d39\u6258\u7ba1\u670d\u52a1\u548c\u5bbd\u677e\u9650\u5236\u663e\u8457\u589e\u52a0\u6076\u610f\u6d3b\u52a8\uff0c\u800c\u4e25\u683c\u9650\u5236\u53ef\u6709\u6548\u51cf\u5c11\u6ee5\u7528\u3002", "motivation": "\u63ed\u793a\u4e86\u6076\u610f\u57df\u540d\u6ce8\u518c\u7684\u9a71\u52a8\u56e0\u7d20\uff0c\u586b\u8865\u4e86\u4ee5\u5f80\u7814\u7a76\u7684\u7a7a\u767d\uff0c\u4e3a\u6ce8\u518c\u5546\u5236\u5b9a\u9488\u5bf9\u6027\u53cd\u6ee5\u7528\u7b56\u7565\u63d0\u4f9b\u4e86\u4f9d\u636e\u3002", "method": "\u91c7\u7528GLM\u56de\u5f52\u5206\u6790\uff0c\u5f52\u7eb3\u4e8673\u4e2a\u4e0e\u6ce8\u518c\u5c5e\u6027\u3001\u4e3b\u52a8\u9a8c\u8bc1\u548c\u88ab\u52a8\u5b89\u5168\u76f8\u5173\u7684\u7279\u5f81\uff0c\u91cf\u5316\u4e86\u5404\u56e0\u7d20\u5bf9\u6076\u610f\u6ce8\u518c\u7684\u5f71\u54cd\u3002", "result": "\u6bcf\u964d\u4f4e1\u7f8e\u5143\u6ce8\u518c\u8d39\uff0c\u6076\u610f\u57df\u540d\u589e\u52a049%\uff1b\u514d\u8d39\u670d\u52a1\u4f7f\u9493\u9c7c\u6d3b\u52a8\u6fc0\u589e88%\uff1b\u4e25\u683c\u9650\u5236\u51cf\u5c11\u6ee5\u752863%\uff0c\u800c\u63d0\u4f9bAPI\u6ce8\u518c\u4f7f\u6076\u610f\u57df\u540d\u589e\u957f401%\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u53ef\u5e2e\u52a9\u6ce8\u518c\u5546\u5236\u5b9a\u53cd\u6ee5\u7528\u7b56\u7565\uff0c\u540c\u65f6\u9700\u5e73\u8861\u7ecf\u6d4e\u6fc0\u52b1\u3002"}}
{"id": "2512.01020", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.01020", "abs": "https://arxiv.org/abs/2512.01020", "authors": ["Jinu Lee", "Kyoung-Woon On", "Simeng Han", "Arman Cohan", "Julia Hockenmaier"], "title": "Evaluating Legal Reasoning Traces with Legal Issue Tree Rubrics", "comment": null, "summary": "Evaluating the quality of LLM-generated reasoning traces in expert domains (e.g., law) is essential for ensuring credibility and explainability, yet remains challenging due to the inherent complexity of such reasoning tasks. We introduce LEGIT (LEGal Issue Trees), a novel large-scale (24K instances) expert-level legal reasoning dataset with an emphasis on reasoning trace evaluation. We convert court judgments into hierarchical trees of opposing parties' arguments and the court's conclusions, which serve as rubrics for evaluating the issue coverage and correctness of the reasoning traces. We verify the reliability of these rubrics via human expert annotations and comparison with coarse, less informative rubrics. Using the LEGIT dataset, we show that (1) LLMs' legal reasoning ability is seriously affected by both legal issue coverage and correctness, and that (2) retrieval-augmented generation (RAG) and RL with rubrics bring complementary benefits for legal reasoning abilities, where RAG improves overall reasoning capability, whereas RL improves correctness albeit with reduced coverage.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2512.01437", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2512.01437", "abs": "https://arxiv.org/abs/2512.01437", "authors": ["Suhyeon Lee", "Hyeongyeong Kim"], "title": "Inside Qubic's Selfish Mining Campaign on Monero: Evidence, Tactics, and Limits", "comment": null, "summary": "We analyze Qubic's advertised selfish mining campaign on Monero in 2025. Combining data from Monero nodes, and the Qubic pool API, we reconstruct Qubic-attributed blocks and hashrate and detect ten intervals consistent with selfish mining strategies. In these intervals, Qubic's average hashrate share rises to the 23-34\\% range, yet sustained 51\\% control is never observed. We evaluate the campaign against the classical selfish mining model and a modified Markov-chain model that reflects Qubic's conservative release strategy: both predict lower revenue than honest mining at the inferred parameters, and the data largely confirms this while still showing noticeable deviations from the predicted curve. We interpret this gap between model and measurements in terms of Qubic's time-varying hashrate and coarse-grained attack segmentation.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2512.01045", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.01045", "abs": "https://arxiv.org/abs/2512.01045", "authors": ["Shenxi Liu", "Kan Li", "Mingyang Zhao", "Yuhang Tian", "Shoujun Zhou", "Bin Li"], "title": "Med-CRAFT: Automated Construction of Interpretable and Multi-Hop Video Workloads via Knowledge Graph Traversal", "comment": "8 pages, 7 figures", "summary": "The scarcity of high-quality, logically annotated video datasets remains a primary bottleneck in advancing Multi-Modal Large Language Models (MLLMs) for the medical domain. Traditional manual annotation is prohibitively expensive and non-scalable, while existing synthetic methods often suffer from stochastic hallucinations and a lack of logical interpretability. To address these challenges, we introduce \\textbf{\\PipelineName}, a novel neuro-symbolic data engineering framework that formalizes benchmark synthesis as a deterministic graph traversal process. Unlike black-box generative approaches, Med-CRAFT extracts structured visual primitives (e.g., surgical instruments, anatomical boundaries) from raw video streams and instantiates them into a dynamic Spatiotemporal Knowledge Graph. By anchoring query generation to valid paths within this graph, we enforce a rigorous Chain-of-Thought (CoT) provenance for every synthesized benchmark item. We instantiate this pipeline to produce M3-Med-Auto, a large-scale medical video reasoning benchmark exhibiting fine-grained temporal selectivity and multi-hop logical complexity. Comprehensive evaluations demonstrate that our automated pipeline generates query workloads with complexity comparable to expert-curated datasets. Furthermore, a logic alignment analysis reveals a high correlation between the prescribed graph topology and the reasoning steps of state-of-the-art MLLMs, validating the system's capability to encode verifiable logic into visual-linguistic benchmarks. This work paves the way for scalable, low-cost construction of robust evaluation protocols in critical domains.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2512.01574", "categories": ["cs.CR", "cs.AR"], "pdf": "https://arxiv.org/pdf/2512.01574", "abs": "https://arxiv.org/abs/2512.01574", "authors": ["Sangpyo Kim", "Hyesung Ji", "Jongmin Kim", "Wonseok Choi", "Jaiyoung Park", "Jung Ho Ahn"], "title": "IVE: An Accelerator for Single-Server Private Information Retrieval Using Versatile Processing Elements", "comment": "15 pages, 14 figures, accepted at HPCA 2026", "summary": "Private information retrieval (PIR) is an essential cryptographic protocol for privacy-preserving applications, enabling a client to retrieve a record from a server's database without revealing which record was requested. Single-server PIR based on homomorphic encryption has particularly gained immense attention for its ease of deployment and reduced trust assumptions. However, single-server PIR remains impractical due to its high computational and memory bandwidth demands. Specifically, reading the entirety of large databases from storage, such as SSDs, severely limits its performance. To address this, we propose IVE, an accelerator for single-server PIR with a systematic extension that enables practical retrieval from large databases using DRAM. Recent advances in DRAM capacity allow PIR for large databases to be served entirely from DRAM, removing its dependence on storage bandwidth. Although the memory bandwidth bottleneck still remains, multi-client batching effectively amortizes database access costs across concurrent requests to improve throughput. However, client-specific data remains a bottleneck, whose bandwidth requirements ultimately limits performance. IVE overcomes this by employing a large on-chip scratchpad with an operation scheduling algorithm that maximizes data reuse, further boosting throughput. Additionally, we introduce sysNTTU, a versatile functional unit that enhances area efficiency without sacrificing performance. We also propose a heterogeneous memory system architecture, which enables a linear scaling of database sizes without a throughput degradation. Consequently, IVE achieves up to 1,275x higher throughput compared to prior PIR hardware solutions.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2512.01046", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.01046", "abs": "https://arxiv.org/abs/2512.01046", "authors": ["Hadi Nekoei", "Alexandre Blondin Mass\u00e9", "Rachid Hassani", "Sarath Chandar", "Vincent Mai"], "title": "Shielded Controller Units for RL with Operational Constraints Applied to Remote Microgrids", "comment": null, "summary": "Reinforcement learning (RL) is a powerful framework for optimizing decision-making in complex systems under uncertainty, an essential challenge in real-world settings, particularly in the context of the energy transition. A representative example is remote microgrids that supply power to communities disconnected from the main grid. Enabling the energy transition in such systems requires coordinated control of renewable sources like wind turbines, alongside fuel generators and batteries, to meet demand while minimizing fuel consumption and battery degradation under exogenous and intermittent load and wind conditions. These systems must often conform to extensive regulations and complex operational constraints. To ensure that RL agents respect these constraints, it is crucial to provide interpretable guarantees. In this paper, we introduce Shielded Controller Units (SCUs), a systematic and interpretable approach that leverages prior knowledge of system dynamics to ensure constraint satisfaction. Our shield synthesis methodology, designed for real-world deployment, decomposes the environment into a hierarchical structure where each SCU explicitly manages a subset of constraints. We demonstrate the effectiveness of SCUs on a remote microgrid optimization task with strict operational requirements. The RL agent, equipped with SCUs, achieves a 24% reduction in fuel consumption without increasing battery degradation, outperforming other baselines while satisfying all constraints. We hope SCUs contribute to the safe application of RL to the many decision-making challenges linked to the energy transition.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2512.01577", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2512.01577", "abs": "https://arxiv.org/abs/2512.01577", "authors": ["Wenkai Li", "Zongwei Li", "Xiaoqi Li", "Chunyi Zhang", "Xiaoyan Zhang", "Yuqing Zhang"], "title": "Beyond the Hype: A Large-Scale Empirical Analysis of On-Chain Transactions in NFT Scams", "comment": "9 pages", "summary": "Non-fungible tokens (NFTs) serve as a representative form of digital asset ownership and have attracted numerous investors, creators, and tech enthusiasts in recent years. However, related fraud activities, especially phishing scams, have caused significant property losses. There are many graph analysis methods to detect malicious scam incidents, but no research on the transaction patterns of the NFT scams. Therefore, to fill this gap, we are the first to systematically explore NFT phishing frauds through graph analysis, aiming to comprehensively investigate the characteristics and patterns of NFT phishing frauds on the transaction graph. During the research process, we collect transaction records, log data, and security reports related to NFT phishing incidents published on multiple platforms. After collecting, sanitizing, and unifying the data, we construct a transaction graph and analyze the distribution, transaction features, and interaction patterns of NFT phishing scams. We find that normal transactions on the blockchain accounted for 96.71% of all transactions. Although phishing-related accounts accounted for only 0.94% of the total accounts, they appeared in 8.36% of the transaction scenarios, and their interaction probability with normal accounts is significantly higher in large-scale transaction networks. Moreover, NFT phishing scammers often carry out fraud in a collective manner, targeting specific accounts, tend to interact with victims through multiple token standards, have shorter transaction cycles than normal transactions, and involve more multi-party transactions. This study reveals the core behavioral features of NFT phishing scams, providing important references for the detection and prevention of NFT phishing scams in the future.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u56fe\u5206\u6790\u7cfb\u7edf\u6027\u5730\u63a2\u8ba8\u4e86NFT\u9493\u9c7c\u6b3a\u8bc8\u7684\u4ea4\u6613\u6a21\u5f0f\u548c\u7279\u5f81\uff0c\u4e3a\u672a\u6765NFT\u9493\u9c7c\u6b3a\u8bc8\u7684\u68c0\u6d4b\u4e0e\u9632\u8303\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u4ef7\u503c\u3002", "motivation": "NFT\u9493\u9c7c\u6b3a\u8bc8\u6d3b\u52a8\u9020\u6210\u4e86\u91cd\u5927\u8d22\u4ea7\u635f\u5931\uff0c\u5f53\u524d\u5c1a\u65e0\u7814\u7a76\u9488\u5bf9NFT\u9493\u9c7c\u6b3a\u8bc8\u7684\u4ea4\u6613\u6a21\u5f0f\u3002", "method": "\u6536\u96c6\u3001\u6e05\u6d17\u3001\u7edf\u4e00\u591a\u5e73\u53f0\u53d1\u5e03\u7684NFT\u9493\u9c7c\u4e8b\u4ef6\u7684\u4ea4\u6613\u8bb0\u5f55\u7b49\u6570\u636e\uff0c\u6784\u5efa\u4ea4\u6613\u56fe\u5206\u6790NFT\u9493\u9c7c\u6b3a\u8bc8\u7684\u5206\u5e03\u3001\u4ea4\u4e92\u7279\u5f81\u7b49\u3002", "result": "\u666e\u901a\u4ea4\u6613\u5360\u6bd496.71%\uff0c\u9493\u9c7c\u8d26\u6237\u5360\u6bd40.94%\u5374\u51fa\u73b0\u57288.36%\u7684\u4ea4\u6613\u573a\u666f\uff0cNFT\u9493\u9c7c\u6b3a\u8bc8\u6709\u96c6\u4f53\u6027\u3001\u7279\u5b9a\u76ee\u6807\u3001\u591a\u4ee3\u5e01\u6807\u51c6\u4ea4\u4e92\u7b49\u7279\u5f81\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86NFT\u9493\u9c7c\u6b3a\u8bc8\u7684\u6838\u5fc3\u884c\u4e3a\u7279\u5f81\u3002"}}
{"id": "2512.01047", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.01047", "abs": "https://arxiv.org/abs/2512.01047", "authors": ["Tanmay Ambadkar", "\u0110or\u0111e \u017dikeli\u0107", "Abhinav Verma"], "title": "Automating the Refinement of Reinforcement Learning Specifications", "comment": null, "summary": "Logical specifications have been shown to help reinforcement learning algorithms in achieving complex tasks. However, when a task is under-specified, agents might fail to learn useful policies. In this work, we explore the possibility of improving coarse-grained logical specifications via an exploration-guided strategy. We propose \\textsc{AutoSpec}, a framework that searches for a logical specification refinement whose satisfaction implies satisfaction of the original specification, but which provides additional guidance therefore making it easier for reinforcement learning algorithms to learn useful policies. \\textsc{AutoSpec} is applicable to reinforcement learning tasks specified via the SpectRL specification logic. We exploit the compositional nature of specifications written in SpectRL, and design four refinement procedures that modify the abstract graph of the specification by either refining its existing edge specifications or by introducing new edge specifications. We prove that all four procedures maintain specification soundness, i.e. any trajectory satisfying the refined specification also satisfies the original. We then show how \\textsc{AutoSpec} can be integrated with existing reinforcement learning algorithms for learning policies from logical specifications. Our experiments demonstrate that \\textsc{AutoSpec} yields promising improvements in terms of the complexity of control tasks that can be solved, when refined logical specifications produced by \\textsc{AutoSpec} are utilized.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2512.01078", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.01078", "abs": "https://arxiv.org/abs/2512.01078", "authors": ["Jiawei Ren", "Yan Zhuang", "Xiaokang Ye", "Lingjun Mao", "Xuhong He", "Jianzhi Shen", "Mrinaal Dogra", "Yiming Liang", "Ruixuan Zhang", "Tianai Yue", "Yiqing Yang", "Eric Liu", "Ryan Wu", "Kevin Benavente", "Rajiv Mandya Nagaraju", "Muhammad Faayez", "Xiyan Zhang", "Dhruv Vivek Sharma", "Xianrui Zhong", "Ziqiao Ma", "Tianmin Shu", "Zhiting Hu", "Lianhui Qin"], "title": "SimWorld: An Open-ended Realistic Simulator for Autonomous Agents in Physical and Social Worlds", "comment": null, "summary": "While LLM/VLM-powered AI agents have advanced rapidly in math, coding, and computer use, their applications in complex physical and social environments remain challenging. Building agents that can survive and thrive in the real world (for example, by autonomously earning income or running a business) requires massive-scale interaction, reasoning, training, and evaluation across diverse embodied scenarios. However, existing world simulators for such development fall short: they often rely on limited hand-crafted environments, simulate simplified game-like physics and social rules, and lack native support for LLM/VLM agents. We introduce SimWorld, a new simulator built on Unreal Engine 5, designed for developing and evaluating LLM/VLM agents in rich, real-world-like settings. SimWorld offers three core capabilities: (1) realistic, open-ended world simulation, including accurate physical and social dynamics and language-driven procedural environment generation; (2) a rich interface for LLM/VLM agents, with multimodal world inputs and open-vocabulary actions at varying levels of abstraction; and (3) diverse and extensible physical and social reasoning scenarios that are easily customizable by users. We demonstrate SimWorld by deploying frontier LLM agents (e.g., GPT-4o, Gemini-2.5-Flash, Claude-3.5, and DeepSeek-Prover-V2) on long-horizon multi-agent delivery tasks involving strategic cooperation and competition. The results reveal distinct reasoning patterns and limitations across models. We open-source SimWorld and hope it becomes a foundational platform for advancing real-world agent intelligence across disciplines: https://simworld.org.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2512.01595", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2512.01595", "abs": "https://arxiv.org/abs/2512.01595", "authors": ["Harish Yadav", "Vikas Maurya", "Abhilash Jindal", "Vireshwar Kumar"], "title": "WhiteLie: A Robust System for Spoofing User Data in Android Platforms", "comment": null, "summary": "Android employs a permission framework that empowers users to either accept or deny sharing their private data (for example, location) with an app. However, many apps tend to crash when they are denied permission, leaving users no choice but to allow access to their data in order to use the app. In this paper, we introduce a comprehensive and robust user data spoofing system, WhiteLie, that can spoof a variety of user data and feed it to target apps. Additionally, it detects privacy-violating behaviours, automatically responding by supplying spoofed data instead of the user's real data, without crashing or disrupting the apps. Unlike prior approaches, WhiteLie requires neither device rooting nor altering the app's binary, making it deployable on stock Android devices. Through experiments on more than 70 popular Android apps, we demonstrate that WhiteLie is able to deceive apps into accepting spoofed data without getting detected. Our evaluation further demonstrates that WhiteLie introduces negligible overhead in terms of battery usage, CPU consumption, and app execution latency. Our findings underscore the feasibility of implementing user-centric privacy-enhancing mechanisms within the existing Android ecosystem.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2512.01081", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.MA", "cs.NE", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2512.01081", "abs": "https://arxiv.org/abs/2512.01081", "authors": ["Stephen Fitz"], "title": "Testing the Machine Consciousness Hypothesis", "comment": null, "summary": "The Machine Consciousness Hypothesis states that consciousness is a substrate-free functional property of computational systems capable of second-order perception. I propose a research program to investigate this idea in silico by studying how collective self-models (coherent, self-referential representations) emerge from distributed learning systems embedded within universal self-organizing environments. The theory outlined here starts from the supposition that consciousness is an emergent property of collective intelligence systems undergoing synchronization of prediction through communication. It is not an epiphenomenon of individual modeling but a property of the language that a system evolves to internally describe itself. For a model of base reality, I begin with a minimal but general computational world: a cellular automaton, which exhibits both computational irreducibility and local reducibility. On top of this computational substrate, I introduce a network of local, predictive, representational (neural) models capable of communication and adaptation. I use this layered model to study how collective intelligence gives rise to self-representation as a direct consequence of inter-agent alignment. I suggest that consciousness does not emerge from modeling per se, but from communication. It arises from the noisy, lossy exchange of predictive messages between groups of local observers describing persistent patterns in the underlying computational substrate (base reality). It is through this representational dialogue that a shared model arises, aligning many partial views of the world. The broader goal is to develop empirically testable theories of machine consciousness, by studying how internal self-models may form in distributed systems without centralized control.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2512.01596", "categories": ["cs.CR", "cs.NI", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.01596", "abs": "https://arxiv.org/abs/2512.01596", "authors": ["Hamed Alimohammadi", "Samara Mayhoub", "Sotiris Chatzimiltis", "Mohammad Shojafar", "Muhammad Nasir Mumtaz Bhutta"], "title": "Towards a Multi-Layer Defence Framework for Securing Near-Real-Time Operations in Open RAN", "comment": "This is the authors preprint version. The manuscript has been submitted to IEEE", "summary": "Securing the near-real-time (near-RT) control operations in Open Radio Access Networks (Open RAN) is increasingly critical, yet remains insufficiently addressed, as new runtime threats target the control loop while the system is operational. In this paper, we propose a multi-layer defence framework designed to enhance the security of near-RT RAN Intelligent Controller (RIC) operations. We classify operational-time threats into three categories, message-level, data-level, and control logic-level, and design and implement a dedicated detection and mitigation component for each: a signature-based E2 message inspection module performing structural and semantic validation of signalling exchanges, a telemetry poisoning detector based on temporal anomaly scoring using an LSTM network, and a runtime xApp attestation mechanism based on execution-time hash challenge-response. The framework is evaluated on an O-RAN testbed comprising FlexRIC and a commercial RAN emulator, demonstrating effective detection rates, low latency overheads, and practical integration feasibility. Results indicate that the proposed safeguards can operate within near-RT time constraints while significantly improving protection against runtime attacks, introducing less than 80 ms overhead for a network with 500 User Equipment (UEs). Overall, this work lays the foundation for deployable, layered, and policy-driven runtime security architectures for the near-RT RIC control loop in Open RAN, and provides an extensible framework into which future mitigation policies and threat-specific modules can be integrated.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2512.01089", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.01089", "abs": "https://arxiv.org/abs/2512.01089", "authors": ["Peter Jansen", "Samiah Hassan", "Pragnya Narasimha"], "title": "CodeDistiller: Automatically Generating Code Libraries for Scientific Coding Agents", "comment": "8 pages, 3 figures, 2 tables", "summary": "Automated Scientific Discovery (ASD) systems can help automatically generate and run code-based experiments, but their capabilities are limited by the code they can reliably generate from parametric knowledge alone. As a result, current systems either mutate a small number of manually-crafted experiment examples, or operate solely from parametric knowledge, limiting quality and reach. We introduce CodeDistiller, a system that automatically distills large collections of scientific Github repositories into a vetted library of working domain-specific code examples, allowing ASD agents to expand their capabilities without manual effort. Using a combination of automatic and domain-expert evaluation on 250 materials science repositories, we find the best model is capable of producing functional examples for 74% of repositories, while our downstream evaluation shows an ASD agent augmented with a CodeDistiller generated library produces more accurate, complete, and scientifically sound experiments than an agent with only general materials-science code examples.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2512.01604", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2512.01604", "abs": "https://arxiv.org/abs/2512.01604", "authors": ["Shuai Feng", "Liang Feng Zhang"], "title": "On the Context-Hiding Property of Shamir-Based Homomorphic Secret Sharing", "comment": "2025 IEEE International Symposium on Information Theory", "summary": "Homomorphic secret sharing (HSS) allows multiple input clients to secretly share their private inputs to a function among several servers such that each server can homomorphically compute the function over its share to produce a share of the function's output. In HSS-enabled applications such as secure multi-party computation (MPC), security requires that the output shares leak no more information about the inputs than the function output. Such security is ensured by the context-hiding property of HSS. The typical rerandomization technique achieves context hiding but increases the share size. To address this, we formalize the context-hiding property of HSS for individual functions, examine the context-hiding property of Shamir-based HSS for monomials, and extend the study to polynomials.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2512.01099", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.01099", "abs": "https://arxiv.org/abs/2512.01099", "authors": ["Daria Smirnova", "Hamid Nasiri", "Marta Adamska", "Zhengxin Yu", "Peter Garraghan"], "title": "Energy-Aware Data-Driven Model Selection in LLM-Orchestrated AI Systems", "comment": null, "summary": "As modern artificial intelligence (AI) systems become more advanced and capable, they can leverage a wide range of tools and models to perform complex tasks. Today, the task of orchestrating these models is often performed by Large Language Models (LLMs) that rely on qualitative descriptions of models for decision-making. However, the descriptions provided to these LLM-based orchestrators do not reflect true model capabilities and performance characteristics, leading to suboptimal model selection, reduced accuracy, and increased energy costs. In this paper, we conduct an empirical analysis of LLM-based orchestration limitations and propose GUIDE, a new energy-aware model selection framework that accounts for performance-energy trade-offs by incorporating quantitative model performance characteristics in decision-making. Experimental results demonstrate that GUIDE increases accuracy by 0.90%-11.92% across various evaluated tasks, and achieves up to 54% energy efficiency improvement, while reducing orchestrator model selection latency from 4.51 s to 7.2 ms.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2512.01651", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2512.01651", "abs": "https://arxiv.org/abs/2512.01651", "authors": ["Antoine Leblanc", "Jacques Robin", "Nourh\u00e8ne Ben Rabah", "Zequan Huang", "B\u00e9n\u00e9dicte Le Grand"], "title": "Rethinking Cybersecurity Ontology Classification and Evaluation: Towards a Credibility-Centered Framework", "comment": null, "summary": "This paper analyzes the proliferation of cybersecurity ontologies, arguing that this surge cannot be explained solely by technical shortcomings related to quality, but also by a credibility deficit - a lack of trust, endorsement, and adoption by users. This conclusion is based on our first contribution, which is a state-of-the-art review and categorization of cybersecurity ontologies using the Framework for Ontologies Classification framework. To address this gap, we propose a revised framework for assessing credibility, introducing indicators such as institutional support, academic recognition, day-to-day practitioner validation, and industrial adoption. Based on these new credibility indicators, we construct a classification scheme designed to guide the selection of ontologies that are relevant to specific security needs. We then apply this framework to a concrete use case: the Franco-Luxembourgish research project ANCILE, which illustrates how a credibility-aware evaluation can reshape ontology selection for operational contexts.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2512.01107", "categories": ["cs.AI", "econ.EM", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.01107", "abs": "https://arxiv.org/abs/2512.01107", "authors": ["Sanjog Misra"], "title": "Foundation Priors", "comment": null, "summary": "Foundation models, and in particular large language models, can generate highly informative responses, prompting growing interest in using these ''synthetic'' outputs as data in empirical research and decision-making. This paper introduces the idea of a foundation prior, which shows that model-generated outputs are not as real observations, but draws from the foundation prior induced prior predictive distribution. As such synthetic data reflects both the model's learned patterns and the user's subjective priors, expectations, and biases. We model the subjectivity of the generative process by making explicit the dependence of synthetic outputs on the user's anticipated data distribution, the prompt-engineering process, and the trust placed in the foundation model.\n  We derive the foundation prior as an exponential-tilted, generalized Bayesian update of the user's primitive prior, where a trust parameter governs the weight assigned to synthetic data. We then show how synthetic data and the associated foundation prior can be incorporated into standard statistical and econometric workflows, and discuss their use in applications such as refining complex models, informing latent constructs, guiding experimental design, and augmenting random-coefficient and partially linear specifications. By treating generative outputs as structured, explicitly subjective priors rather than as empirical observations, the framework offers a principled way to harness foundation models in empirical work while avoiding the conflation of synthetic ''facts'' with real data.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2512.01666", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2512.01666", "abs": "https://arxiv.org/abs/2512.01666", "authors": ["Tianheng Qu", "Hongsong Zhu", "Limin Sun", "Haining Wang", "Haiqiang Fei", "Zheng He", "Zhi Li"], "title": "Demystifying Feature Engineering in Malware Analysis of API Call Sequences", "comment": null, "summary": "Machine learning (ML) has been widely used to analyze API call sequences in malware analysis, which typically requires the expertise of domain specialists to extract relevant features from raw data. The extracted features play a critical role in malware analysis. Traditional feature extraction is based on human domain knowledge, while there is a trend of using natural language processing (NLP) for automatic feature extraction. This raises a question: how do we effectively select features for malware analysis based on API call sequences? To answer it, this paper presents a comprehensive study of investigating the impact of feature engineering upon malware classification.We first conducted a comparative performance evaluation under three models, Convolutional Neural Network (CNN), Long Short-Term Memory (LSTM), and Transformer, with respect to knowledge-based and NLP-based feature engineering methods. We observed that models with knowledge-based feature engineering inputs generally outperform those using NLP-based across all metrics, especially under smaller sample sizes. Then we analyzed a complete set of data features from API call sequences, our analysis reveals that models often focus on features such as handles and virtual addresses, which vary across executions and are difficult for human analysts to interpret.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2512.01149", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.01149", "abs": "https://arxiv.org/abs/2512.01149", "authors": ["Krishna Taduri", "Shaunak Dhande", "Giacinto Paolo", "Saggese", "Paul Smith"], "title": "A Benchmark of Causal vs Correlation AI for Predictive Maintenance", "comment": null, "summary": "Predictive maintenance in manufacturing environments presents a challenging optimization problem characterized by extreme cost asymmetry, where missed failures incur costs roughly fifty times higher than false alarms. Conventional machine learning approaches typically optimize statistical accuracy metrics that do not reflect this operational reality and cannot reliably distinguish causal relationships from spurious correlations. This study evaluates eight predictive models, ranging from baseline statistical approaches to formal causal inference methods, on a dataset of 10,000 CNC machines with a 3.3% failure prevalence. The formal causal inference model (L5) achieved estimated annual cost savings of 1.16 million USD (a 70.2 percent reduction), outperforming the best correlation-based decision tree model (L3) by approximately 80,000 USD per year. The causal model matched the highest observed recall (87.9 percent) while reducing false alarms by 97 percent (from 165 to 5) and attained a precision of 92.1 percent, with a train-test performance gap of only 2.6 percentage points. These results indicate that causal AI methods, when combined with domain knowledge, can yield superior financial outcomes and more interpretable predictions compared to correlation-based approaches in predictive maintenance applications.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2512.01727", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2512.01727", "abs": "https://arxiv.org/abs/2512.01727", "authors": ["Benjamin Blakely", "Yeni Li", "Akshay Dave", "Derek Kultgen", "Rick Vilim"], "title": "AI-Driven Cybersecurity Testbed for Nuclear Infrastructure: Comprehensive Evaluation Using METL Operational Data", "comment": null, "summary": "Advanced nuclear reactor systems face increasing cybersecurity threats as sophisticated attackers exploit cyber-physical interfaces to manipulate control systems while evading traditional IT security measures. This research presents a comprehensive evaluation of artificial intelligence approaches for cybersecurity protection in nuclear infrastructure, using Argonne National Laboratory's Mechanisms Engineering Test Loop (METL) as an experimental platform. We developed a systematic evaluation framework encompassing four machine learning detection paradigms: Change Point Detection, LSTM-based Anomaly Detection, Dependency Violation analysis, and Autoencoder reconstruction methods. Our comprehensive attack taxonomy includes 15 distinct scenarios targeting reactor control systems, each implemented across five severity tiers to evaluate detection performance under varying attack intensities. The experimental evaluation encompassed 300 rigorous experiments using realistic METL operational data. Change Point Detection emerged as the leading approach with mean AUC performance of 0.785, followed by LSTM Anomaly Detection (0.636), Dependency Violation (0.621), and Autoencoder methods (0.580). Attack detectability varied significantly, with multi-site coordinated attacks proving most detectable (AUC = 0.739) while precision trust decay attacks presented the greatest detection challenge (AUC = 0.592). This work delivers practical performance benchmarks and reference architecture that advance AI-based cybersecurity capabilities for critical nuclear infrastructure, providing essential foundations for operational deployment and enhanced threat response in cyber-physical systems.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2512.01189", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.01189", "abs": "https://arxiv.org/abs/2512.01189", "authors": ["Chunzheng Zhu", "Jialin Shao", "Jianxin Lin", "Yijun Wang", "Jing Wang", "Jinhui Tang", "Kenli Li"], "title": "fMRI2GES: Co-speech Gesture Reconstruction from fMRI Signal with Dual Brain Decoding Alignment", "comment": "IEEE Transactions on Circuits and Systems for Video Technology (TCSVT) 2025", "summary": "Understanding how the brain responds to external stimuli and decoding this process has been a significant challenge in neuroscience. While previous studies typically concentrated on brain-to-image and brain-to-language reconstruction, our work strives to reconstruct gestures associated with speech stimuli perceived by brain. Unfortunately, the lack of paired \\{brain, speech, gesture\\} data hinders the deployment of deep learning models for this purpose. In this paper, we introduce a novel approach, \\textbf{fMRI2GES}, that allows training of fMRI-to-gesture reconstruction networks on unpaired data using \\textbf{Dual Brain Decoding Alignment}. This method relies on two key components: (i) observed texts that elicit brain responses, and (ii) textual descriptions associated with the gestures. Then, instead of training models in a completely supervised manner to find a mapping relationship among the three modalities, we harness an fMRI-to-text model, a text-to-gesture model with paired data and an fMRI-to-gesture model with unpaired data, establishing dual fMRI-to-gesture reconstruction patterns. Afterward, we explicitly align two outputs and train our model in a self-supervision way. We show that our proposed method can reconstruct expressive gestures directly from fMRI recordings. We also investigate fMRI signals from different ROIs in the cortex and how they affect generation results. Overall, we provide new insights into decoding co-speech gestures, thereby advancing our understanding of neuroscience and cognitive science.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2512.01832", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2512.01832", "abs": "https://arxiv.org/abs/2512.01832", "authors": ["Francesco Buccafurri", "Carmen Licciardi"], "title": "A Privacy-Preserving Information-Sharing Protocol for Federated Authentication", "comment": null, "summary": "This paper presents a privacy-preserving protocol for identity registration and information sharing in federated authentication systems. The goal is to enable Identity Providers (IdPs) to detect duplicate or fraudulent identity enrollments without revealing users personal data or enabling cross-domain correlation. The protocol relies on Oblivious Pseudorandom Functions (OPRFs) combined with domain-specific transformations, ensuring that each IdP generates independent pseudonymous identifiers derived from a shared cryptographic service while maintaining full input confidentiality. A central authority maintains a blind registry that records successful and failed identity verifications using only pseudonymous identifiers, allowing global consistency checks without exposing sensitive information or linking users across domains. The proposed construction provides a general and abstract framework suitable for a wide range of federated authentication systems, achieving strong privacy guarantees while supporting effective fraud-prevention mechanisms during identity registration.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u4fdd\u62a4\u9690\u79c1\u7684\u8054\u90a6\u8ba4\u8bc1\u8eab\u4efd\u6ce8\u518c\u548c\u4fe1\u606f\u5171\u4eab\u534f\u8bae\uff0c\u5229\u7528OPRFs\u548c\u7279\u5b9a\u57df\u8f6c\u6362\u5728\u4e0d\u6cc4\u9732\u7528\u6237\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u68c0\u6d4b\u6b3a\u8bc8\u3002", "motivation": "\u5728\u8054\u90a6\u8ba4\u8bc1\u7cfb\u7edf\u4e2d\uff0c\u9700\u68c0\u6d4b\u91cd\u590d\u6216\u6b3a\u8bc8\u6027\u8eab\u4efd\u6ce8\u518c\uff0c\u540c\u65f6\u4fdd\u62a4\u7528\u6237\u9690\u79c1\u548c\u9632\u6b62\u8de8\u57df\u5173\u8054\u3002", "method": "\u7ed3\u5408\u4f7f\u7528Oblivious Pseudorandom Functions (OPRFs)\u548c\u7279\u5b9a\u57df\u8f6c\u6362\uff0c\u751f\u6210\u72ec\u7acb\u7684\u4f2a\u533f\u540d\u6807\u8bc6\u7b26\uff0c\u786e\u4fdd\u8f93\u5165\u673a\u5bc6\u6027\u3002\u5f15\u5165\u4e00\u4e2a\u4e2d\u592e\u673a\u6784\uff0c\u7528\u4f2a\u533f\u540d\u6807\u8bc6\u7b26\u7ef4\u62a4\u76f2\u5bc4\u5b58\u5668\uff0c\u8bb0\u5f55\u6210\u529f\u548c\u5931\u8d25\u7684\u8eab\u4efd\u9a8c\u8bc1\uff0c\u5b9e\u73b0\u5168\u5c40\u4e00\u81f4\u6027\u68c0\u67e5\uff0c\u800c\u4e0d\u66b4\u9732\u654f\u611f\u4fe1\u606f\u6216\u8de8\u57df\u94fe\u63a5\u7528\u6237\u3002", "result": "\u8be5\u534f\u8bae\u63d0\u4f9b\u4e86\u4e00\u4e2a\u901a\u7528\u4e14\u62bd\u8c61\u7684\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u5404\u79cd\u8054\u90a6\u8ba4\u8bc1\u7cfb\u7edf\uff0c\u5b9e\u73b0\u4e86\u5f3a\u5927\u7684\u9690\u79c1\u4fdd\u8bc1\uff0c\u5e76\u6709\u6548\u9632\u6b62\u8eab\u4efd\u6ce8\u518c\u671f\u95f4\u7684\u6b3a\u8bc8\u884c\u4e3a\u3002", "conclusion": ""}}
{"id": "2512.01210", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.01210", "abs": "https://arxiv.org/abs/2512.01210", "authors": ["Ruiyu Wang", "Tuan Vinh", "Ran Xu", "Yuyin Zhou", "Jiaying Lu", "Carl Yang", "Francisco Pasquel"], "title": "Knowledge Graph Augmented Large Language Models for Next-Visit Disease Prediction", "comment": null, "summary": "Electronic health records (EHRs) support powerful clinical prediction models, but existing methods typically provide coarse, post hoc explanations that offer limited value for patient-level decision making. We introduce a knowledge graph (KG)-guided chain-of-thought (CoT) framework that generates clinically grounded and temporally consistent reasoning for visit-level disease prediction in MIMIC-III. ICD-9 codes are mapped to PrimeKG, from which disease-relevant nodes and multi-hop reasoning paths are extracted and used as scaffolds for CoT generation; only explanations whose conclusions match observed outcomes are retained. Lightweight LLaMA-3.1-Instruct-8B and Gemma-7B models are then fine-tuned on this supervision corpus. Across ten PrimeKG-mapped diseases and limited training cohorts (400 and 1000 cases), KG-guided models outperform strong classical baselines, achieving AUROC values of 0.66 to 0.70 and macro-AUPR values of 0.40 to 0.47. The models also transfer zero-shot to the CRADLE cohort, improving accuracy from approximately 0.40 to 0.51 up to 0.72 to 0.77. A blinded clinician evaluation shows consistent preference for KG-guided CoT explanations in clarity, relevance, and clinical correctness.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u5f15\u5bfc\u7684\u94fe\u5f0f\u601d\u7ef4\uff08KG-guided CoT\uff09\u6846\u67b6\uff0c\u7528\u4e8e\u5728MIMIC-III\u6570\u636e\u96c6\u4e2d\u8fdb\u884c\u75be\u75c5\u9884\u6d4b\u3002", "motivation": "\u89e3\u51b3\u5f53\u524d\u4e34\u5e8a\u9884\u6d4b\u6a21\u578b\u89e3\u91ca\u6027\u5dee\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u77e5\u8bc6\u56fe\u8c31\u63d0\u4f9b\u4e34\u5e8a\u76f8\u5173\u7684\u4e34\u65f6\u4e00\u81f4\u63a8\u7406\u8def\u5f84\uff0c\u4ece\u800c\u4e3a\u6bcf\u4e2a\u75c5\u60a3\u751f\u6210\u6709\u610f\u4e49\u7684\u89e3\u91ca\u3002", "method": "\u5c06ICD-9\u4ee3\u7801\u6620\u5c04\u5230PrimeKG\u77e5\u8bc6\u56fe\u8c31\uff0c\u4ece\u4e2d\u63d0\u53d6\u4e0e\u75be\u75c5\u76f8\u5173\u7684\u8282\u70b9\u548c\u591a\u8df3\u63a8\u7406\u8def\u5f84\uff0c\u5e76\u7528\u8fd9\u4e9b\u8def\u5f84\u4f5c\u4e3aCoT\u751f\u6210\u7684\u652f\u67b6\u3002\u4f7f\u7528\u8f7b\u91cf\u7ea7LLaMA-3.1-Instruct-8B\u548cGemma-7B\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\uff0c\u4ec5\u5728\u7ed3\u8bba\u7b26\u5408\u89c2\u6d4b\u7ed3\u679c\u65f6\u624d\u4fdd\u7559\u89e3\u91ca\u3002", "result": "\u5728\u5341\u4e2a\u75be\u75c5\u6620\u5c04\u548c\u6709\u9650\u8bad\u7ec3\u96c6\uff08400\u548c1000\u75c5\u4f8b\uff09\u4e0a\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u7ecf\u5178\u57fa\u51c6\u6a21\u578b\uff0cAUROC\u503c\u4e3a0.66\u81f30.70\uff0cmacro-AUPR\u4e3a0.40\u81f30.47\uff0c\u540c\u65f6\u5728CRADLE\u6570\u636e\u96c6\u4e0a\u5c55\u73b0\u826f\u597d\u96f6\u6837\u672c\u8fc1\u79fb\u6027\u80fd\u3002\u4e34\u5e8a\u533b\u5e08\u7684\u76f2\u6d4b\u663e\u793a\u5bf9KG-guided CoT\u89e3\u91ca\u5728\u6e05\u6670\u6027\u3001\u76f8\u5173\u6027\u548c\u4e34\u5e8a\u6b63\u786e\u6027\u4e0a\u7684\u4e00\u81f4\u504f\u597d\u3002", "conclusion": "KG-guided CoT\u6846\u67b6\u80fd\u591f\u6709\u6548\u751f\u6210\u4e34\u5e8a\u76f8\u5173\u548c\u4e34\u65f6\u4e00\u81f4\u7684\u63a8\u7406\uff0c\u63d0\u9ad8\u75be\u75c5\u9884\u6d4b\u6a21\u578b\u7684\u89e3\u91ca\u80fd\u529b\uff0c\u5e76\u5c55\u73b0\u51fa\u4e86\u826f\u597d\u7684\u8fc1\u79fb\u6027\u3002"}}
{"id": "2512.01845", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2512.01845", "abs": "https://arxiv.org/abs/2512.01845", "authors": ["Pericle Perazzo", "Massimiliano Mattei", "Giuseppe Anastasi", "Marco Avvenuti", "Gianluca Dini", "Giuseppe Lettieri", "Carlo Vallati"], "title": "JPEGs Just Got Snipped: Croppable Signatures Against Deepfake Images", "comment": null, "summary": "Deepfakes are a type of synthetic media created using artificial intelligence, specifically deep learning algorithms. This technology can for example superimpose faces and voices onto videos, creating hyper-realistic but artificial representations. Deepfakes pose significant risks regarding misinformation and fake news, because they can spread false information by depicting public figures saying or doing things they never did, undermining public trust. In this paper, we propose a method that leverages BLS signatures (Boneh, Lynn, and Shacham 2004) to implement signatures that remain valid after image cropping, but are invalidated in all the other types of manipulation, including deepfake creation. Our approach does not require who crops the image to know the signature private key or to be trusted in general, and it is O(1) in terms of signature size, making it a practical solution for scenarios where images are disseminated through web servers and cropping is the primary transformation. Finally, we adapted the signature scheme for the JPEG standard, and we experimentally tested the size of a signed image.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2512.01222", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.01222", "abs": "https://arxiv.org/abs/2512.01222", "authors": ["Ching Fang", "Samuel Marks"], "title": "Unsupervised decoding of encoded reasoning using language model interpretability", "comment": null, "summary": "As large language models become increasingly capable, there is growing concern that they may develop reasoning processes that are encoded or hidden from human oversight. To investigate whether current interpretability techniques can penetrate such encoded reasoning, we construct a controlled testbed by fine-tuning a reasoning model (DeepSeek-R1-Distill-Llama-70B) to perform chain-of-thought reasoning in ROT-13 encryption while maintaining intelligible English outputs. We evaluate mechanistic interpretability methods--in particular, logit lens analysis--on their ability to decode the model's hidden reasoning process using only internal activations. We show that logit lens can effectively translate encoded reasoning, with accuracy peaking in intermediate-to-late layers. Finally, we develop a fully unsupervised decoding pipeline that combines logit lens with automated paraphrasing, achieving substantial accuracy in reconstructing complete reasoning transcripts from internal model representations. These findings suggest that current mechanistic interpretability techniques may be more robust to simple forms of encoded reasoning than previously understood. Our work provides an initial framework for evaluating interpretability methods against models that reason in non-human-readable formats, contributing to the broader challenge of maintaining oversight over increasingly capable AI systems.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2512.01891", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2512.01891", "abs": "https://arxiv.org/abs/2512.01891", "authors": ["Giada Stivala", "Rafael Mrowczynski", "Maria Hellenthal", "Giancarlo Pellegrino"], "title": "Behind the Curtain: How Shared Hosting Providers Respond to Vulnerability Notifications", "comment": "Accepted at IEEE S&P 2026", "summary": "Large-scale vulnerability notifications (VNs) can help hosting provider organizations (HPOs) identify and remediate security vulnerabilities that attackers can exploit in data breaches or phishing campaigns. Previous VN studies have primarily focused on factors under the control of reporters, such as sender reputation, email formatting, and communication channels. Despite these efforts, remediation rates for vulnerability notifications continue to remain consistently low. This paper presents the first in-depth study of how HPOs process vulnerability notifications internally and what organizational and operational factors influence VN effectiveness. We examine the problem from a different perspective to provide the first detailed understanding of the reasons behind persistently low remediation rates. Instead of manipulating parameters of VN campaigns, we interview hosting providers directly, investigating how they handle vulnerability notifications and what factors may influence VN effectiveness, such as VN awareness and reachability, HPOs' service models, and perceived security risks.\n  We conducted semi-structured interviews with 24 HPOs across shared hosting and web development services, representing varied company sizes and operator roles. Our findings reveal practical insights on VN processing and abuse workflows. While some providers remain hard to reach due to complex infrastructures, most report routinely handling VNs. However, limited remediation often stems from strict responsibility boundaries, where web application issues are seen as the customer's domain. Low hosting fees and high volumes of daily compromises further discourage both proactive and reactive measures. Our findings show that HPOs blame negligent website owners, and prior works on website owners confirms they often undervalue their sites or lack security know-how.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2512.01289", "categories": ["cs.AI", "cs.GR"], "pdf": "https://arxiv.org/pdf/2512.01289", "abs": "https://arxiv.org/abs/2512.01289", "authors": ["Mingqin Yu", "Fethi Rabhi", "Boming Xia", "Zhengyi Yang", "Felix Tan", "Qinghua Lu"], "title": "OntoMetric: An Ontology-Guided Framework for Automated ESG Knowledge Graph Construction", "comment": null, "summary": "Environmental, Social, and Governance (ESG) disclosure frameworks such as SASB, TCFD, and IFRS S2 require organizations to compute and report numerous metrics for compliance, yet these requirements are embedded in long, unstructured PDF documents that are difficult to interpret, standardize, and audit. Manual extraction is unscalable, while unconstrained large language model (LLM) extraction often produces inconsistent entities, hallucinated relationships, missing provenance, and high validation failure rates. We present OntoMetric, an ontology-guided framework that transforms ESG regulatory documents into validated, AI- and web-ready knowledge graphs. OntoMetric operates through a three-stage pipeline: (1) structure-aware segmentation using table-of-contents boundaries, (2) ontology-constrained LLM extraction that embeds the ESGMKG schema into prompts while enriching entities with semantic fields for downstream reasoning, and (3) two-phase validation that combines LLM-based semantic verification with rule-based schema checking across entity, property, and relationship levels (VR001-VR006). The framework preserves both segment-level and page-level provenance for audit traceability. Evaluated on five ESG standards (SASB Commercial Banks, SASB Semiconductors, TCFD, IFRS S2, AASB S2) totaling 228 pages and 60 segments, OntoMetric achieves 65-90% semantic accuracy and 80-90% schema compliance, compared to 3-10% for baseline unconstrained extraction, at approximately 0.01 to 0.02 USD per validated entity. Our results demonstrate that combining symbolic ontology constraints with neural extraction enables reliable, auditable knowledge graphs suitable for regulatory compliance and web integration, supporting downstream applications such as sustainable-finance analytics, transparency portals, and automated compliance tools.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2512.01893", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2512.01893", "abs": "https://arxiv.org/abs/2512.01893", "authors": ["Francesco Greco", "Giuseppe Desolda", "Cesare Tucci", "Andrea Esposito", "Antonio Curci", "Antonio Piccinno"], "title": "Improving Phishing Resilience with AI-Generated Training: Evidence on Prompting, Personalization, and Duration", "comment": "Data and code available at: https://doi.org/10.6084/m9.figshare.30664793", "summary": "Phishing remains a persistent cybersecurity threat; however, developing scalable and effective user training is labor-intensive and challenging to maintain. Generative Artificial Intelligence offers an interesting opportunity, but empirical evidence on its instructional efficacy remains scarce. This paper provides an experimental validation of Large Language Models (LLMs) as autonomous engines for generating phishing resilience training. Across two controlled studies (N=480), we demonstrate that AI-generated content yields significant pre-post learning gains regardless of the specific prompting strategy employed. Study 1 (N=80) compares four prompting techniques, finding that even a straightforward \"direct-profile\" strategy--simply embedding user traits into the prompt--produces effective training material. Study 2 (N=400) investigates the scalability of this approach by testing personalization and training duration. Results show that complex psychometric personalization offers no measurable advantage over well-designed generic content, while longer training duration provides a modest boost in accuracy. These findings suggest that organizations can leverage LLMs to generate high-quality, effective training at scale without the need for complex user profiling, relying instead on the inherent capabilities of the model.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2512.01300", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.01300", "abs": "https://arxiv.org/abs/2512.01300", "authors": ["Dacheng Liao", "Mengshi Qi", "Peng Shu", "Zhining Zhang", "Yuxin Lin", "Liang Liu", "Huadong Ma"], "title": "RoboDriveVLM: A Novel Benchmark and Baseline towards Robust Vision-Language Models for Autonomous Driving", "comment": null, "summary": "Current Vision-Language Model (VLM)-based end-to-end autonomous driving systems often leverage large language models to generate driving decisions directly based on their understanding of the current scene. However, such systems introduce multiple risks in real-world driving scenarios. To evaluate whether VLMs are truly viable for autonomous driving, we introduce RoboDriveBench, the first robustness benchmark focused on end-to-end trajectory prediction tasks. This benchmark systematically evaluates two critical categories of real-world challenges for VLM-based end-to-end autonomous driving systems through 11 simulated scenarios encompassing various corruption types, including 6 scenarios of sensor corruption caused by environmental variations, along with 5 cases of prompt corruption resulting from human intervention and data transmission failures. Each corruption type includes 250 unique driving scenarios and 5,689 frames, resulting in 64,559 total trajectory prediction cases per evaluation. To overcome these real-world challenges, we propose a novel VLM-based autonomous driving framework called RoboDriveVLM, which enhances robustness by mapping more multimodal data-e.g., lidar and radar-into a unified latent space. Furthermore, we introduce a new Test-Time Adaptation (TTA) method based on cross-modal knowledge distillation to improve the robustness of VLM-based autonomous driving systems. Through extensive experiments, our work highlights the limitations of current VLM-based end-to-end autonomous driving systems and provides a more reliable solution for real-world deployment. Source code and datasets will be released.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2512.01311", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.01311", "abs": "https://arxiv.org/abs/2512.01311", "authors": ["Shinji Mai", "Yunpeng Zhai", "Ziqian Chen", "Cheng Chen", "Anni Zou", "Shuchang Tao", "Zhaoyang Liu", "Bolin Ding"], "title": "CuES: A Curiosity-driven and Environment-grounded Synthesis Framework for Agentic RL", "comment": null, "summary": "Large language model based agents are increasingly deployed in complex, tool augmented environments. While reinforcement learning provides a principled mechanism for such agents to improve through interaction, its effectiveness critically depends on the availability of structured training tasks. In many realistic settings, however, no such tasks exist a challenge we term task scarcity, which has become a key bottleneck for scaling agentic RL. Existing approaches typically assume predefined task collections, an assumption that fails in novel environments where tool semantics and affordances are initially unknown. To address this limitation, we formalize the problem of Task Generation for Agentic RL, where an agent must learn within a given environment that lacks predefined tasks. We propose CuES, a Curiosity driven and Environment grounded Synthesis framework that autonomously generates diverse, executable, and meaningful tasks directly from the environment structure and affordances, without relying on handcrafted seeds or external corpora. CuES drives exploration through intrinsic curiosity, abstracts interaction patterns into reusable task schemas, and refines them through lightweight top down guidance and memory based quality control. Across three representative environments, AppWorld, BFCL, and WebShop, CuES produces task distributions that match or surpass manually curated datasets in both diversity and executability, yielding substantial downstream policy improvements. These results demonstrate that curiosity driven, environment grounded task generation provides a scalable foundation for agents that not only learn how to act, but also learn what to learn. The code is available at https://github.com/modelscope/AgentEvolver/research/CuES.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2512.01321", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.01321", "abs": "https://arxiv.org/abs/2512.01321", "authors": ["Juan Hernandez", "Diego Fern\u00e1ndez", "Manuel Cifuentes", "Denis Parra", "Rodrigo Toro Icarte"], "title": "Extending NGU to Multi-Agent RL: A Preliminary Study", "comment": "9 pages, 4 figures, 1 table. Accepted at the LatinX in AI (LXAI) Workshop at NeurIPS 2025. Includes experimental results for Multi-NGU and Multi-DQN in the PettingZoo simple_tag environment", "summary": "The Never Give Up (NGU) algorithm has proven effective in reinforcement learning tasks with sparse rewards by combining episodic novelty and intrinsic motivation. In this work, we extend NGU to multi-agent environments and evaluate its performance in the simple_tag environment from the PettingZoo suite. Compared to a multi-agent DQN baseline, NGU achieves moderately higher returns and more stable learning dynamics. We investigate three design choices: (1) shared replay buffer versus individual replay buffers, (2) sharing episodic novelty among agents using different k thresholds, and (3) using heterogeneous values of the beta parameter. Our results show that NGU with a shared replay buffer yields the best performance and stability, highlighting that the gains come from combining NGU intrinsic exploration with experience sharing. Novelty sharing performs comparably when k = 1 but degrades learning for larger values. Finally, heterogeneous beta values do not improve over a small common value. These findings suggest that NGU can be effectively applied in multi-agent settings when experiences are shared and intrinsic exploration signals are carefully tuned.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2512.01331", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.01331", "abs": "https://arxiv.org/abs/2512.01331", "authors": ["Saman Ahmadi", "Mahdi Jalili"], "title": "A Fast Heuristic Search Approach for Energy-Optimal Profile Routing for Electric Vehicles", "comment": "13 pages, 5 Figures, 1 table, To appear as part of AAAI 2026 Proceedings", "summary": "We study the energy-optimal shortest path problem for electric vehicles (EVs) in large-scale road networks, where recuperated energy along downhill segments introduces negative energy costs. While traditional point-to-point pathfinding algorithms for EVs assume a known initial energy level, many real-world scenarios involving uncertainty in available energy require planning optimal paths for all possible initial energy levels, a task known as energy-optimal profile search. Existing solutions typically rely on specialized profile-merging procedures within a label-correcting framework that results in searching over complex profiles. In this paper, we propose a simple yet effective label-setting approach based on multi-objective A* search, which employs a novel profile dominance rule to avoid generating and handling complex profiles. We develop four variants of our method and evaluate them on real-world road networks enriched with realistic energy consumption data. Experimental results demonstrate that our energy profile A* search achieves performance comparable to energy-optimal A* with a known initial energy level.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2512.01351", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.01351", "abs": "https://arxiv.org/abs/2512.01351", "authors": ["Elinor Poole-Dayan", "Jiayi Wu", "Taylor Sorensen", "Jiaxin Pei", "Michiel A. Bakker"], "title": "Benchmarking Overton Pluralism in LLMs", "comment": null, "summary": "We introduce a novel framework for measuring Overton pluralism in LLMs--the extent to which diverse viewpoints are represented in model outputs. We (i) formalize Overton pluralism as a set coverage metric (OvertonScore), (ii) conduct a large-scale U.S.-representative human study (N = 1209; 60 questions; 8 LLMs), and (iii) develop an automated benchmark that closely reproduces human judgments. On average, models achieve OvertonScores of 0.35--0.41, with DeepSeek V3 performing best; yet all models remain far below the theoretical maximum of 1.0, revealing substantial headroom for improvement. Because repeated large-scale human studies are costly and slow, scalable evaluation tools are essential for model development. Hence, we propose an automated benchmark that achieves high rank correlation with human judgments ($\u03c1=0.88$), providing a practical proxy without replacing human assessment. By turning pluralistic alignment from a normative aim into a measurable benchmark, our work establishes a foundation for systematic progress toward more pluralistic LLMs.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2512.01354", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.LG", "q-fin.TR"], "pdf": "https://arxiv.org/pdf/2512.01354", "abs": "https://arxiv.org/abs/2512.01354", "authors": ["Zhongjie Jiang"], "title": "The Necessity of Imperfection:Reversing Model Collapse via Simulating Cognitive Boundedness", "comment": "38 pages,5 figures,30 tables. This paper proposes the Prompt-driven Cognitive Computing Framework (PMCSF) and validates it with A-share market stress tests (N=23 for 2015 crash, N=13 for 2024 bull market). Includes detailed appendices on cognitive vector definitions, perturbation operators, and financial backtest data", "summary": "Although synthetic data is widely promoted as a remedy, its prevailing production paradigm -- one optimizing for statistical smoothness -- systematically removes the long-tail, cognitively grounded irregularities that characterize human text. Prolonged training on such statistically optimal but cognitively impoverished data accelerates model collapse.\n  This paper proposes a paradigm shift: instead of imitating the surface properties of data, we simulate the cognitive processes that generate human text. We introduce the Prompt-driven Cognitive Computing Framework (PMCSF), whose core consists of a Cognitive State Decoder (CSD) that reverse-engineers unstructured text into structured cognitive vectors, and a Cognitive Text Encoder (CTE) that re-materializes these states into text enriched with human-typical imperfections via mathematically defined Cognitive Perturbation Operators.\n  The framework is validated through a two-stage objective evaluation pipeline. First, in cognitive codec verification, CTE text yields a Jensen-Shannon divergence of 0.0614 from human text (vs. 0.4431 for standard LLM output), passes double-blind professional media review, and achieves an intraclass correlation coefficient ICC > 0.9 for cognitive profile alignment across heterogeneous models. Second, in functional gain evaluation, isomorphic stress tests in the A-share market show that strategies incorporating CTE-generated data reduce maximum drawdown by 47.4% during the 2015 crash and deliver 8.6% Defensive Alpha, exceeding transaction costs by a factor of 33.\n  Our findings demonstrate that modelling human cognitive limitations -- not copying surface data -- enables synthetic data with genuine functional gain, offering a viable technical pathway toward resolving the AI data-collapse crisis.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2512.01434", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.01434", "abs": "https://arxiv.org/abs/2512.01434", "authors": ["Daull Xavier", "Patrice Bellot", "Emmanuel Bruno", "Vincent Martin", "Elisabeth Murisasco"], "title": "A Flexible Multi-Agent LLM-Human Framework for Fast Human Validated Tool Building", "comment": null, "summary": "We introduce CollabToolBuilder, a flexible multiagent LLM framework with expert-in-the-loop (HITL) guidance that iteratively learns to create tools for a target goal, aligning with human intent and process, while minimizing time for task/domain adaptation effort and human feedback capture. The architecture generates and validates tools via four specialized agents (Coach, Coder, Critic, Capitalizer) using a reinforced dynamic prompt and systematic human feedback integration to reinforce each agent's role toward goals and constraints. This work is best viewed as a system-level integration and methodology combining multi-agent in-context learning, HITL controls, and reusable tool capitalization for complex iterative problems such as scientific document generation. We illustrate it with preliminary experiments (e.g., generating state-of-the-art research papers or patents given an abstract) and discuss its applicability to other iterative problem-solving.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2512.01440", "categories": ["cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.01440", "abs": "https://arxiv.org/abs/2512.01440", "authors": ["Sylvain Mari\u00e9", "Pablo Knecht"], "title": "A Selective Temporal Hamming distance to find patterns in state transition event timeseries, at scale", "comment": null, "summary": "Discrete event systems are present both in observations of nature, socio economical sciences, and industrial systems. Standard analysis approaches do not usually exploit their dual event / state nature: signals are either modeled as transition event sequences, emphasizing event order alignment, or as categorical or ordinal state timeseries, usually resampled a distorting and costly operation as the observation period and number of events grow. In this work we define state transition event timeseries (STE-ts) and propose a new Selective Temporal Hamming distance (STH) leveraging both transition time and duration-in-state, avoiding costly and distorting resampling on large databases. STH generalizes both resampled Hamming and Jaccard metrics with better precision and computation time, and an ability to focus on multiple states of interest. We validate these benefits on simulated and real-world datasets.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2512.01452", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.01452", "abs": "https://arxiv.org/abs/2512.01452", "authors": ["Lingbo Li", "Anuradha Mathrani", "Teo Susnjak"], "title": "Automated Risk-of-Bias Assessment of Randomized Controlled Trials: A First Look at a GEPA-trained Programmatic Prompting Framework", "comment": null, "summary": "Assessing risk of bias (RoB) in randomized controlled trials is essential for trustworthy evidence synthesis, but the process is resource-intensive and prone to variability across reviewers. Large language models (LLMs) offer a route to automation, but existing methods rely on manually engineered prompts that are difficult to reproduce, generalize, or evaluate. This study introduces a programmable RoB assessment pipeline that replaces ad-hoc prompt design with structured, code-based optimization using DSPy and its GEPA module. GEPA refines LLM reasoning through Pareto-guided search and produces inspectable execution traces, enabling transparent replication of every step in the optimization process. We evaluated the method on 100 RCTs from published meta-analyses across seven RoB domains. GEPA-generated prompts were applied to both open-weight models (Mistral Small 3.1 with GPT-oss-20b) and commercial models (GPT-5 Nano and GPT-5 Mini). In domains with clearer methodological reporting, such as Random Sequence Generation, GEPA-generated prompts performed best, with similar results for Allocation Concealment and Blinding of Participants, while the commercial model performed slightly better overall. We also compared GEPA with three manually designed prompts using Claude 3.5 Sonnet. GEPA achieved the highest overall accuracy and improved performance by 30%-40% in Random Sequence Generation and Selective Reporting, and showed generally comparable, competitively aligned performance in the other domains relative to manual prompts. These findings suggest that GEPA can produce consistent and reproducible prompts for RoB assessment, supporting the structured and principled use of LLMs in evidence synthesis.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2512.01507", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.01507", "abs": "https://arxiv.org/abs/2512.01507", "authors": ["Daniel Armstrong", "Zlatko Jon\u010dev", "Andres M Bran", "Philippe Schwaller"], "title": "SynthStrategy: Extracting and Formalizing Latent Strategic Insights from LLMs in Organic Chemistry", "comment": null, "summary": "Modern computer-assisted synthesis planning (CASP) systems show promises at generating chemically valid reaction steps but struggle to incorporate strategic considerations such as convergent assembly, protecting group minimization, and optimal ring-forming sequences. We introduce a methodology that leverages Large Language Models to distill synthetic knowledge into code. Our system analyzes synthesis routes and translates strategic principles into Python functions representing diverse strategic and tactical rules, such as strategic functional group interconversions and ring construction strategies. By formalizing this knowledge as verifiable code rather than simple heuristics, we create testable, interpretable representations of synthetic strategy. We release the complete codebase and the USPTO-ST dataset -- synthesis routes annotated with strategic tags. This framework unlocks a novel capability for CASP: natural language-based route retrieval, achieving 75\\% Top-3 accuracy on our benchmark. We further validate our library through temporal analysis of historical trends and chemically intuitive route clustering that offers more granular partitioning than common previous methods. This work bridges the tactical-strategic divide in CASP, enabling specification, search, and evaluation of routes by strategic criteria rather than structure alone.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2512.01556", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.01556", "abs": "https://arxiv.org/abs/2512.01556", "authors": ["Zhiyuan Wang", "Aniri", "Tianlong Chen", "Yue Zhang", "Heng Tao Shen", "Xiaoshuang Shi", "Kaidi Xu"], "title": "LEC: Linear Expectation Constraints for False-Discovery Control in Selective Prediction and Routing Systems", "comment": null, "summary": "Large language models (LLMs) often generate unreliable answers, while heuristic uncertainty methods fail to fully distinguish correct from incorrect predictions, causing users to accept erroneous answers without statistical guarantees. We address this issue through the lens of false discovery rate (FDR) control, ensuring that among all accepted predictions, the proportion of errors does not exceed a target risk level. To achieve this in a principled way, we propose LEC, which reinterprets selective prediction as a constrained decision problem by enforcing a Linear Expectation Constraint over selection and error indicators. Then, we establish a finite-sample sufficient condition, which relies only on a held-out set of exchangeable calibration samples, to compute an FDR-constrained, coverage-maximizing threshold. Furthermore, we extend LEC to a two-model routing mechanism: given a prompt, if the current model's uncertainty exceeds its calibrated threshold, we delegate it to a stronger model, while maintaining a unified FDR guarantee. Evaluations on closed-ended and open-ended question-answering (QA) datasets show that LEC achieves tighter FDR control and substantially improves sample retention over prior methods. Moreover, the two-model routing mechanism achieves lower risk levels while accepting more correct samples than each individual model.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2512.01616", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.01616", "abs": "https://arxiv.org/abs/2512.01616", "authors": ["Chainesh Gautam", "Raghuram Bharadwaj Diddigi"], "title": "CLIP-RL: Aligning Language and Policy Representations for Task Transfer in Reinforcement Learning", "comment": "4 pages, 2 figures, accepted as a extended abstract at RLDM 2025", "summary": "Recently, there has been an increasing need to develop agents capable of solving multiple tasks within the same environment, especially when these tasks are naturally associated with language. In this work, we propose a novel approach that leverages combinations of pre-trained (language, policy) pairs to establish an efficient transfer pipeline. Our algorithm is inspired by the principles of Contrastive Language-Image Pretraining (CLIP) in Computer Vision, which aligns representations across different modalities under the philosophy that ''two modalities representing the same concept should have similar representations.'' The central idea here is that the instruction and corresponding policy of a task represent the same concept, the task itself, in two different modalities. Therefore, by extending the idea of CLIP to RL, our method creates a unified representation space for natural language and policy embeddings. Experimental results demonstrate the utility of our algorithm in achieving faster transfer across tasks.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2512.01723", "categories": ["cs.AI", "cs.GT", "math.PR"], "pdf": "https://arxiv.org/pdf/2512.01723", "abs": "https://arxiv.org/abs/2512.01723", "authors": ["Saba Kublashvili"], "title": "Probabilistic Neuro-Symbolic Reasoning for Sparse Historical Data: A Framework Integrating Bayesian Inference, Causal Models, and Game-Theoretic Allocation", "comment": "Preprint. Code and simulation notebooks available at the GitHub repository: https://github.com/Saba-Kublashvili/bayesian-computational-modeling.-", "summary": "Modeling historical events poses fundamental challenges for machine learning: extreme data scarcity (N << 100), heterogeneous and noisy measurements, missing counterfactuals, and the requirement for human interpretable explanations. We present HistoricalML, a probabilistic neuro-symbolic framework that addresses these challenges through principled integration of (1) Bayesian uncertainty quantification to separate epistemic from aleatoric uncertainty, (2) structural causal models for counterfactual reasoning under confounding, (3) cooperative game theory (Shapley values) for fair allocation modeling, and (4) attention based neural architectures for context dependent factor weighting. We provide theoretical analysis showing that our approach achieves consistent estimation in the sparse data regime when strong priors from domain knowledge are available, and that Shapley based allocation satisfies axiomatic fairness guarantees that pure regression approaches cannot provide. We instantiate the framework on two historical case studies: the 19th century partition of Africa (N = 7 colonial powers) and the Second Punic War (N = 2 factions). Our model identifies Germany's +107.9 percent discrepancy as a quantifiable structural tension preceding World War I, with tension factor 36.43 and 0.79 naval arms race correlation. For the Punic Wars, Monte Carlo battle simulations achieve a 57.3 percent win probability for Carthage at Cannae and 57.8 percent for Rome at Zama, aligning with historical outcomes. Counterfactual analysis reveals that Carthaginian political support (support score 6.4 vs Napoleon's 7.1), rather than military capability, was the decisive factor.", "AI": {"tldr": "\u63d0\u51fa\u4e86HistoricalML\uff0c\u4e00\u4e2a\u6982\u7387\u795e\u7ecf\u7b26\u53f7\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u8d1d\u53f6\u65af\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u3001\u7ed3\u6784\u56e0\u679c\u6a21\u578b\u3001\u5408\u4f5c\u535a\u5f08\u8bba\u548c\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u795e\u7ecf\u67b6\u6784\uff0c\u6765\u89e3\u51b3\u5386\u53f2\u4e8b\u4ef6\u7684\u5efa\u6a21\u6311\u6218\u3002", "motivation": "\u5386\u53f2\u4e8b\u4ef6\u5efa\u6a21\u9762\u4e34\u6570\u636e\u7a00\u7f3a\u3001\u566a\u58f0\u3001\u6df7\u6742\u56e0\u7d20\u548c\u89e3\u91ca\u6027\u8981\u6c42\u7b49\u591a\u91cd\u6311\u6218\u3002", "method": "\u6574\u5408\u4e861) \u8d1d\u53f6\u65af\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\uff1b2) \u7ed3\u6784\u56e0\u679c\u6a21\u578b\uff1b3) \u5408\u4f5c\u535a\u5f08\u8bba\uff08Shapley\u503c\uff09\uff1b4) \u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u795e\u7ecf\u67b6\u6784\u3002", "result": "\u5728\u4e24\u4e2a\u5386\u53f2\u6848\u4f8b\u7814\u7a76\u4e2d\uff0c\u6a21\u578b\u6210\u529f\u8bc6\u522b\u4e86\u5fb7\u56fd\u5728\u975e\u6d32\u5206\u5272\u4e2d\u7684\u7ed3\u6784\u6027\u7d27\u5f20\u5173\u7cfb\uff0c\u5e76\u5728\u7b2c\u4e8c\u6b21\u5e03\u533f\u6218\u4e89\u4e2d\u901a\u8fc7\u8499\u7279\u5361\u6d1b\u6218\u6597\u6a21\u62df\u4e0e\u5386\u53f2\u7ed3\u679c\u4fdd\u6301\u4e00\u81f4\u3002", "conclusion": "HistoricalML\u6846\u67b6\u5728\u6570\u636e\u7a00\u758f\u548c\u89e3\u91ca\u6027\u8981\u6c42\u9ad8\u7684\u5386\u53f2\u4e8b\u4ef6\u5efa\u6a21\u4e2d\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002"}}
{"id": "2512.01786", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.01786", "abs": "https://arxiv.org/abs/2512.01786", "authors": ["Xiaochuan Li", "Ke Wang", "Girija Gouda", "Shubham Choudhary", "Yaqun Wang", "Linwei Hu", "Joel Vaughan", "Freddy Lecue"], "title": "Who Judges the Judge? LLM Jury-on-Demand: Building Trustworthy LLM Evaluation Systems", "comment": "66 pages, 22 figures, 37 tables", "summary": "As Large Language Models (LLMs) become integrated into high-stakes domains, there is a growing need for evaluation methods that are both scalable for real-time deployment and reliable for critical decision-making. While human evaluation is reliable, it is slow and costly. Single LLM judges are biased, and static juries lack adaptability. To overcome these limitations, we propose LLM Jury-on-Demand - a dynamic, learning-based framework for scalable and context-aware evaluation. Our method trains a set of reliability predictors to assess when LLM judges will agree with human experts, leveraging token distributions, embeddings, and structural input features. This enables a fully adaptive evaluation where, for each data point, an optimal jury of the most reliable judges is dynamically selected, and their scores are aggregated using their reliability as weights. Experiments on summarization and RAG benchmarks show that our dynamic jury system achieves significantly higher correlation with human judgment than both single-judge and static-jury baselines. These results highlight the promise of adaptive, learning-based juries for building scalable, more reliable and trustworthy evaluation systems for modern LLMs in high-stakes domains.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86LLM Jury-on-Demand\uff0c\u8fd9\u662f\u4e00\u4e2a\u52a8\u6001\u7684\u3001\u57fa\u4e8e\u5b66\u4e60\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u5728\u9ad8\u98ce\u9669\u9886\u57df\u96c6\u6210\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u65f6\uff0c\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u5b58\u5728\u7684\u53ef\u6269\u5c55\u6027\u548c\u9002\u5e94\u6027\u7684\u95ee\u9898\u3002", "motivation": "\u968f\u7740LLM\u88ab\u5e94\u7528\u5230\u9ad8\u98ce\u9669\u9886\u57df\uff0c\u9700\u8981\u53ef\u8fdb\u884c\u5b9e\u65f6\u90e8\u7f72\u5e76\u4e14\u53ef\u9760\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002\u4eba\u5de5\u8bc4\u4f30\u53ef\u9760\u4f46\u901f\u5ea6\u6162\u4e14\u82b1\u8d39\u9ad8\u3002\u5355\u4e00\u7684LLM\u8bc4\u5224\u504f\u9887\uff0c\u800c\u9759\u6001\u8bc4\u5ba1\u56e2\u7f3a\u4e4f\u9002\u5e94\u6027\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u7684\u6846\u67b6\u6765\u514b\u670d\u8fd9\u4e9b\u9650\u5236\u3002", "method": "LLM Jury-on-Demand\u6846\u67b6\u901a\u8fc7\u8bad\u7ec3\u591a\u4e2a\u53ef\u9760\u6027\u9884\u6d4b\u5668\u6765\u8bc4\u4f30LLM\u8bc4\u5224\u4e0e\u4eba\u7c7b\u4e13\u5bb6\u8bc4\u5ba1\u4e4b\u95f4\u7684\u4e00\u81f4\u6027\u3002\u5b83\u5229\u7528\u6807\u8bb0\u5206\u5e03\u3001\u5d4c\u5165\u548c\u7ed3\u6784\u8f93\u5165\u7279\u5f81\u9009\u62e9\u6bcf\u4e2a\u6570\u636e\u70b9\u7684\u6700\u53ef\u9760\u8bc4\u5224\u5458\uff0c\u7ec4\u6210\u4e00\u4e2a\u6700\u4f18\u8bc4\u5ba1\u56e2\uff0c\u5e76\u4f7f\u7528\u5176\u53ef\u9760\u6027\u4f5c\u4e3a\u6743\u91cd\u6765\u6574\u5408\u8bc4\u5206\u3002", "result": "\u5728\u6458\u8981\u548cRAG\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u8fd9\u79cd\u52a8\u6001\u8bc4\u5ba1\u56e2\u7684\u7cfb\u7edf\u5728\u4e0e\u4eba\u7c7b\u5224\u65ad\u7684\u76f8\u5173\u6027\u4e0a\u663e\u8457\u9ad8\u4e8e\u5355\u4e00\u8bc4\u5224\u548c\u9759\u6001\u8bc4\u5ba1\u56e2\u57fa\u7ebf\u3002", "conclusion": "\u8fd9\u4e9b\u7ed3\u679c\u7a81\u51fa\u4e86\u57fa\u4e8e\u5b66\u4e60\u7684\u9002\u5e94\u6027\u8bc4\u5ba1\u56e2\u5728\u6784\u5efa\u53ef\u4f38\u7f29\u3001\u66f4\u53ef\u9760\u548c\u503c\u5f97\u4fe1\u8d56\u7684LLM\u8bc4\u4f30\u7cfb\u7edf\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2512.01797", "categories": ["cs.AI", "cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2512.01797", "abs": "https://arxiv.org/abs/2512.01797", "authors": ["Cheng Gao", "Huimin Chen", "Chaojun Xiao", "Zhiyi Chen", "Zhiyuan Liu", "Maosong Sun"], "title": "H-Neurons: On the Existence, Impact, and Origin of Hallucination-Associated Neurons", "comment": "20 pages, 4 figures", "summary": "Large language models (LLMs) frequently generate hallucinations -- plausible but factually incorrect outputs -- undermining their reliability. While prior work has examined hallucinations from macroscopic perspectives such as training data and objectives, the underlying neuron-level mechanisms remain largely unexplored. In this paper, we conduct a systematic investigation into hallucination-associated neurons (H-Neurons) in LLMs from three perspectives: identification, behavioral impact, and origins. Regarding their identification, we demonstrate that a remarkably sparse subset of neurons (less than $0.1\\%$ of total neurons) can reliably predict hallucination occurrences, with strong generalization across diverse scenarios. In terms of behavioral impact, controlled interventions reveal that these neurons are causally linked to over-compliance behaviors. Concerning their origins, we trace these neurons back to the pre-trained base models and find that these neurons remain predictive for hallucination detection, indicating they emerge during pre-training. Our findings bridge macroscopic behavioral patterns with microscopic neural mechanisms, offering insights for developing more reliable LLMs.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2512.01870", "categories": ["cs.AI", "cond-mat.dis-nn", "math-ph", "math.NT"], "pdf": "https://arxiv.org/pdf/2512.01870", "abs": "https://arxiv.org/abs/2512.01870", "authors": ["Alessandro Breccia", "Federica Gerace", "Marco Lippi", "Gabriele Sicuro", "Pierluigi Contucci"], "title": "Testing Transformer Learnability on the Arithmetic Sequence of Rooted Trees", "comment": "21 pages, 8 figures", "summary": "We study whether a Large Language Model can learn the deterministic sequence of trees generated by the iterated prime factorization of the natural numbers. Each integer is mapped into a rooted planar tree and the resulting sequence $ \\mathbb{N}\\mathcal{T}$ defines an arithmetic text with measurable statistical structure. A transformer network (the GPT-2 architecture) is trained from scratch on the first $10^{11}$ elements to subsequently test its predictive ability under next-word and masked-word prediction tasks. Our results show that the model partially learns the internal grammar of $\\mathbb{N}\\mathcal{T}$, capturing non-trivial regularities and correlations. This suggests that learnability may extend beyond empirical data to the very structure of arithmetic.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2512.01878", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.01878", "abs": "https://arxiv.org/abs/2512.01878", "authors": ["Gaganpreet Jhajj", "Fuhua Lin"], "title": "Graph Distance as Surprise: Free Energy Minimization in Knowledge Graph Reasoning", "comment": "Accepted to NORA Workshop at NeurIPS 2025", "summary": "In this work, we propose that reasoning in knowledge graph (KG) networks can be guided by surprise minimization. Entities that are close in graph distance will have lower surprise than those farther apart. This connects the Free Energy Principle (FEP) from neuroscience to KG systems, where the KG serves as the agent's generative model. We formalize surprise using the shortest-path distance in directed graphs and provide a framework for KG-based agents. Graph distance appears in graph neural networks as message passing depth and in model-based reinforcement learning as world model trajectories. This work-in-progress study explores whether distance-based surprise can extend recent work showing that syntax minimizes surprise and free energy via tree structures.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2512.01880", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.01880", "abs": "https://arxiv.org/abs/2512.01880", "authors": ["Daren Zhong", "Dingcheng Huang", "Clayton Greenberg"], "title": "Predicting Human Chess Moves: An AI Assisted Analysis of Chess Games Using Skill-group Specific n-gram Language Models", "comment": null, "summary": "Chess, a deterministic game with perfect information, has long served as a benchmark for studying strategic decision-making and artificial intelligence. Traditional chess engines or tools for analysis primarily focus on calculating optimal moves, often neglecting the variability inherent in human chess playing, particularly across different skill levels.\n  To overcome this limitation, we propose a novel and computationally efficient move prediction framework that approaches chess move prediction as a behavioral analysis task. The framework employs n-gram language models to capture move patterns characteristic of specific player skill levels. By dividing players into seven distinct skill groups, from novice to expert, we trained separate models using data from the open-source chess platform Lichess. The framework dynamically selects the most suitable model for prediction tasks and generates player moves based on preceding sequences.\n  Evaluation on real-world game data demonstrates that the model selector module within the framework can classify skill levels with an accuracy of up to 31.7\\% when utilizing early game information (16 half-moves). The move prediction framework also shows substantial accuracy improvements, with our Selector Assisted Accuracy being up to 39.1\\% more accurate than our benchmark accuracy. The computational efficiency of the framework further enhances its suitability for real-time chess analysis.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2512.01958", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.01958", "abs": "https://arxiv.org/abs/2512.01958", "authors": ["Jie Meng", "Jin Mao"], "title": "Learned-Rule-Augmented Large Language Model Evaluators", "comment": null, "summary": "Large language models (LLMs) are predominantly used as evaluators for natural language generation (NLG) tasks, but their application to broader evaluation scenarios remains limited. In this work, we explore the potential of LLMs as general evaluators across diverse tasks. Although LLM-based evaluators have made progress in different areas, existing methods struggle to generalize due to their reliance on costly, human-designed evaluation principles, which are often misaligned with both annotated data and LLMs' understanding.To address these challenges, we propose a rule-augmented evaluation paradigm. First, we introduce a rule distillation method that automatically extracts scoring rules from data using an LLM-assisted Monte Carlo Tree Search (MCTS), alleviating scalability issues and improving alignment with data. Second, to enable LLMs to effectively apply the learned rules, we propose two strategies: (1) Chain-of-Rule (CoR), which guides LLM to follow distilled rules, and (2) training a rule-augmented LLM evaluator (RuAE) via reinforcement learning, further bridging the gap between rules and LLMs' reasoning. Extensive experiments on diverse tasks demonstrate the effectiveness and generalizability of our approach across various evaluation scenarios.", "AI": {"tldr": "\u672c\u6587\u63a2\u7d22\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLMs)\u4f5c\u4e3a\u591a\u79cd\u4efb\u52a1\u901a\u7528\u8bc4\u4f30\u8005\u7684\u6f5c\u529b\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u89c4\u5219\u589e\u5f3a\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u4ee5\u514b\u670d\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u7684\u6cdb\u5316\u74f6\u9888\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u7136\u8bed\u8a00\u751f\u6210(NLG)\u4efb\u52a1\u4e2d\u4e3b\u8981\u4f5c\u4e3a\u8bc4\u4f30\u8005\uff0c\u4f46\u5176\u5728\u66f4\u5e7f\u6cdb\u8bc4\u4f30\u573a\u666f\u4e2d\u7684\u5e94\u7528\u4ecd\u53d7\u9650\u3002", "method": "\u5f15\u5165\u89c4\u5219\u7cbe\u70bc\u65b9\u6cd5\uff0c\u901a\u8fc7LLM\u8f85\u52a9\u7684\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u4ece\u6570\u636e\u4e2d\u81ea\u52a8\u63d0\u53d6\u8bc4\u5206\u89c4\u5219\uff0c\u5e76\u63d0\u51fa\u94fe\u5f0f\u89c4\u5219\uff08CoR\uff09\u548c\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u89c4\u5219\u589e\u5f3a\u8bc4\u4f30\u8005\uff08RuAE\uff09\u4e24\u4e2a\u7b56\u7565\u3002", "result": "\u5728\u591a\u79cd\u4efb\u52a1\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5404\u79cd\u8bc4\u4f30\u573a\u666f\u4e2d\u7684\u6709\u6548\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": ""}}
{"id": "2512.01970", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.01970", "abs": "https://arxiv.org/abs/2512.01970", "authors": ["Sitao Cheng", "Xunjian Yin", "Ruiwen Zhou", "Yuxuan Li", "Xinyi Wang", "Liangming Pan", "William Yang Wang", "Victor Zhong"], "title": "From Atomic to Composite: Reinforcement Learning Enables Generalization in Complementary Reasoning", "comment": "Work in Progress. Code and data will be available at https://github.com/sitaocheng/from_atomic_to_composite", "summary": "The mechanism by which RL contributes to reasoning capabilities-whether it incentivizes the synthesis of new skills or merely amplifies existing behaviors-remains a subject of intense debate. In this work, we investigate this question through the lens of Complementary Reasoning, a complex task that requires integrating internal parametric knowledge with external contextual information. Using a controlled synthetic dataset of human biographies, we strictly decouple this ability into two atomic skills: Parametric Reasoning (relying on internal knowledge) and Contextual Reasoning (depending on external information). To rigorously assess capability boundaries, we evaluate generalization across three distinct levels of difficulty: I.I.D., Composition, and Zero-shot settings. We find that while SFT is sufficient for in-distribution performance, it struggles with O.O.D. generalization, particularly in Zero-shot settings where relational combinations are novel. Crucially, we identify the SFT Generalization Paradox: Models supervised solely on the composite task achieve near-perfect in-distribution accuracy but collapse on out-of-distribution generalization, indicating their reliance on rote memorization of path shortcuts. In contrast, we find that RL acts as a reasoning synthesizer rather than a probability amplifier. However, we uncover a strict atomic prerequisite: RL can only synthesize these complex strategies if the base model has first mastered the independent atomic skills (Parametric and Contextual) via SFT. These findings challenge the view of RL as a mere amplifier, suggesting that given sufficient atomic foundations, RL can actively synthesize complex reasoning strategies from learned primitives without explicit supervision on such complex strategies. This indicates that decoupled atomic training followed by RL offers a scalable path to generalization for complex reasoning tasks.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2512.01979", "categories": ["cs.AI", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.01979", "abs": "https://arxiv.org/abs/2512.01979", "authors": ["Aiden Yiliu Li", "Bizhi Yu", "Daoan Lei", "Tianhe Ren", "Shilong Liu"], "title": "Chain-of-Ground: Improving GUI Grounding via Iterative Reasoning and Reference Feedback", "comment": null, "summary": "GUI grounding aims to align natural language instructions with precise regions in complex user interfaces. Advanced multimodal large language models show strong ability in visual GUI grounding but still struggle with small or visually similar targets and ambiguity in real world layouts. These limitations arise from limited grounding capacity and from underuse of existing reasoning potential. We present Chain of Ground CoG a training free multi step grounding framework that uses multimodal large language models for iterative visual reasoning and refinement. Instead of direct prediction the model progressively reflects and adjusts its hypotheses leading to more accurate and interpretable localization. Our approach achieves 68.4 accuracy on the ScreenSpot Pro benchmark an improvement of 4.8 points. To measure real world generalization we introduce TPanel UI a dataset of 420 labeled industrial control panels with visual distortions such as blur and masking. On TPanel UI Chain of Ground improves over the strong baseline Qwen3 VL 235B by 6.9 points showing the effectiveness of multi step training free grounding across real world and digital interfaces. These results highlight a direction for unlocking grounding potential through structured iterative refinement instead of additional training.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
