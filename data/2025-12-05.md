<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 10]
- [cs.AI](#cs.AI) [Total: 33]
- [cs.SE](#cs.SE) [Total: 16]
- [cs.CR](#cs.CR) [Total: 16]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [Toward Sustainability-Aware LLM Inference on Edge Clusters](https://arxiv.org/abs/2512.04088)
*Kolichala Rajashekar,Nafiseh Sharghivand,Radu Prodan,Reza Farahani*

Main category: cs.DC

TL;DR: True


<details>
  <summary>Details</summary>
Motivation: True

Method: True

Result: True

Conclusion: True

Abstract: Large language models (LLMs) require substantial computational resources, leading to significant carbon emissions and operational costs. Although training is energy-intensive, the long-term environmental burden arises from inference, amplified by the massive global query volume. Cloud-based inference offers scalability but suffers from latency and bandwidth constraints due to centralized processing and continuous data transfer. Edge clusters instead can mitigate these limitations by enabling localized execution, yet they face trade-offs between performance, energy efficiency, and device constraints. This short paper presents a sustainability-aware LLM inference for edge clusters comprising NVIDIA Jetson Orin NX (8GB) and Nvidia Ada 2000 (16GB) devices. It aims to balance inference latency and carbon footprint through carbon- and latency-aware routing strategies, guided by empirical benchmarking of energy consumption and execution time across diverse prompts and batch (i.e., group of prompts) configurations. We compared baseline greedy strategies to carbon-aware and latency-aware strategies in prompt routing to specific hardware based on benchmarking information. Experimental evaluation shows that a batch size of four prompts achieves a trade-off between throughput, energy efficiency, while larger batches risk GPU memory saturation.

</details>


### [2] [Serverless Everywhere: A Comparative Analysis of WebAssembly Workflows Across Browser, Edge, and Cloud](https://arxiv.org/abs/2512.04089)
*Mario Colosi,Reza Farahani,Lauri Loven,Radu Prodan,Massimo Villari*

Main category: cs.DC

TL;DR: 该论文评估了 WebAssembly (Wasm) 在浏览器、边缘节点和云服务器上的无服务器工作流执行性能。重点考察了启动开销、运行时执行模型和资源差异。


<details>
  <summary>Details</summary>
Motivation: Wasm 的二进制格式和沙箱特性使其适合在不同平台上进行无服务器工作流执行，但性能和稳定性仍有待详细评估。

Method: 使用 wasm32-wasi 模块，在浏览器中通过 Web Worker 执行，在边缘和云上用 HTTP shim 将数据流传输到 Wasm 运行时。测量冷启动和热启动延迟、每步延迟、整个工作流时间、吞吐量以及 CPU/内存利用率。

Result: AOT 编译和实例预热大幅减少启动延迟。小数据负载工作流中，浏览器性能表现较好；但随着负载增加，边缘和云上的 AOT 执行性能明显优于浏览器性能。

Conclusion: AOT 编译和实例预热能显著提升 Wasm 执行性能，浏览器在小数据负载下具有竞争力，而边缘和云在高负载时表现更佳。

Abstract: WebAssembly (Wasm) is a binary instruction format that enables portable, sandboxed, and near-native execution across heterogeneous platforms, making it well-suited for serverless workflow execution on browsers, edge nodes, and cloud servers. However, its performance and stability depend heavily on factors such as startup overhead, runtime execution model (e.g., Ahead-of-Time (AOT) and Just-in-Time (JIT) compilation), and resource variability across deployment contexts. This paper evaluates a Wasm-based serverless workflow executed consistently from the browser to edge and cloud instances. The setup uses wasm32-wasi modules: in the browser, execution occurs within a web worker, while on Edge and Cloud, an HTTP shim streams frames to the Wasm runtime. We measure cold- and warm-start latency, per-step delays, workflow makespan, throughput, and CPU/memory utilization to capture the end-to-end behavior across environments. Results show that AOT compilation and instance warming substantially reduce startup latency. For workflows with small payloads, the browser achieves competitive performance owing to fully in-memory data exchanges. In contrast, as payloads grow, the workflow transitions into a compute- and memory-intensive phase where AOT execution on edge and cloud nodes distinctly surpasses browser performance.

</details>


### [3] [Energy-Efficient Resource Management in Microservices-based Fog and Edge Computing: State-of-the-Art and Future Directions](https://arxiv.org/abs/2512.04093)
*Ali Akbar Vali,Sadoon Azizi,Mohammad Shojafar,Rajkumar Buyya*

Main category: cs.DC

TL;DR: 本文针对雾计算和边缘计算中的资源管理问题，特别是面向基于微服务架构的应用场景，进行了全面综述，并提出了一个分类框架，将现有研究归纳为五个关键子领域，强调了能效优化的重要性，同时指出了当前研究中的挑战和未来的潜在研究方向。 核心要点包括： 1. 研究背景：物联网设备快速增长，对服务高效性和响应速度的需求日益增强，由此推动了雾计算和边缘计算的发展，但也带来了资源管理的挑战，包括资源受限、计算异构性、动态工作负载和多样化的服务质量（QoS）需求。2. 主要贡献： 
   - 对2020-2024年间136项相关研究系统性回顾与分类，聚焦能效优化； 
   - 提出了涵盖五大领域的分类法：服务部署、资源供应、任务调度与卸载、资源分配、实例选择； 
   - 指出了现有调查的空白，明确了尚未解决的挑战和技术短板，如资源管理组件间协同缺失； 
   - 提出了未来可能的研究方向，包括AI驱动的优化、量子计算和函数即服务（FaaS）等新兴技术的应用潜力。 3. 分类法详细内容：每一类都从优化的技术、目标、优劣势等维度进行了深入的分析，旨在为从业人员提供统一且具能量认知的观点，助力构建更集成、高效和可持续的边缘/雾计算解决方案。4. 未来研究展望：
    - AI驱动的优化：利用人工智能算法如深度强化学习等提高资源利用率，实现动态环境下的智能决策；
    - 量子计算：探索在资源受限情况下，如何通过量子计算提升计算效率，尤其是针对高并行度和复杂度高的任务；
    - 函数即服务（FaaS, Serverless Computing）: 研究其作为一种自动扩展模式的适用性，减少运维成本，提高弹性伸缩能力。总体而言，该文不仅提供了丰富的文献汇总，还通过明确指出现有工作的局限性和未来可能的技术路径，为后续研究奠定了坚实基础，具有很强的参考价值。


<details>
  <summary>Details</summary>
Motivation: 本文旨在针对雾计算和边缘计算中基于微服务架构的资源管理问题，提供一个全面、系统性的综述，并提出的分类法进行整理和归纳，以五大关键子领域为核心，突出能效优化的关键角色。此外，本文还特别强调了当前研究的不足之处，并基于此提出了几个未来有潜力的研究方向，包括AI优化、量子计算及函数即服务（serverless）机制等。

Method: 1. 时间范围限定法：挑选自2020年至2024年期间发表的文献，保证信息的新颖性和时效性。
2. 多维度分析法：从优化技术、目标、优点、缺陷等多个角度出发，对每篇入选论文进行细致剖析。
3. 主题聚类法：根据研究内容和服务模型将工作划分为‘服务部署’、‘资源供给’、‘任务调度与卸载’、‘资源分配’、‘实例选择’五大类别，便于后续的总结与对比。
4. 跨领域融合法：结合AI、量子计算、无服务器架构等多种前沿技术趋势，探索它们在未来资源管理方案中的潜在作用。
5. 空白定位法：细致审查已有综述性文献，识别出目前研究中存在的盲点及尚待突破的关键环节，为进一步的研究指明方向。

Result: 通过对136篇相关研究（2020-2024）进行系统性回顾和分类，本文成功构建了涵盖五个核心领域的分类体系：服务放置、资源调配、任务调度与卸载、资源分配、实例选择，并提出了几种未来有潜力的技术路径，如AI优化、量子计算以及无服务器计算模式。该分类体系不仅帮助读者快速掌握各类研究的关键特征及其内在联系，也为设计新一代的、更加注重能源效率的雾/边缘计算解决方案提供了理论支持。此外，作者指出的研究短板和潜在发展机遇，也有助于启发新思路，推动本领域的持续发展。

Conclusion: 本综述文章系统梳理并分类了近四年（2020-2024）来有关微服务架构下雾计算与边缘计算中的资源管理策略，尤其集中在能效优化方面。五大分类——服务布置、资源准备、任务调度和迁移、资源分配、实例挑选——全面覆盖了当前的主流研究路径。通过对现有研究工作的深入分析，作者不仅揭示了存在的问题（如各部分之间缺乏协同），还前瞻性的讨论了几个可能的技术突破口，比如借助AI、量子计算技术和采用Serverless架构。这些讨论和展望，对于指导后续研究活动、促进技术创新具有重要意义，同时也为希望了解该领域全貌的人士提供了一份详实且易于理解的参考资料。本文为未来雾计算和边缘计算领域的可持续发展奠定了坚实的理论基础。

Abstract: The exponential growth of Internet of Things (IoT) devices has intensified the demand for efficient and responsive services. To address this demand, fog and edge computing have emerged as distributed paradigms that bring computational resources closer to end users, reducing latency, bandwidth limitations, and energy consumption. However, these paradigms present challenges in resource management due to resource constraints, computational heterogeneity, dynamic workloads, and diverse Quality of Service (QoS) requirements. This paper presents a comprehensive survey of state-of-the-art resource management strategies in microservices-based fog and edge computing, focusing on energy-efficient solutions. We systematically review and classify more than 136 studies (2020-2024) into five key subdomains: service placement, resource provisioning, task scheduling and offloading, resource allocation, and instance selection. Our categorization is based on optimization techniques, targeted objectives, and the strengths and limitations of each approach. In addition, we examine existing surveys and identify unresolved challenges and gaps in the literature. By highlighting the lack of synergy among fundamental resource management components, we outline promising research directions leveraging AI-driven optimization, quantum computing, and serverless computing. This survey serves as a comprehensive reference for researchers and practitioners by providing a unified and energy-aware perspective on resource management in microservices-based fog and edge computing, paving the way for more integrated, efficient, and sustainable future solutions.

</details>


### [4] [Formal Specification for Fast ACS: Low-Latency File-Based Ordered Message Delivery at Scale](https://arxiv.org/abs/2512.04096)
*Sushant Kumar Gupta,Anil Raghunath Iyer,Chang Yu,Neel Bagora,Olivier Pomerleau,Vivek Kumar,Prunthaban Kanthakumar*

Main category: cs.DC

TL;DR: Fast ACS是一种基于文件的，利用RPC和RMA通信方式，成功在数十个生产集群中部署的，可扩展至数千消费者的低延迟消息传递系统的设计与实现。


<details>
  <summary>Details</summary>
Motivation: 论述了实时系统对低延迟消息传递的需求以及随之而来的挑战，引出了对强健消息系统的需求。

Method: 提出了Fast ACS的设计与实现，该系统是一种基于文件的有序消息传递系统，利用了两端（集群间）和单边（集群内）通信原语，即远程过程调用（RPC）和远程内存访问（RMA）。

Result: Fast ACS已经成功部署到数十个生产集群，并可扩展至每个集群中的数千个消费者，峰值时甚至能达到Tbps级别的集群内消费者流量。该系统在资源成本较低的情况下，可以在几秒甚至更少的时间内（p99）将消息传递给全球的消费者。

Conclusion: Fast ACS通过利用RPC和RMA，实现了低延迟、可扩展和高吞吐量的消息传递，为实时系统提供了有效的解决方案。

Abstract: Low-latency message delivery is crucial for real-time systems. Data originating from a producer must be delivered to consumers, potentially distributed in clusters across metropolitan and continental boundaries. With the growing scale of computing, there can be several thousand consumers of the data. Such systems require a robust messaging system capable of transmitting messages containing data across clusters and efficiently delivering them to consumers. The system must offer guarantees like ordering and at-least-once delivery while avoiding overload on consumers, allowing them to consume messages at their own pace.
  This paper presents the design of Fast ACS (an abbreviation for Ads Copy Service), a file-based ordered message delivery system that leverages a combination of two-sided (inter-cluster) and one-sided (intra-cluster) communication primitives - namely, Remote Procedure Call and Remote Memory Access, respectively - to deliver messages. The system has been successfully deployed to dozens of production clusters and scales to accommodate several thousand consumers within each cluster, which amounts to Tbps-scale intra-cluster consumer traffic at peak. Notably, Fast ACS delivers messages to consumers across the globe within a few seconds or even sub-seconds (p99) based on the message volume and consumer scale, at a low resource cost.

</details>


### [5] [tritonBLAS: Triton-based Analytical Approach for GEMM Kernel Parameter Selection](https://arxiv.org/abs/2512.04226)
*Ryan Swann,Muhammad Osama,Xiaohu Guo,Bryant Nelson,Lixun Zhang,Alex Brown,Yen Ong,Ali Yazdani,Sean Siddens,Ganesh Dasika,Alex Underwood*

Main category: cs.DC

TL;DR: tritonBLAS是一个快速且确定性的分析模型，利用架构参数生成高性能的GPU GEMM内核，无需运行时自动调优即可预测最优配置。


<details>
  <summary>Details</summary>
Motivation: 解决运行自动调优（autotuning）在实际生产环境中的时间成本高和问题性能优化的挑战。

Method: 提出了一种名为tritonBLAS的分析模型，该模型运用体系结构参数如缓存层次结构和代码及数据的相对位置，生成高效能的GPU GEMM内核。

Result:  tritonBLAS实现了超过95%的自动调优解决方案的性能，同时将自动调优时间减少到零。

Conclusion: tritonBLAS可以实际应用于高效率计算（HPC）和机器学习（ML）工作任务中，作为经验调优的替代方法。

Abstract: We present tritonBLAS, a fast and deterministic analytical model that uses architectural parameters like the cache hierarchy, and relative code and data placement to generate performant GPU GEMM kernels. tritonBLAS explicitly models the relationship between architectural topology, matrix shapes, and algorithmic blocking behavior to predict near-optimal configurations without runtime autotuning. Based on this model, we developed and implemented a lightweight GEMM framework entirely within Triton. We evaluate the performance of tritonBLAS across a diverse set of GEMM problem sizes on modern GPUs. tritonBLAS achieves over 95% of the performance of autotuning solutions, while reducing autotuning time to zero. This makes tritonBLAS a practical drop-in replacement for empirical tuning in production HPC and ML workloads.

</details>


### [6] [VLCs: Managing Parallelism with Virtualized Libraries](https://arxiv.org/abs/2512.04320)
*Yineng Yan,William Ruys,Hochan Lee,Ian Henriksen,Arthur Peters,Sean Stephens,Bozhi You,Henrique Fingler,Martin Burtscher,Milos Gligoric,Keshav Pingali,Mattan Erez,George Biros,Christopher J. Rossbach*

Main category: cs.DC

TL;DR: 随着现代并行机器的复杂性和规模不断增长，程序员越来越依赖于软件库的组合来封装和利用并行性。然而，许多库的设计并未考虑到组合，它们假定可以独占所有资源。同时使用这些库可能导致资源争用和性能下降。先前的解决方案涉及修改库或操作系统，而这通常是不可行的。本文提出了虚拟库上下文（VLCs），它是封装库和关联资源分配的过程子单元。VLCs无需修改库代码就能控制这些库的资源利用。这使用户可以在库之间划分资源以防止争用，或者在同一个进程中加载多个相同的库以允许并行执行原本不安全的代码。本文描述并评估了C++和Python的VLCs原型。实验证明，在使用OpenMP、OpenBLAS和LibTorch等应用的基准测试中，VLCs可以实现高达2.85倍的速度提升。


<details>
  <summary>Details</summary>
Motivation: 现代并行机器的复杂性和规模不断增长，程序员需要组合软件库以封装和利用并行性。然而，许多库未设计为组合使用，假设它们独占所有资源，这导致使用时可能产生资源争用和性能下降的问题。修改库或操作系统以解决这一问题通常不可行。

Method: 提出了虚拟库上下文（VLCs），无需修改库代码即可封装和控制库的资源利用。VLCs使用户能够划分资源以防止库间争用，或在同一进程中加载多个相同库以并行执行原本不安全的代码。该方案在C++和Python中实现原型，并评估性能。

Result: 实验展示了VLCs在包括使用OpenMP、OpenBLAS和LibTorch的应用程序的基准测试中，能够实现高达2.85倍的性能速度提升。

Conclusion: VLCs是一种有效的解决方案，能够在不修改库代码的情况下控制库的资源利用，防止资源争用，从而提升性能。这一方案在现代并行机器的环境中具有实际应用潜力。

Abstract: As the complexity and scale of modern parallel machines continue to grow, programmers increasingly rely on composition of software libraries to encapsulate and exploit parallelism. However, many libraries are not designed with composition in mind and assume they have exclusive access to all resources. Using such libraries concurrently can result in contention and degraded performance. Prior solutions involve modifying the libraries or the OS, which is often infeasible.
  We propose Virtual Library Contexts (VLCs), which are process subunits that encapsulate sets of libraries and associated resource allocations. VLCs control the resource utilization of these libraries without modifying library code. This enables the user to partition resources between libraries to prevent contention, or load multiple copies of the same library to allow parallel execution of otherwise thread-unsafe code within the same process.
  In this paper, we describe and evaluate C++ and Python prototypes of VLCs. Experiments show VLCs enable a speedup up to 2.85x on benchmarks including applications using OpenMP, OpenBLAS, and LibTorch.

</details>


### [7] [Counting Without Running: Evaluating LLMs' Reasoning About Code Complexity](https://arxiv.org/abs/2512.04355)
*Gregory Bolet,Giorgis Georgakoudis,Konstantinos Parasyris,Harshitha Menon,Niranjan Hasabnis,Kirk W. Cameron,Gal Oren*

Main category: cs.DC

TL;DR: 该研究介绍了gpuFLOPBench，一个为测试LLMs在预测CUDA核函数中FLOP计数的基准，强调了硬件特定微码效应对模型推理的影响。


<details>
  <summary>Details</summary>
Motivation: 现代GPU软件栈需要开发者能够在启动核心之前预测性能瓶颈，但现有大型语言模型(LLMs)很少在预测工作负载方面被测试。

Method: 研究者创建了gpuFLOPBench，该基准要求模型通过预测从HeCBench选取的577个CUDA核心的单/双精度FLOP计数来“不运行计数”。核心被配有基准真相分析和八种执行属性。

Result: 研究评估最新的闭源推理模型，表明尽管最新的LLMs能在简单的核心上实现完美分类，但在隐式FLOPs出现时仍会导致数量级错误。这反映了现有代码助手在面对硬件特定微码效应时的局限。

Conclusion: gpuFLOPBench可以作为开发LLM工具的专注测试平台，以发展出与经验丰富的GPU开发者相媲美的性能推理能力。

Abstract: Modern GPU software stacks demand developers who can anticipate performance bottlenecks before ever launching a kernel; misjudging floating-point workloads upstream can derail tuning, scheduling, and even hardware procurement. Yet despite rapid progress in code generation, today's Large Language Models (LLMs) are rarely tested on this kind of forward-looking reasoning. We close that gap with gpuFLOPBench, a benchmark that asks models to "count without running" by predicting single and double-precision FLOP counts for 577 CUDA kernels drawn from HeCBench, annotated with ground-truth profiles and eight execution attributes that distinguish trivially analyzable code from kernels whose FLOPs depend on hidden compiler or runtime behavior. Evaluating current closed-source reasoning models shows clear but uneven progress: the newest LLMs achieve perfect classification on straightforward kernels but still incur multiple order-of-magnitude errors whenever implicit FLOPs arise from division, intrinsic math functions, or common subexpressions. These results surface a core limitation of existing code assistants -- the inability to internalize hardware-specific microcode effects -- and position gpuFLOPBench as a focused testbed for developing LLM tooling that can reason about performance with the same rigor as experienced GPU developers. Sources are available at our repository: https://github.com/Scientific-Computing-Lab/gpuFLOPBench

</details>


### [8] [A Structure-Aware Irregular Blocking Method for Sparse LU Factorization](https://arxiv.org/abs/2512.04389)
*Zhen Hu,Dongliang Xiong,Kai Huang,Changjun Wu,Xiaowen Jiang*

Main category: cs.DC

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: In sparse LU factorization, nonzero elements after symbolic factorization tend to distribute in diagonal and right-bottom region of sparse matrices. However, regular 2D blocking on this non-uniform distribution structure may lead to workload imbalance across blocks. Besides, existing matrix features fail to guide us effectively in blocking. In this paper, we propose a structure-aware irregular blocking method for numerical factorization. A novel diagonal block-based feature is introduced to effectively characterize the local nonzero distribution of sparse matrices. Based on this, we further propose an irregular blocking method that adjusts block sizes according to the local distribution of nonzeros. The strategy utilizes fine-grained blocks in dense regions and coarse-grained blocks in sparse regions, adequately balancing the nonzeros of blocks both within the same level and across levels in the dependency tree. Experiments demonstrate that, on a single NVIDIA A100 GPU, our proposed irregular blocking method achieves average speedups of 1.50x and 3.32x over PanguLU and the latest SuperLU_DIST, respectively. In addition, it achieves speedups of 1.40x and 3.84x over PanguLU and SuperLU_DIST on 4 NVIDIA A100 GPUs.

</details>


### [9] [Offloading to CXL-based Computational Memory](https://arxiv.org/abs/2512.04449)
*Suyeon Lee,Kangkyu Park,Kwangsik Shin,Ada Gavrilovska*

Main category: cs.DC

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: CXL-based Computational Memory (CCM) enables near-memory processing within expanded remote memory, presenting opportunities to address data movement costs associated with disaggregated memory systems and to accelerate overall performance. However, existing operation offloading mechanisms are not capable of leveraging the trade-offs of different models based on different CXL protocols. This work first examines these tradeoffs and demonstrates their impact on end-to-end performance and system efficiency for workloads with diverse data and processing requirements. We propose a novel 'Asynchronous Back-Streaming' protocol by carefully layering data and control transfer operations on top of the underlying CXL protocols. We design KAI, a system that realizes the asynchronous back-streaming model that supports asynchronous data movement and lightweight pipelining in host-CCM interactions. Overall, KAI reduces end-to-end runtime by up to 50.4%, and CCM and host idle times by average 22.11x and 3.85x, respectively.

</details>


### [10] [Federated Learning for Terahertz Wireless Communication](https://arxiv.org/abs/2512.04984)
*O. Tansel Baydas,Ozgur B. Akan*

Main category: cs.DC

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The convergence of Terahertz (THz) communications and Federated Learning (FL) promises ultra-fast distributed learning, yet the impact of realistic wideband impairments on optimization dynamics remains theoretically uncharacterized. This paper bridges this gap by developing a multicarrier stochastic framework that explicitly couples local gradient updates with frequency-selective THz effects, including beam squint, molecular absorption, and jitter. Our analysis uncovers a critical diversity trap: under standard unbiased aggregation, the convergence error floor is driven by the harmonic mean of subcarrier SNRs. Consequently, a single spectral hole caused by severe beam squint can render the entire bandwidth useless for reliable model updates. We further identify a fundamental bandwidth limit, revealing that expanding the spectrum beyond a critical point degrades convergence due to the integration of thermal noise and gain collapse at band edges. Finally, we demonstrate that an SNR-weighted aggregation strategy is necessary to suppress the variance singularity at these spectral holes, effectively recovering convergence in high-squint regimes where standard averaging fails. Numerical results validate the expected impact of the discussed physical layer parameters' on performance of THz-FL systems.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [11] [Solving N-Queen Problem using Las Vegas Algorithm with State Pruning](https://arxiv.org/abs/2512.04139)
*Susmita Sharma,Aayush Shrestha,Sitasma Thapa,Prashant Timalsina,Prakash Poudyal*

Main category: cs.AI

TL;DR: true


<details>
  <summary>Details</summary>
Motivation: true

Method: true

Result: true

Conclusion: true

Abstract: The N-Queens problem, placing all N queens in a N x N chessboard where none attack the other, is a classic problem for constraint satisfaction algorithms. While complete methods like backtracking guarantee a solution, their exponential time complexity makes them impractical for large-scale instances thus, stochastic approaches, such as Las Vegas algorithm, are preferred. While it offers faster approximate solutions, it suffers from significant performance variance due to random placement of queens on the board. This research introduces a hybrid algorithm built on top of the standard Las Vegas framework through iterative pruning, dynamically eliminating invalid placements during the random assignment phase, thus this method effectively reduces the search space. The analysis results that traditional backtracking scales poorly with increasing N. In contrast, the proposed technique consistently generates valid solutions more rapidly, establishing it as a superior alternative to use where a single, timely solution is preferred over completeness. Although large N causes some performance variability, the algorithm demonstrates a highly effective trade-off between computational cost and solution fidelity, making it particularly suited for resource-constrained computing environments.

</details>


### [12] [RippleBench: Capturing Ripple Effects Using Existing Knowledge Repositories](https://arxiv.org/abs/2512.04144)
*Roy Rinberg,Usha Bhalla,Igor Shilov,Flavio P. Calmon,Rohit Gandikota*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Targeted interventions on language models, such as unlearning, debiasing, or model editing, are a central method for refining model behavior and keeping knowledge up to date. While these interventions aim to modify specific information within models (e.g., removing virology content), their effects often propagate to related but unintended areas (e.g., allergies); these side-effects are commonly referred to as the ripple effect. In this work, we present RippleBench-Maker, an automatic tool for generating Q&A datasets that allow for the measurement of ripple effects in any model-editing task. RippleBench-Maker builds on a Wikipedia-based RAG pipeline (WikiRAG) to generate multiple-choice questions at varying semantic distances from the target concept (e.g., the knowledge being unlearned). Using this framework, we construct RippleBench-Bio, a benchmark derived from the WMDP (Weapons of Mass Destruction Paper) dataset, a common unlearning benchmark. We evaluate eight state-of-the-art unlearning methods and find that all exhibit non-trivial accuracy drops on topics increasingly distant from the unlearned knowledge, each with distinct propagation profiles. To support ongoing research, we release our codebase for on-the-fly ripple evaluation, along with the benchmark, RippleBench-Bio.

</details>


### [13] [Balancing Safety and Helpfulness in Healthcare AI Assistants through Iterative Preference Alignment](https://arxiv.org/abs/2512.04210)
*Huy Nghiem,Swetasudha Panda,Devashish Khatwani,Huy V. Nguyen,Krishnaram Kenthapadi,Hal Daumé*

Main category: cs.AI

TL;DR: 提出一个迭代优化框架，利用KTO和DPO提升医疗领域LLMs的安全性与可信度。


<details>
  <summary>Details</summary>
Motivation: 确保医疗对话助手的响应安全且可靠，同时尽量减少对有益查询的过度拒绝。

Method: 采用KTO和DPO方法，通过CARES-18K基准评估Llama-3B/8B, Meditron-8B, Mistral-7B等模型的安全性，并进行多项迭代优化实验。

Result: 在有害请求检测中安全性相关指标提升达42%，但也发现模型构架相关的校准偏差和误拒绝的权衡。

Conclusion: 需要采用最佳实践来平衡患者安全、用户信任和临床应用在医疗对话助手设计中的重要性。

Abstract: Large Language Models (LLMs) are increasingly used in healthcare, yet ensuring their safety and trustworthiness remains a barrier to deployment. Conversational medical assistants must avoid unsafe compliance without over-refusing benign queries. We present an iterative post-deployment alignment framework that applies Kahneman-Tversky Optimization (KTO) and Direct Preference Optimization (DPO) to refine models against domain-specific safety signals. Using the CARES-18K benchmark for adversarial robustness, we evaluate four LLMs (Llama-3B/8B, Meditron-8B, Mistral-7B) across multiple cycles. Our results show up to 42% improvement in safety-related metrics for harmful query detection, alongside interesting trade-offs against erroneous refusals, thereby exposing architecture-dependent calibration biases. We also perform ablation studies to identify when self-evaluation is reliable and when external or finetuned judges are necessary to maximize performance gains. Our findings underscore the importance of adopting best practices that balance patient safety, user trust, and clinical utility in the design of conversational medical assistants.

</details>


### [14] [Educational Cone Model in Embedding Vector Spaces](https://arxiv.org/abs/2512.04227)
*Yo Ehara*

Main category: cs.AI

TL;DR: 本文提出了一种新的几何框架——教育锥模型，用以分析和选择适合文本难度分析的嵌入方法。该模型基于不同难度文本的概念多样性假设，寻找嵌入空间中的锥状分布，并将嵌入评估问题表述为一个优化问题，通过设计特定的损失函数，得到高效且闭式解法。实验证明，这一模型能够快速有效地发现与难度标注教育文本最为匹配的嵌入空间。


<details>
  <summary>Details</summary>
Motivation: 智能教育系统中，人类标注含明确难度评级的数据集很重要。然而，众多嵌入方法使得选择最适用的方法变得困难，因此有必要研究如何分析文本难度。

Method: 教育锥模型，一个基于文本难度与概念多样性关系的几何框架，并设计了一种高效且闭式的解法。

Result: 在真实数据集上的实证测试表明，所提出的模型能够快速有效地识别出与难度标注教育文本更匹配的嵌入空间。

Conclusion: 教育锥模型为嵌入方法的选择提供了一个快速而有效的框架，可用于智能教育系统的文本难度分析。

Abstract: Human-annotated datasets with explicit difficulty ratings are essential in intelligent educational systems. Although embedding vector spaces are widely used to represent semantic closeness and are promising for analyzing text difficulty, the abundance of embedding methods creates a challenge in selecting the most suitable method. This study proposes the Educational Cone Model, which is a geometric framework based on the assumption that easier texts are less diverse (focusing on fundamental concepts), whereas harder texts are more diverse. This assumption leads to a cone-shaped distribution in the embedding space regardless of the embedding method used. The model frames the evaluation of embeddings as an optimization problem with the aim of detecting structured difficulty-based patterns. By designing specific loss functions, efficient closed-form solutions are derived that avoid costly computation. Empirical tests on real-world datasets validated the model's effectiveness and speed in identifying the embedding spaces that are best aligned with difficulty-annotated educational texts.

</details>


### [15] [Addressing Logical Fallacies In Scientific Reasoning From Large Language Models: Towards a Dual-Inference Training Framework](https://arxiv.org/abs/2512.04228)
*Peter B. Walker,Hannah Davidson,Aiden Foster,Matthew Lienert,Thomas Pardue,Dale Russell*

Main category: cs.AI

TL;DR: 本文揭示了大型语言模型在科学推理中的弱点，并提出了通过结合肯定与否定推理的双推理训练框架，以提升模型的逻辑一致性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然在自然语言处理方面表现出色，但在科学领域面对否定、反例和错误前提时存在系统性缺陷，导致易受逻辑谬误和对抗性攻击。

Method: 提出一个双推理训练框架，将肯定生成与结构化反事实否定相结合，引入“否认前件”的机制以提高模型的推理鲁棒性和可解释性。

Result: 实验显示该框架能够帮助模型不仅确认有效推论，还能拒绝无效推论，使模型更具韧性，并与人类推理更一致。

Conclusion: 这一方法为改进大型语言模型的科学推理能力提供了新思路，通过结合形式逻辑和认知科学，使其在实际应用中更具可靠性和可信度。

Abstract: Large Language Models (LLMs) have transformed natural language processing and hold growing promise for advancing science, healthcare, and decision-making. Yet their training paradigms remain dominated by affirmation-based inference, akin to \textit{modus ponens}, where accepted premises yield predicted consequents. While effective for generative fluency, this one-directional approach leaves models vulnerable to logical fallacies, adversarial manipulation, and failures in causal reasoning. This paper makes two contributions. First, it demonstrates how existing LLMs from major platforms exhibit systematic weaknesses when reasoning in scientific domains with negation, counterexamples, or faulty premises \footnote{Code to recreate these experiments are at https://github.com/hannahdavidsoncollege-maker/ScientificReasoningForEnvironment-MedicineWithLLMs. Second, it introduces a dual-reasoning training framework that integrates affirmative generation with structured counterfactual denial. Grounded in formal logic, cognitive science, and adversarial training, this training paradigm formalizes a computational analogue of ``denying the antecedent'' as a mechanism for disconfirmation and robustness. By coupling generative synthesis with explicit negation-aware objectives, the framework enables models that not only affirm valid inferences but also reject invalid ones, yielding systems that are more resilient, interpretable, and aligned with human reasoning.

</details>


### [16] [Toward Virtuous Reinforcement Learning](https://arxiv.org/abs/2512.04246)
*Majid Ghasemi,Mark Crowley*

Main category: cs.AI

TL;DR: 本文讨论了当前机器伦理在强化学习中的局限性，提出了以美德为重点的替代方案。它指出，当前主流方法存在两大问题：基于规则的方法在模糊性和非稳定性情况下效果不佳，而单目标奖励方法把多种道德考量简化为单一标量信号。文章主张将伦理视为策略层面的倾向，强调稳定性、习惯性和价值冲突的透明性。为实现这一目标，文章提出了四个关键组成部分：多智能体社会学习、多目标和带约束的公式化表达、基于亲和性美德先验的正则化，以及将不同伦理传统操作化为实际的控制信号。


<details>
  <summary>Details</summary>
Motivation: 动机在于当前机器伦理在强化学习中的局限性，尤其是基于规则和单目标奖励的方法无法有效处理复杂现实场景中的道德问题，需要新的方法解决这些局限性。

Method: 作者提出将伦理视为政策水平上的倾向和习惯，并提出了以下四种方法：1. 多智能体强化学习用于不完美的规范性学习 2. 多目标和带约束的表述 3. 基于亲和性的正则化保持更新的美德 4. 将各种伦理传统操作化为实用控制信号。

Result: 没有具体提及结果，但指出了新方法的预期效果：能够更好地处理现实生活中的伦理困境，并维持在不同激励和环境下的一致性和适应性。这篇文章还为如何设计强化学习系统的伦理基准提供了新思路。

Conclusion: 文章得出的结论是，当前机器伦理的方法需要转变，应更专注于稳定性、道德冲突的透明性和文化价值的明确。通过提出的新方法，可以为机器伦理发展提供一个更全面的框架。

Abstract: This paper critiques common patterns in machine ethics for Reinforcement Learning (RL) and argues for a virtue focused alternative. We highlight two recurring limitations in much of the current literature: (i) rule based (deontological) methods that encode duties as constraints or shields often struggle under ambiguity and nonstationarity and do not cultivate lasting habits, and (ii) many reward based approaches, especially single objective RL, implicitly compress diverse moral considerations into a single scalar signal, which can obscure trade offs and invite proxy gaming in practice. We instead treat ethics as policy level dispositions, that is, relatively stable habits that hold up when incentives, partners, or contexts change. This shifts evaluation beyond rule checks or scalar returns toward trait summaries, durability under interventions, and explicit reporting of moral trade offs. Our roadmap combines four components: (1) social learning in multi agent RL to acquire virtue like patterns from imperfect but normatively informed exemplars; (2) multi objective and constrained formulations that preserve value conflicts and incorporate risk aware criteria to guard against harm; (3) affinity based regularization toward updateable virtue priors that support trait like stability under distribution shift while allowing norms to evolve; and (4) operationalizing diverse ethical traditions as practical control signals, making explicit the value and cultural assumptions that shape ethical RL benchmarks.

</details>


### [17] [The Geometry of Benchmarks: A New Path Toward AGI](https://arxiv.org/abs/2512.04276)
*Przemyslaw Chojecki*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Benchmarks are the primary tool for assessing progress in artificial intelligence (AI), yet current practice evaluates models on isolated test suites and provides little guidance for reasoning about generality or autonomous self-improvement. Here we introduce a geometric framework in which all psychometric batteries for AI agents are treated as points in a structured moduli space, and agent performance is described by capability functionals over this space. First, we define an Autonomous AI (AAI) Scale, a Kardashev-style hierarchy of autonomy grounded in measurable performance on batteries spanning families of tasks (for example reasoning, planning, tool use and long-horizon control). Second, we construct a moduli space of batteries, identifying equivalence classes of benchmarks that are indistinguishable at the level of agent orderings and capability inferences. This geometry yields determinacy results: dense families of batteries suffice to certify performance on entire regions of task space. Third, we introduce a general Generator-Verifier-Updater (GVU) operator that subsumes reinforcement learning, self-play, debate and verifier-based fine-tuning as special cases, and we define a self-improvement coefficient $κ$ as the Lie derivative of a capability functional along the induced flow. A variance inequality on the combined noise of generation and verification provides sufficient conditions for $κ> 0$. Our results suggest that progress toward artificial general intelligence (AGI) is best understood as a flow on moduli of benchmarks, driven by GVU dynamics rather than by scores on individual leaderboards.

</details>


### [18] [Artificial Intelligence Applications in Horizon Scanning for Infectious Diseases](https://arxiv.org/abs/2512.04287)
*Ian Miles,Mayumi Wakimoto,Wagner Meira,Daniela Paula,Daylene Ticiane,Bruno Rosa,Jane Biddulph,Stelios Georgiou,Valdir Ermida*

Main category: cs.AI

TL;DR: The paper review the integration of AI in Horizon Scanning for infectious diseases, covering signal detection, data monitoring, scenario analysis, decision support, and AI adoption risks.


<details>
  <summary>Details</summary>
Motivation: 探索人工智能在扫描传染病相关的新兴威胁和机遇的整合应用，并增强前瞻文献中公共卫生准备中AI的潜力和局限性。

Method: 审查和评估人工智能工具在信号检测、数据监控、场景分析和决策支持方面的应用，并提出实施和治理的有效策略。

Result: 展示了人工智能在公共卫生准备中的潜力与限制，为有效实行和监督AI应用提供了策略与参考。

Conclusion: 人工智能在提升公共卫生准备方面具有显著潜力，但也需谨慎对待相关的实施和监督风险。

Abstract: This review explores the integration of Artificial Intelligence into Horizon Scanning, focusing on identifying and responding to emerging threats and opportunities linked to Infectious Diseases. We examine how AI tools can enhance signal detection, data monitoring, scenario analysis, and decision support. We also address the risks associated with AI adoption and propose strategies for effective implementation and governance. The findings contribute to the growing body of Foresight literature by demonstrating the potential and limitations of AI in Public Health preparedness.

</details>


### [19] [Towards better dense rewards in Reinforcement Learning Applications](https://arxiv.org/abs/2512.04302)
*Shuyuan Zhang*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Finding meaningful and accurate dense rewards is a fundamental task in the field of reinforcement learning (RL) that enables agents to explore environments more efficiently. In traditional RL settings, agents learn optimal policies through interactions with an environment guided by reward signals. However, when these signals are sparse, delayed, or poorly aligned with the intended task objectives, agents often struggle to learn effectively. Dense reward functions, which provide informative feedback at every step or state transition, offer a potential solution by shaping agent behavior and accelerating learning. Despite their benefits, poorly crafted reward functions can lead to unintended behaviors, reward hacking, or inefficient exploration. This problem is particularly acute in complex or high-dimensional environments where handcrafted rewards are difficult to specify and validate. To address this, recent research has explored a variety of approaches, including inverse reinforcement learning, reward modeling from human preferences, and self-supervised learning of intrinsic rewards. While these methods offer promising directions, they often involve trade-offs between generality, scalability, and alignment with human intent. This proposal explores several approaches to dealing with these unsolved problems and enhancing the effectiveness and reliability of dense reward construction in different RL applications.

</details>


### [20] [Efficient Reinforcement Learning with Semantic and Token Entropy for LLM Reasoning](https://arxiv.org/abs/2512.04359)
*Hongye Cao,Zhixin Bai,Ziyue Peng,Boyan Wang,Tianpei Yang,Jing Huo,Yuyao Zhang,Yang Gao*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Reinforcement learning with verifiable rewards (RLVR) has demonstrated superior performance in enhancing the reasoning capability of large language models (LLMs). However, this accuracy-oriented learning paradigm often suffers from entropy collapse, which reduces policy exploration and limits reasoning capabilities. To address this challenge, we propose an efficient reinforcement learning framework that leverages entropy signals at both the semantic and token levels to improve reasoning. From the data perspective, we introduce semantic entropy-guided curriculum learning, organizing training data from low to high semantic entropy to guide progressive optimization from easier to more challenging tasks. For the algorithmic design, we adopt non-uniform token treatment by imposing KL regularization on low-entropy tokens that critically impact policy exploration and applying stronger constraints on high-covariance portions within these tokens. By jointly optimizing data organization and algorithmic design, our method effectively mitigates entropy collapse and enhances LLM reasoning. Experimental results across 6 benchmarks with 3 different parameter-scale base models demonstrate that our method outperforms other entropy-based approaches in improving reasoning.

</details>


### [21] [AgentBay: A Hybrid Interaction Sandbox for Seamless Human-AI Intervention in Agentic Systems](https://arxiv.org/abs/2512.04367)
*Yun Piao,Hongbo Min,Hang Su,Leilei Zhang,Lei Wang,Yue Yin,Xiao Wu,Zhejing Xu,Liwei Qu,Hang Li,Xinxin Zeng,Wei Tian,Fei Yu,Xiaowei Li,Jiayi Jiang,Tongxu Liu,Hao Tian,Yufei Que,Xiaobing Tu,Bing Suo,Yuebing Li,Xiangting Chen,Zeen Zhao,Jiaming Tang,Wei Huang,Xuguang Li,Jing Zhao,Jin Li,Jie Shen,Jinkui Ren,Xiantao Zhang*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The rapid advancement of Large Language Models (LLMs) is catalyzing a shift towards autonomous AI Agents capable of executing complex, multi-step tasks. However, these agents remain brittle when faced with real-world exceptions, making Human-in-the-Loop (HITL) supervision essential for mission-critical applications. In this paper, we present AgentBay, a novel sandbox service designed from the ground up for hybrid interaction. AgentBay provides secure, isolated execution environments spanning Windows, Linux, Android, Web Browsers, and Code interpreters. Its core contribution is a unified session accessible via a hybrid control interface: An AI agent can interact programmatically via mainstream interfaces (MCP, Open Source SDK), while a human operator can, at any moment, seamlessly take over full manual control. This seamless intervention is enabled by Adaptive Streaming Protocol (ASP). Unlike traditional VNC/RDP, ASP is specifically engineered for this hybrid use case, delivering an ultra-low-latency, smoother user experience that remains resilient even in weak network environments. It achieves this by dynamically blending command-based and video-based streaming, adapting its encoding strategy based on network conditions and the current controller (AI or human). Our evaluation demonstrates strong results in security, performance, and task completion rates. In a benchmark of complex tasks, the AgentBay (Agent + Human) model achieved more than 48% success rate improvement. Furthermore, our ASP protocol reduces bandwidth consumption by up to 50% compared to standard RDP, and in end-to-end latency with around 5% reduction, especially under poor network conditions. We posit that AgentBay provides a foundational primitive for building the next generation of reliable, human-supervised autonomous systems.

</details>


### [22] [GovBench: Benchmarking LLM Agents for Real-World Data Governance Workflows](https://arxiv.org/abs/2512.04416)
*Zhou Liu,Zhaoyang Han,Guochen Yan,Hao Liang,Bohan Zeng,Xing Chen,Yuanfeng Song,Wentao Zhang*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Data governance ensures data quality, security, and compliance through policies and standards, a critical foundation for scaling modern AI development. Recently, large language models (LLMs) have emerged as a promising solution for automating data governance by translating user intent into executable transformation code. However, existing benchmarks for automated data science often emphasize snippet-level coding or high-level analytics, failing to capture the unique challenge of data governance: ensuring the correctness and quality of the data itself. To bridge this gap, we introduce GovBench, a benchmark featuring 150 diverse tasks grounded in real-world scenarios, built on data from actual cases. GovBench employs a novel "reversed-objective" methodology to synthesize realistic noise and utilizes rigorous metrics to assess end-to-end pipeline reliability. Our analysis on GovBench reveals that current models struggle with complex, multi-step workflows and lack robust error-correction mechanisms. Consequently, we propose DataGovAgent, a framework utilizing a Planner-Executor-Evaluator architecture that integrates constraint-based planning, retrieval-augmented generation, and sandboxed feedback-driven debugging. Experimental results show that DataGovAgent significantly boosts the Average Task Score (ATS) on complex tasks from 39.7 to 54.9 and reduces debugging iterations by over 77.9 percent compared to general-purpose baselines.

</details>


### [23] [Solving LLM Repetition Problem in Production: A Comprehensive Study of Multiple Solutions](https://arxiv.org/abs/2512.04419)
*Weiwei Wang,Weijie Zou,Jiyong Min*

Main category: cs.AI

TL;DR: 本文探讨了大语言模型在生产环境中的重复问题，提出了针对三种重复模式的多种实用解决方案。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在生产部署中会生成重复内容且无法正确终止，严重影响性能。

Method: 识别三种重复模式，使用Markov模型进行理论分析，评估三种解决方案：Beam Search解码、presence_penalty超参数、DPO微调。

Result: 实验验证了所提出的三种解决方案的有效性，并建立了任务特定的适用性映射。

Conclusion: 本研究结合了实际生产经验和全面实验验证，提供了可用于生产的解决方案。

Abstract: The repetition problem, where Large Language Models (LLMs) continuously generate repetitive content without proper termination, poses a critical challenge in production deployments, causing severe performance degradation and system stalling. This paper presents a comprehensive investigation and multiple practical solutions for the repetition problem encountered in real-world batch code interpretation tasks.
  We identify three distinct repetition patterns: (1) business rule generation repetition, (2) method call relationship analysis repetition, and (3) PlantUML diagram syntax generation repetition. Through rigorous theoretical analysis based on Markov models, we establish that the root cause lies in greedy decoding's inability to escape repetitive loops, exacerbated by self-reinforcement effects.
  Our comprehensive experimental evaluation demonstrates three viable solutions: (1) Beam Search decoding with early_stopping=True serves as a universal post-hoc mechanism that effectively resolves all three repetition patterns; (2) presence_penalty hyperparameter provides an effective solution specifically for BadCase 1; and (3) Direct Preference Optimization (DPO) fine-tuning offers a universal model-level solution for all three BadCases.
  The primary value of this work lies in combining first-hand production experience with extensive experimental validation. Our main contributions include systematic theoretical analysis of repetition mechanisms, comprehensive evaluation of multiple solutions with task-specific applicability mapping, identification of early_stopping as the critical parameter for Beam Search effectiveness, and practical production-ready solutions validated in real deployment environments.

</details>


### [24] [TaskEval: Synthesised Evaluation for Foundation-Model Tasks](https://arxiv.org/abs/2512.04442)
*Dilani Widanapathiranage,Scott Barnett,Stefanus Kurniawan,Wannita Takerngsaksiri*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Hallucinations are a key concern when creating applications that rely on Foundation models (FMs). Understanding where and how these subtle failures occur in an application relies on evaluation methods known as \textit{evals}. Prior work focuses on defining new eval methods or benchmark datasets for specific tasks. However, neither helps a software team with a task-specific FM application when there is no metric or dataset. The demand for both automated approaches and deep integration of human insight makes this a challenging problem. We address this gap by proposing an approach to synthesise a FM task-specific evaluator program that provides automation and a custom UI for capturing feedback. The core novelty of our approach lies in: (1) a task-agnostic meta-model that captures properties of any FM task, (2) an interaction protocol for efficient use of human feedback, and (3) an eval synthesiser that selects or generates an appropriate set of evals. We implement our approach in \toolname and demonstrate the concept on two diverse FM tasks: chart data extraction and document question answering. A preliminary evaluation on the quality of our selected evals shows 93\% and 90\% accuracy respectively. Our research tackles a growing problem facing engineering teams, how to evaluate and review outputs from FM tasks.

</details>


### [25] [MARL Warehouse Robots](https://arxiv.org/abs/2512.04463)
*Price Allman,Lian Thang,Dre Simmons,Salmon Riaz*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We present a comparative study of multi-agent reinforcement learning (MARL) algorithms for cooperative warehouse robotics. We evaluate QMIX and IPPO on the Robotic Warehouse (RWARE) environment and a custom Unity 3D simulation. Our experiments reveal that QMIX's value decomposition significantly outperforms independent learning approaches (achieving 3.25 mean return vs. 0.38 for advanced IPPO), but requires extensive hyperparameter tuning -- particularly extended epsilon annealing (5M+ steps) for sparse reward discovery. We demonstrate successful deployment in Unity ML-Agents, achieving consistent package delivery after 1M training steps. While MARL shows promise for small-scale deployments (2-4 robots), significant scaling challenges remain. Code and analyses: https://pallman14.github.io/MARL-QMIX-Warehouse-Robots/

</details>


### [26] [Mathematical Framing for Different Agent Strategies](https://arxiv.org/abs/2512.04469)
*Philip Stephens,Emmanuel Salawu*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We introduce a unified mathematical and probabilistic framework for understanding and comparing diverse AI agent strategies. We bridge the gap between high-level agent design concepts, such as ReAct, multi-agent systems, and control flows, and a rigorous mathematical formulation. Our approach frames agentic processes as a chain of probabilities, enabling a detailed analysis of how different strategies manipulate these probabilities to achieve desired outcomes. Our framework provides a common language for discussing the trade-offs inherent in various agent architectures. One of our many key contributions is the introduction of the "Degrees of Freedom" concept, which intuitively differentiates the optimizable levers available for each approach, thereby guiding the selection of appropriate strategies for specific tasks. This work aims to enhance the clarity and precision in designing and evaluating AI agents, offering insights into maximizing the probability of successful actions within complex agentic systems.

</details>


### [27] [AI-Assisted Game Management Decisions: A Fuzzy Logic Approach to Real-Time Substituitions](https://arxiv.org/abs/2512.04480)
*Pedro Passos*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: In elite soccer, substitution decisions entail significant financial and sporting consequences yet remain heavily reliant on intuition or predictive models that merely mimic historical biases. This paper introduces a Fuzzy Logic based Decision Support System (DSS) designed for real time, prescriptive game management. Unlike traditional Machine Learning approaches that encounter a predictive ceiling by attempting to replicate human behavior, our system audits performance through an objective, rule based inference engine. We propose a methodological advancement by reformulating the PlayeRank metric into a Cumulative Mean with Role Aware Normalization, eliminating the play time exposure bias inherent in cumulative sum models to enable accurate intra match comparison. The system integrates this refined metric with physiological proxies (fatigue) and contextual variables (disciplinary risk modulated by tactical role) to calculate a dynamic Substitution Priority (P final). Validation via a case study of the 2018 FIFA World Cup match between Brazil and Belgium demonstrates the system's ecological validity: it not only aligned with expert consensus on executed substitutions (for example Gabriel Jesus) but, crucially, identified high risk scenarios ignored by human decision makers. Specifically, the model flagged the "FAGNER Paradox" - a maximum priority defensive risk - minutes before a critical yellow card, and detected the "Lukaku Paradox", where an isolated assist masked a severe drop in participation. These results confirm that Fuzzy Logic offers a transparent, explainable, and superior alternative to black box models for optimizing real time tactical decisions.

</details>


### [28] [Persona-based Multi-Agent Collaboration for Brainstorming](https://arxiv.org/abs/2512.04488)
*Nate Straub,Saara Khan,Kat Jay,Brian Cabral,Oskar Linde*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We demonstrate the importance of persona-based multi-agents brainstorming for both diverse topics and subject matter ideation. Prior work has shown that generalized multi-agent collaboration often provides better reasoning than a single agent alone. In this paper, we propose and develop a framework for persona-based agent selection, showing how persona domain curation can improve brainstorming outcomes. Using multiple experimental setups, we evaluate brainstorming outputs across different persona pairings (e.g., Doctor vs VR Engineer) and A2A (agent-to-agent) dynamics (separate, together, separate-then-together). Our results show that (1) persona choice shapes idea domains, (2) collaboration mode shifts diversity of idea generation, and (3) multi-agent persona-driven brainstorming produces idea depth and cross-domain coverage.

</details>


### [29] [A Modular Cognitive Architecture for Assisted Reasoning: The Nemosine Framework](https://arxiv.org/abs/2512.04500)
*Edervaldo Melo*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: This paper presents the Nemosine Framework, a modular cognitive architecture designed to support assisted reasoning, structured thinking, and systematic analysis. The model operates through functional cognitive modules ("personas") that organize tasks such as planning, evaluation, cross-checking, and narrative synthesis. The framework combines principles from metacognition, distributed cognition, and modular cognitive systems to offer an operational structure for assisted problem-solving and decision support. The architecture is documented through formal specification, internal consistency criteria, and reproducible structural components. The goal is to provide a clear conceptual basis for future computational implementations and to contribute to the study of symbolic-modular architectures for reasoning.

</details>


### [30] [BiTAgent: A Task-Aware Modular Framework for Bidirectional Coupling between Multimodal Large Language Models and World Models](https://arxiv.org/abs/2512.04513)
*Yu-Wei Zhan,Xin Wang,Pengzhe Mao,Tongtong Feng,Ren Wang,Wenwu Zhu*

Main category: cs.AI

TL;DR: 该论文提出了一种名为BiTAgent的框架，通过双向耦合多模态大型语言模型（MLLMs）和世界模型（WMs），以解决语义意图与动态状态表示的耦合问题，以及实现任务感知适应性，从而在多样真实任务中实现通用型具身智能体。


<details>
  <summary>Details</summary>
Motivation: 尽管MLLMs和WMs在各自领域表现优秀，但它们的组合仍面临两个主要挑战: 1) 如何实现MLLMs与WMs之间的紧密耦合；2) 如何实现任务感知适应性以支持多任务学习和跨环境泛化。

Method: 为了解决这些挑战，作者提出BiTAgent框架，包含两个互补路径：1) 正向路径，将MLLMs的表示注入WM的潜在空间，实现语义指导的想象；2) 逆向路径，利用WM生成的反馈，通过高密度文本条件奖励细化MLLMs的语义空间。框架包括三个协同组件：1) 任务感知动态联合学习，2) 任务感知行为学习，3) MLM-WM联合优化。

Result: 在多任务和跨环境的大量实验中，BiTAgent表现出比最先进的基线更优秀的稳定性和泛化能力。

Conclusion: BiTAgent为开创新一代开放式具身学习迈出重要一步，通过双向耦合MLLMs和WMs，在多个真实任务中实现了通用型的具身智能体。

Abstract: Building generalist embodied agents requires a unified system that can interpret multimodal goals, model environment dynamics, and execute reliable actions across diverse real-world tasks. Multimodal large language models (MLLMs) offer strong semantic priors and cross-modal generalization, while world models (WMs) provide actionable latent dynamics for prediction and control. Their combination holds promise for open-ended embodied intelligence, yet introduces two key challenges: (1) establishing a tight coupling between the semantic intent from MLLMs and the dynamic state representations within the WM's latent space, and (2) achieving task-aware adaptability that supports multi-task learning and cross-environment generalization. To address these limitations, we propose BiTAgent, a task-aware dynamic joint framework that enables bidirectional coupling between MLLMs and WMs. BiTAgent establishes two complementary pathways: a forward path that injects MLLM representations into the WM's latent space for semantically guided imagination, and a backward path where WM-generated feedback refines the MLLM's semantic space via dense text-conditioned rewards. This bidirectional interaction is realized through three synergistic components: Task-Aware Dynamic Joint Learning, Task-Aware Behavior Learning, and MLLM-WM Joint Optimization, which together harmonize semantic reasoning and dynamic prediction. Extensive experiments across multi-task and cross-environment settings demonstrate superior stability and generalization over state-of-the-art baselines, marking a step toward open-ended embodied learning.

</details>


### [31] [The Ethics of Generative AI](https://arxiv.org/abs/2512.04598)
*Michael Klenk*

Main category: cs.AI

TL;DR: 本章讨论了生成式AI的伦理问题，包括技术入门、在AI伦理中加剧或减轻的常见伦理问题，以及生成式AI特有的伦理问题。


<details>
  <summary>Details</summary>
Motivation: 生成式AI提供了将技术视作人类体验的能力，这对哲学伦理有重大意义。

Method: 通过技术入门以及分析现有和特有的伦理问题，讨论生成式AI的多个伦理方面。

Result: 生成式AI对责任、隐私、偏见和公平、异化和剥削等伦理问题有影响。其模仿生成性引发了关于作者身份、信用和机器社交关系的新问题。

Conclusion: 本章总结了生成式AI的独特伦理挑战及其对社会和哲学的影响。

Abstract: This chapter discusses the ethics of generative AI. It provides a technical primer to show how generative AI affords experiencing technology as if it were human, and this affordance provides a fruitful focus for the philosophical ethics of generative AI. It then shows how generative AI can both aggravate and alleviate familiar ethical concerns in AI ethics, including responsibility, privacy, bias and fairness, and forms of alienation and exploitation. Finally, the chapter examines ethical questions that arise specifically from generative AI's mimetic generativity, such as debates about authorship and credit, the emergence of as-if social relationships with machines, and new forms of influence, persuasion, and manipulation.

</details>


### [32] [Neural Decoding of Overt Speech from ECoG Using Vision Transformers and Contrastive Representation Learning](https://arxiv.org/abs/2512.04618)
*Mohamed Baha Ben Ticha,Xingchen Ran,Guillaume Saldanha,Gaël Le Godais,Philémon Roussel,Marc Aubert,Amina Fontanell,Thomas Costecalde,Lucas Struber,Serpil Karakas,Shaomin Zhang,Philippe Kahane,Guillaume Charvet,Stéphan Chabardès,Blaise Yvert*

Main category: cs.AI

TL;DR: 该论文致力于通过脑机接口技术，利用外部ECoG记录来实时重建语音，提出了一种结合Vision Transformers和对比学习的编解码器深度神经架构，并验证了其在临床和植入式设备上的有效性。


<details>
  <summary>Details</summary>
Motivation: 解决严重麻痹患者无法沟通的问题，挑战在于如何通过表面ECoG记录实现语音的实时重建。

Method: 提出了一种离线语音解码器流水线，包括一个深度神经架构的编解码器，整合了Vision Transformers和对比学习，以增强ECoG信号对语音的直接回归。

Result: 该方法在两个数据集上进行了评估，包括临床硬膜下电极和可完全植入的WIMAGINE硬膜外系统，表现出了初步的有效性和长期使用的潜力。

Conclusion: 该研究验证了使用ECoG信号进行实时语音解码的可行性，并为完全植入式、无线硬膜外记录系统的长期应用提供了前景。

Abstract: Speech Brain Computer Interfaces (BCIs) offer promising solutions to people with severe paralysis unable to communicate. A number of recent studies have demonstrated convincing reconstruction of intelligible speech from surface electrocorticographic (ECoG) or intracortical recordings by predicting a series of phonemes or words and using downstream language models to obtain meaningful sentences. A current challenge is to reconstruct speech in a streaming mode by directly regressing cortical signals into acoustic speech. While this has been achieved recently using intracortical data, further work is needed to obtain comparable results with surface ECoG recordings. In particular, optimizing neural decoders becomes critical in this case. Here we present an offline speech decoding pipeline based on an encoder-decoder deep neural architecture, integrating Vision Transformers and contrastive learning to enhance the direct regression of speech from ECoG signals. The approach is evaluated on two datasets, one obtained with clinical subdural electrodes in an epileptic patient, and another obtained with the fully implantable WIMAGINE epidural system in a participant of a motor BCI trial. To our knowledge this presents a first attempt to decode speech from a fully implantable and wireless epidural recording system offering perspectives for long-term use.

</details>


### [33] [Turbo-Muon: Accelerating Orthogonality-Based Optimization with Pre-Conditioning](https://arxiv.org/abs/2512.04632)
*Thibaut Boissin,Thomas Massena,Franck Mamalet,Mathieu Serrurier*

Main category: cs.AI

TL;DR: 该研究提出了一种预处理程序，以加速Newton-Schulz迭代的收敛并减少计算成本。在保持模型性能的同时，无需调整超参数即可简单地集成到现有方法中。


<details>
  <summary>Details</summary>
Motivation: 现有基于正交的优化器，如Muon，需要进行昂贵的梯度正交化，影响计算效率。Newton-Schulz迭代即使进行了高效的近似，仍然计算代价较高。研究旨在解决这些方法的效率问题。

Method: 引入了一种预处理程序，加速Newton-Schulz的收敛并减少其计算成本。通过实验验证了预处理的低开销以及其在常规训练中的高效性。

Result: 该方法的实现使Newton-Schulz近似达到了2.8倍的速度提升。在实际训练场景中，端到端训练运行时间减少了5-10%。在语言和视觉任务中，该方法在改进运行时的同时维持或超越了原模型性能。

Conclusion: 该方法为基于正交的优化器提供了一种高效、易于集成的改进，能够在不降低性能的同时，显著提高训练速度。未来工作中，探索预处理程序在不同机器学习任务中的普适性值得关注。

Abstract: Orthogonality-based optimizers, such as Muon, have recently shown strong performance across large-scale training and community-driven efficiency challenges. However, these methods rely on a costly gradient orthogonalization step. Even efficient iterative approximations such as Newton-Schulz remain expensive, typically requiring dozens of matrix multiplications to converge. We introduce a preconditioning procedure that accelerates Newton-Schulz convergence and reduces its computational cost. We evaluate its impact and show that the overhead of our preconditioning can be made negligible. Furthermore, the faster convergence it enables allows us to remove one iteration out of the usual five without degrading approximation quality. Our publicly available implementation achieves up to a 2.8x speedup in the Newton-Schulz approximation. We also show that this has a direct impact on end-to-end training runtime with 5-10% improvement in realistic training scenarios across two efficiency-focused tasks. On challenging language or vision tasks, we validate that our method maintains equal or superior model performance while improving runtime. Crucially, these improvements require no hyperparameter tuning and can be adopted as a simple drop-in replacement. Our code is publicly available on github.

</details>


### [34] [Playing the Player: A Heuristic Framework for Adaptive Poker AI](https://arxiv.org/abs/2512.04714)
*Andrew Paterson,Carl Sanders*

Main category: cs.AI

TL;DR: 反驳扑克AI追求无法利用性，主张最大化利用人类对手弱点的AI设计与验证。


<details>
  <summary>Details</summary>
Motivation: 挑战传统观点，即围棋AI的成功依赖于无法利用的最优解，而人类对手的弱点和不可预测性才是突破关键。

Method: 提出Patrick AI架构，核心为针对人类决策缺陷的预测锚定学习方法，结合博弈树和随机搜索算法。

Result: 在64,267手牌试验中，Patrick盈利表现验证了利用人类弱点的有效性。

Conclusion: AI研究的重点应从‘解决’游戏转向‘利用’人类不完美，这一方向更具实际意义。

Abstract: For years, the discourse around poker AI has been dominated by the concept of solvers and the pursuit of unexploitable, machine-perfect play. This paper challenges that orthodoxy. It presents Patrick, an AI built on the contrary philosophy: that the path to victory lies not in being unexploitable, but in being maximally exploitative. Patrick's architecture is a purpose-built engine for understanding and attacking the flawed, psychological, and often irrational nature of human opponents. Through detailed analysis of its design, its novel prediction-anchored learning method, and its profitable performance in a 64,267-hand trial, this paper makes the case that the solved myth is a distraction from the real, far more interesting challenge: creating AI that can master the art of human imperfection.

</details>


### [35] [Sequential Enumeration in Large Language Models](https://arxiv.org/abs/2512.04727)
*Kuinan Hou,Marco Zorzi,Alberto Testolin*

Main category: cs.AI

TL;DR: 探究五种LLMs能否可靠地计数和生成序列，发现需明确提示才能计数，表明当前LLM在系统计数方面仍不及符号方法。


<details>
  <summary>Details</summary>
Motivation: 当前神经模型计数能力欠缺，LLMs能否实现此功能尚不清楚。

Method: 研究五种LLMs，测试计数和生成序列能力，探索提示策略和模型大小对计数的影響。

Result: 仅明确提示下计数，不会自发计数，不能稳健并系统部署计数程序。

Conclusion: LLMs目前缺乏系统计数能力，神经与符号方法之间存在差距。

Abstract: Reliably counting and generating sequences of items remain a significant challenge for neural networks, including Large Language Models (LLMs). Indeed, although this capability is readily handled by rule-based symbolic systems based on serial computation, learning to systematically deploy counting procedures is difficult for neural models, which should acquire these skills through learning. Previous research has demonstrated that recurrent architectures can only approximately track and enumerate sequences of events, and it remains unclear whether modern deep learning systems, including LLMs, can deploy systematic counting procedures over sequences of discrete symbols. This paper aims to fill this gap by investigating the sequential enumeration abilities of five state-of-the-art LLMs, including proprietary, open-source, and reasoning models. We probe LLMs in sequential naming and production tasks involving lists of letters and words, adopting a variety of prompting instructions to explore the role of chain-of-thought in the spontaneous emerging of counting strategies. We also evaluate open-source models with the same architecture but increasing size to see whether the mastering of counting principles follows scaling laws, and we analyze the embedding dynamics during sequential enumeration to investigate the emergent encoding of numerosity. We find that some LLMs are indeed capable of deploying counting procedures when explicitly prompted to do so, but none of them spontaneously engage in counting when simply asked to enumerate the number of items in a sequence. Our results suggest that, despite their impressive emergent abilities, LLMs cannot yet robustly and systematically deploy counting procedures, highlighting a persistent gap between neural and symbolic approaches to compositional generalization.

</details>


### [36] [Human Cognitive Biases in Explanation-Based Interaction: The Case of Within and Between Session Order Effect](https://arxiv.org/abs/2512.04764)
*Dario Pesenti,Alessandro Bogani,Katya Tentori,Stefano Teso*

Main category: cs.AI

TL;DR: XIL框架虽有前景，但存在触发顺序效应的担忧。作者通过两项大规模研究（n = 713），发现顺序效应仅在特定条件下影响用户信任，对反馈质量影响较小，因此XIL的应用并未受到显著影响。


<details>
  <summary>Details</summary>
Motivation: 为了解决关于XIL框架中顺序效应如何影响用户反馈的争议，尤其是在该设计与实际使用情境差异较大的情况下。

Method: 作者进行了两项大规模用户研究（n = 713），模拟常见XIL任务，通过在调试会话内和会话之间改变正确和错误解释的展示顺序来评估顺序效应。

Result: 顺序效应仅在调试会话内对用户与模型的一致性（信任的行为测量）有显著影响，而在会话之间没有影响。用户反馈的质量总体上是满意的，只有小而不一致的影响来自顺序效应。

Conclusion: 研究结果表明，顺序效应并未对XIL框架的成功应用构成重大问题，为AI中的人类因素提供了重要见解。

Abstract: Explanatory Interactive Learning (XIL) is a powerful interactive learning framework designed to enable users to customize and correct AI models by interacting with their explanations. In a nutshell, XIL algorithms select a number of items on which an AI model made a decision (e.g. images and their tags) and present them to users, together with corresponding explanations (e.g. image regions that drive the model's decision). Then, users supply corrective feedback for the explanations, which the algorithm uses to improve the model. Despite showing promise in debugging tasks, recent studies have raised concerns that explanatory interaction may trigger order effects, a well-known cognitive bias in which the sequence of presented items influences users' trust and, critically, the quality of their feedback. We argue that these studies are not entirely conclusive, as the experimental designs and tasks employed differ substantially from common XIL use cases, complicating interpretation. To clarify the interplay between order effects and explanatory interaction, we ran two larger-scale user studies (n = 713 total) designed to mimic common XIL tasks. Specifically, we assessed order effects both within and between debugging sessions by manipulating the order in which correct and wrong explanations are presented to participants. Order effects had a limited, through significant impact on users' agreement with the model (i.e., a behavioral measure of their trust), and only when examined withing debugging sessions, not between them. The quality of users' feedback was generally satisfactory, with order effects exerting only a small and inconsistent influence in both experiments. Overall, our findings suggest that order effects do not pose a significant issue for the successful employment of XIL approaches. More broadly, our work contributes to the ongoing efforts for understanding human factors in AI.

</details>


### [37] [Model-Based and Sample-Efficient AI-Assisted Math Discovery in Sphere Packing](https://arxiv.org/abs/2512.04829)
*Rasul Tutunov,Alexandre Maraval,Antoine Grosnit,Xihan Li,Jun Wang,Haitham Bou-Ammar*

Main category: cs.AI

TL;DR: 提出用AI组合优化方法改进球堆积问题的高维上界


<details>
  <summary>Details</summary>
Motivation: 传统AI难以应对需要高精度的数学优化问题，球堆积问题是典型挑战

Method: 将半正定规划(SDP)建模为决策游戏，结合贝叶斯优化与蒙特卡洛树搜索

Result: 在4-16维空间实现新最优上界，验证AI辅助数学研究新范式

Conclusion: 模型驱动的搜索策略可在数学严格领域取得突破，补充大语言模型研究范式

Abstract: Sphere packing, Hilbert's eighteenth problem, asks for the densest arrangement of congruent spheres in n-dimensional Euclidean space. Although relevant to areas such as cryptography, crystallography, and medical imaging, the problem remains unresolved: beyond a few special dimensions, neither optimal packings nor tight upper bounds are known. Even a major breakthrough in dimension $n=8$, later recognised with a Fields Medal, underscores its difficulty. A leading technique for upper bounds, the three-point method, reduces the problem to solving large, high-precision semidefinite programs (SDPs). Because each candidate SDP may take days to evaluate, standard data-intensive AI approaches are infeasible. We address this challenge by formulating SDP construction as a sequential decision process, the SDP game, in which a policy assembles SDP formulations from a set of admissible components. Using a sample-efficient model-based framework that combines Bayesian optimisation with Monte Carlo Tree Search, we obtain new state-of-the-art upper bounds in dimensions $4-16$, showing that model-based search can advance computational progress in longstanding geometric problems. Together, these results demonstrate that sample-efficient, model-based search can make tangible progress on mathematically rigid, evaluation limited problems, pointing towards a complementary direction for AI-assisted discovery beyond large-scale LLM-driven exploration.

</details>


### [38] [Are LLMs Truly Multilingual? Exploring Zero-Shot Multilingual Capability of LLMs for Information Retrieval: An Italian Healthcare Use Case](https://arxiv.org/abs/2512.04834)
*Vignesh Kumar Kembu,Pierandrea Morandini,Marta Bianca Maria Ranzini,Antonino Nocera*

Main category: cs.AI

TL;DR: 该论文探讨了开源多语言大模型在意大利语电子健康记录（EHR）中实时提取共病信息的性能，发现其在零样本、本地化部署场景下表现不稳定，且难以泛化到不同疾病。


<details>
  <summary>Details</summary>
Motivation: 传统NLP技术在临床文本信息提取中面临挑战，如复杂性和高语义多样性，而大模型（LLMs）为突破这些瓶颈提供了新可能。

Method: 实验针对开源多语言LLMs在共病信息提取任务上的表现，对比了零样本（zero-shot）环境下的模型性能，并评估其与模式匹配和人工标注的差距。

Result: 部分LLMs在零样本和本地化部署中性能受限，表现波动显著，尤其在跨病种泛化能力上与基线方法存在差距。

Conclusion: 当前开源多语言LLMs在临床文本信息提取中仍需优化以提升泛化能力和稳定性。

Abstract: Large Language Models (LLMs) have become a key topic in AI and NLP, transforming sectors like healthcare, finance, education, and marketing by improving customer service, automating tasks, providing insights, improving diagnostics, and personalizing learning experiences. Information extraction from clinical records is a crucial task in digital healthcare. Although traditional NLP techniques have been used for this in the past, they often fall short due to the complexity, variability of clinical language, and high inner semantics in the free clinical text. Recently, Large Language Models (LLMs) have become a powerful tool for better understanding and generating human-like text, making them highly effective in this area. In this paper, we explore the ability of open-source multilingual LLMs to understand EHRs (Electronic Health Records) in Italian and help extract information from them in real-time. Our detailed experimental campaign on comorbidity extraction from EHR reveals that some LLMs struggle in zero-shot, on-premises settings, and others show significant variation in performance, struggling to generalize across various diseases when compared to native pattern matching and manual annotations.

</details>


### [39] [From Task Executors to Research Partners: Evaluating AI Co-Pilots Through Workflow Integration in Biomedical Research](https://arxiv.org/abs/2512.04854)
*Lukas Weidener,Marko Brkić,Chiara Bacci,Mihailo Jovanović,Emre Ulgac,Alex Dobrin,Johannes Weniger,Martin Vlas,Ritvik Singh,Aakaash Meduri*

Main category: cs.AI

TL;DR: 这篇论文指出了现有生物医学研究中人工智能系统基准测试的局限，并提出了一个面向整体研究合作流程的评估框架。


<details>
  <summary>Details</summary>
Motivation: 现有评估框架可能不足以衡量AI在生物医学研究中的实际合作能力，需更全面的评估标准。

Method: 通过快速回顾2018年1月至2025年10月间的三个主要数据库和两个预印服务器，确定了14个AI在文献理解、实验设计和假设生成中的基准。

Result: 当前基准只评估了数据质量、假设有效性和实验协议设计等独立组件能力，而真实的研究合作需集成的工作流。

Conclusion: 提出了过程导向的评估框架，涵盖对话质量、工作流编排、会话连续性和研究员经验四个关键维度，用于全面评估AI系统作为研究协作助手的表现。

Abstract: Artificial intelligence systems are increasingly deployed in biomedical research. However, current evaluation frameworks may inadequately assess their effectiveness as research collaborators. This rapid review examines benchmarking practices for AI systems in preclinical biomedical research. Three major databases and two preprint servers were searched from January 1, 2018 to October 31, 2025, identifying 14 benchmarks that assess AI capabilities in literature understanding, experimental design, and hypothesis generation. The results revealed that all current benchmarks assess isolated component capabilities, including data analysis quality, hypothesis validity, and experimental protocol design. However, authentic research collaboration requires integrated workflows spanning multiple sessions, with contextual memory, adaptive dialogue, and constraint propagation. This gap implies that systems excelling on component benchmarks may fail as practical research co-pilots. A process-oriented evaluation framework is proposed that addresses four critical dimensions absent from current benchmarks: dialogue quality, workflow orchestration, session continuity, and researcher experience. These dimensions are essential for evaluating AI systems as research co-pilots rather than as isolated task executors.

</details>


### [40] [STELLA: Guiding Large Language Models for Time Series Forecasting with Semantic Abstractions](https://arxiv.org/abs/2512.04871)
*Junjie Fan,Hongye Zhao,Linduo Wei,Jiayu Rao,Guijia Li,Jiaxin Yuan,Wenqi Xu,Yong Qi*

Main category: cs.AI

TL;DR: 本文提出了一种名为STELLA的框架，旨在改进大型语言模型(LLMs)在时间序列预测中的应用。该框架通过动态语义抽象和分层语义锚点，为时间序列分析提供更有效的信息利用。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs在时间序列预测中未能充分发挥其潜力，主要因为提示策略依赖于静态关联，缺乏全局和实例特定的上下文。为此，作者提出STELLA框架以提供更丰富的动态信息。

Method: STELLA通过动态语义抽象机制，将时间序列解耦为趋势、季节性和残余成分，转化为‘分层语义锚点’。该锚点包括两层：用于全局上下文的语料级语义先验(CSP)和用于实例级模式的精细行为提示(FBP)。

Result: 在八个基准数据集上的实验表明，STELLA在长期和短期预测中均优于最先进的方法，尤其是在零样本和少样本设置中展示了更高的泛化能力。通过ablation研究进一步验证了其动态生成语义锚点的有效性。

Conclusion: STELLA框架通过增强LLMs的信息处理和对动态行为的解释能力，显著提升了时间序列预测的性能。

Abstract: Recent adaptations of Large Language Models (LLMs) for time series forecasting often fail to effectively enhance information for raw series, leaving LLM reasoning capabilities underutilized. Existing prompting strategies rely on static correlations rather than generative interpretations of dynamic behavior, lacking critical global and instance-specific context. To address this, we propose STELLA (Semantic-Temporal Alignment with Language Abstractions), a framework that systematically mines and injects structured supplementary and complementary information. STELLA employs a dynamic semantic abstraction mechanism that decouples input series into trend, seasonality, and residual components. It then translates intrinsic behavioral features of these components into Hierarchical Semantic Anchors: a Corpus-level Semantic Prior (CSP) for global context and a Fine-grained Behavioral Prompt (FBP) for instance-level patterns. Using these anchors as prefix-prompts, STELLA guides the LLM to model intrinsic dynamics. Experiments on eight benchmark datasets demonstrate that STELLA outperforms state-of-the-art methods in long- and short-term forecasting, showing superior generalization in zero-shot and few-shot settings. Ablation studies further validate the effectiveness of our dynamically generated semantic anchors.

</details>


### [41] [Algorithmic Thinking Theory](https://arxiv.org/abs/2512.04923)
*MohammadHossein Bateni,Vincent Cohen-Addad,Yuzhou Gu,Silvio Lattanzi,Simon Meierhans,Christopher Mohri*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Large language models (LLMs) have proven to be highly effective for solving complex reasoning tasks. Surprisingly, their capabilities can often be improved by iterating on previously generated solutions. In this context, a reasoning plan for generating and combining a set of solutions can be thought of as an algorithm for reasoning using a probabilistic oracle.
  We introduce a theoretical framework for analyzing such reasoning algorithms. This framework formalizes the principles underlying popular techniques for iterative improvement and answer aggregation, providing a foundation for designing a new generation of more powerful reasoning methods. Unlike approaches for understanding models that rely on architectural specifics, our model is grounded in experimental evidence. As a result, it offers a general perspective that may extend to a wide range of current and future reasoning oracles.

</details>


### [42] [Toward Continuous Neurocognitive Monitoring: Integrating Speech AI with Relational Graph Transformers for Rare Neurological Diseases](https://arxiv.org/abs/2512.04938)
*Raquel Norel,Michele Merler,Pavitra Modi*

Main category: cs.AI

TL;DR: 该论文提出通过智能手机语音分析和RELGT架构对罕见神经疾病患者的认知症状进行持续监测，并验证了其在PKU患者中的相关性和潜力。


<details>
  <summary>Details</summary>
Motivation: 罕见神经疾病患者的认知症状（如“脑雾”）通常无法通过传统测试检测。研究提出了一种通过语音分析和深度学习进行持续神经认知监测的新方法。

Method: 利用智能手机语音分析结合关系图变换器（RELGT）模型进行连续监测。

Result: 在PKU患者中的初步研究表明，语音分析得出的‘话语表达能力’与血液苯丙氨酸水平相关（p = -0.50, p < 0.005），而标准认知测试结果无显著相关性（|r| < 0.35）。RELGT能处理异质性医疗数据，并提前数周预测病情恶化。

Conclusion: 尽管面临多疾病验证、临床整合和公平多语言使用的挑战，该技术在推动连续个性化监测方面具备潜力，可转变全球数百万罕见神经疾病患者的治疗方式。

Abstract: Patients with rare neurological diseases report cognitive symptoms -"brain fog"- invisible to traditional tests. We propose continuous neurocognitive monitoring via smartphone speech analysis integrated with Relational Graph Transformer (RELGT) architectures. Proof-of-concept in phenylketonuria (PKU) shows speech-derived "Proficiency in Verbal Discourse" correlates with blood phenylalanine (p = -0.50, p < 0.005) but not standard cognitive tests (all |r| < 0.35). RELGT could overcome information bottlenecks in heterogeneous medical data (speech, labs, assessments), enabling predictive alerts weeks before decompensation. Key challenges: multi-disease validation, clinical workflow integration, equitable multilingual deployment. Success would transform episodic neurology into continuous personalized monitoring for millions globally.

</details>


### [43] [Detecting Perspective Shifts in Multi-agent Systems](https://arxiv.org/abs/2512.05013)
*Eric Bridgeford,Hayden Helm*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Generative models augmented with external tools and update mechanisms (or \textit{agents}) have demonstrated capabilities beyond intelligent prompting of base models. As agent use proliferates, dynamic multi-agent systems have naturally emerged. Recent work has investigated the theoretical and empirical properties of low-dimensional representations of agents based on query responses at a single time point. This paper introduces the Temporal Data Kernel Perspective Space (TDKPS), which jointly embeds agents across time, and proposes several novel hypothesis tests for detecting behavioral change at the agent- and group-level in black-box multi-agent systems. We characterize the empirical properties of our proposed tests, including their sensitivity to key hyperparameters, in simulations motivated by a multi-agent system of evolving digital personas. Finally, we demonstrate via natural experiment that our proposed tests detect changes that correlate sensitively, specifically, and significantly with a real exogenous event. As far as we are aware, TDKPS is the first principled framework for monitoring behavioral dynamics in black-box multi-agent systems -- a critical capability as generative agent deployment continues to scale.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [44] [Retrieval-Augmented Few-Shot Prompting Versus Fine-Tuning for Code Vulnerability Detection](https://arxiv.org/abs/2512.04106)
*Fouad Trad,Ali Chehab*

Main category: cs.SE

TL;DR: 该论文研究了检索增强提示在代码漏洞检测中的有效性，发现其优于其他提示策略和微调模型，并且相比微调模型能节省训练时间和成本。


<details>
  <summary>Details</summary>
Motivation: 检索增强提示可以提高少样本学习在复杂领域（如代码漏洞检测）中的性能。该研究旨在比较不同提示策略和微调模型的效果，并探讨检索增强提示的实用性。

Method: 研究使用了Gemini-1.5-Flash模型，并对比了三种方法：(1) 标准少样本提示与随机选择示例，(2) 使用语义相似实例的检索增强提示，(3) 基于检索示例的标签分配，无需模型推断。此外，还与零样本提示和经过微调的模型进行了比较。

Result: 检索增强提示在20个实例下，达到了74.05%的F1分数和83.90%的部分匹配精度。它优于零样本提示和经过微调的Gemini模型，但相比经过微调的CodeBERT（91.22%的F1分数）仍有差距，后者需要额外训练和资源。

Conclusion: 检索增强提示是一种有效且高效的代码漏洞检测策略，尤其适合需要快速部署的场景。然而，在某些需要极高性能的场景中，微调模型仍是更好的选择。

Abstract: Few-shot prompting has emerged as a practical alternative to fine-tuning for leveraging the capabilities of large language models (LLMs) in specialized tasks. However, its effectiveness depends heavily on the selection and quality of in-context examples, particularly in complex domains. In this work, we examine retrieval-augmented prompting as a strategy to improve few-shot performance in code vulnerability detection, where the goal is to identify one or more security-relevant weaknesses present in a given code snippet from a predefined set of vulnerability categories. We perform a systematic evaluation using the Gemini-1.5-Flash model across three approaches: (1) standard few-shot prompting with randomly selected examples, (2) retrieval-augmented prompting using semantically similar examples, and (3) retrieval-based labeling, which assigns labels based on retrieved examples without model inference. Our results show that retrieval-augmented prompting consistently outperforms the other prompting strategies. At 20 shots, it achieves an F1 score of 74.05% and a partial match accuracy of 83.90%. We further compare this approach against zero-shot prompting and several fine-tuned models, including Gemini-1.5-Flash and smaller open-source models such as DistilBERT, DistilGPT2, and CodeBERT. Retrieval-augmented prompting outperforms both zero-shot (F1 score: 36.35%, partial match accuracy: 20.30%) and fine-tuned Gemini (F1 score: 59.31%, partial match accuracy: 53.10%), while avoiding the training time and cost associated with model fine-tuning. On the other hand, fine-tuning CodeBERT yields higher performance (F1 score: 91.22%, partial match accuracy: 91.30%) but requires additional training, maintenance effort, and resources.

</details>


### [45] [Reusing Model Validation Methods for the Continuous Validation of Digital Twins of Cyber-Physical Systems](https://arxiv.org/abs/2512.04117)
*Joost Mertens,Joachim Denil*

Main category: cs.SE

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: One of the challenges in twinned systems is ensuring the digital twin remains a valid representation of the system it twins. Depending on the type of twinning occurring, it is either trivial, such as in dashboarding/visualizations that mirror the system with real-time data, or challenging, in case the digital twin is a simulation model that reflects the behavior of a physical twinned system. The challenge in this latter case comes from the fact that in contrast to software systems, physical systems are not immutable once deployed, but instead they evolve through processes like maintenance, wear and tear or user error. It is therefore important to detect when changes occur in the physical system to evolve the twin alongside it. We employ and reuse validation techniques from model-based design for this goal. Model validation is one of the steps used to gain trust in the representativeness of a simulation model. In this work, we provide two contributions: (i) we provide a generic approach that, through the use of validation metrics, is able to detect anomalies in twinned systems, and (ii) we demonstrate these techniques with the help of an academic yet industrially relevant case study of a gantry crane such as found in ports. Treating anomalies also means correcting the error in the digital twin, which we do with a parameter estimation based on the historical data.

</details>


### [46] [DrP: Meta's Efficient Investigations Platform at Scale](https://arxiv.org/abs/2512.04250)
*Shubham Somani,Vanish Talwar,Madhura Parikh,Eduardo Hernandez,Jimmy Wang,Shreya Shah,Chinmay Gandhi,Sanjay Sundarajan,Neeru Sharma,Srikanth Kamath,Nitin Gupta,Benjamin Renard,Ohad Yahalom,Chris Davis*

Main category: cs.SE

TL;DR: DrP是一个端到端的自动化调查框架和系统，旨在减少事故的平均解决时间（MTTR）和待命工程师的重复工作。它由灵活的SDK、可扩展的后端系统、与主流工作流集成的插件以及用于采取补救措施的后处理系统组成。


<details>
  <summary>Details</summary>
Motivation: 现有人工或adhoc脚本的调查过程会导致调查效率低下，增加故障解决时间，并且耗费工程师大量时间和精力。因此，需要一个自动化框架来改善现有的调查过程。

Method: DrP框架包括一个表达性强且灵活的SDK，一个可扩展的后端系统，用于将这些自动化的'操作书'（analyzers）集成到警报和事故管理工具中的插件，以及一个处理补救措施的后处理系统。

Result: DrP在Meta大规模应用中，包括300多个团队，2000+ analyzers。DrP每天执行50K的自动分析，成功将平均MTTR减少了20%，某些团队减少了80%以上，极大地提升了工程师的待命工作效率。

Conclusion: DrP在大规模系统中有效减低了MTTR和待命工程师的重复工作，提升了工作效率。

Abstract: Investigations are a significant step in the operational workflows for large scale systems across multiple domains such as services, data, AI/ML, mobile. Investigation processes followed by on-call engineers are often manual or rely on ad-hoc scripts. This leads to inefficient investigations resulting in increased time to mitigate and isolate failures/SLO violations. It also contributes to on-call toil and poor productivity leading to multiple hours/days spent in triaging/debugging incidents. In this paper, we present DrP, an end-to-end framework and system to automate investigations that reduces the mean time to resolve incidents (MTTR) and reduces on-call toil. DrP consists of an expressive and flexible SDK to author investigation playbooks in code (called analyzers), a scalable backend system to execute these automated playbooks, plug-ins to integrate playbooks into mainstream workflows such as alerts and incident management tools, and a post-processing system to take actions on investigations including mitigation steps.
  We have implemented and deployed DrP at large scale at Meta covering 300+ teams, 2000+ analyzers, across a large set of use cases across domains such as services, core infrastructure, AI/ML, hardware, mobile. DrP has been running in production for the past 5 years and executes 50K automated analyses per day. Overall, our results and experience show that DrP has been able to reduce average MTTR by 20 percent at large scale (with over 80 percent for some teams) and has significantly improved on-call productivity.

</details>


### [47] [On the Role and Impact of GenAI Tools in Software Engineering Education](https://arxiv.org/abs/2512.04256)
*Qiaolin Qin,Ronnie de Souza Santos,Rodrigo Spinola*

Main category: cs.SE

TL;DR: 该研究调查了本科软件工程学生使用生成式AI工具的情况，指出了它们的优势、挑战、伦理问题和教学期望。


<details>
  <summary>Details</summary>
Motivation: 生成式AI工具在教育和软件工程中的普及带来新的支持和挑战，研究者希望了解学生使用这些工具的具体体验和看法。

Method: 从两所大学对130名本科生进行调查，使用了结构化Likert量表和开放式问题，调查分析五个方面：使用情境、感知的好处、挑战、伦理和教学观念。

Result: 学生主要使用生成式AI进行渐进式学习和高级实现，常报告的好处包括支持头脑风暴和增强信心。挑战包括AI输出解释不清和适应困难。对公平性和不当行为等伦理问题表示担忧，需要更明确的教学指导。

Conclusion: 该研究强调了对生成式AI进行更好的教学支持、制定伦理政策和适应性教学策略的重要性，以确保其支持公平和有效的学习。

Abstract: Context. The rise of generative AI (GenAI) tools like ChatGPT and GitHub Copilot has transformed how software is learned and written. In software engineering (SE) education, these tools offer new opportunities for support, but also raise concerns about over-reliance, ethical use, and impacts on learning. Objective. This study investigates how undergraduate SE students use GenAI tools, focusing on the benefits, challenges, ethical concerns, and instructional expectations that shape their experiences. Method. We conducted a survey with 130 undergraduate students from two universities. The survey combined structured Likert-scale items and open-ended questions to investigate five dimensions: usage context, perceived benefits, challenges, ethical and instructional perceptions. Results. Students most often use GenAI for incremental learning and advanced implementation, reporting benefits such as brainstorming support and confidence-building. At the same time, they face challenges including unclear rationales and difficulty adapting outputs. Students highlight ethical concerns around fairness and misconduct, and call for clearer instructional guidance. Conclusion. GenAI is reshaping SE education in nuanced ways. Our findings underscore the need for scaffolding, ethical policies, and adaptive instructional strategies to ensure that GenAI supports equitable and effective learning.

</details>


### [48] [Catching UX Flaws in Code: Leveraging LLMs to Identify Usability Flaws at the Development Stage](https://arxiv.org/abs/2512.04262)
*Nolan Platt,Ethan Luchs,Sehrish Nizamani*

Main category: cs.SE

TL;DR: 探讨大型语言模型（LLM）在开发阶段提供可靠且一致启发式评估的能力。研究采用GPT-4o对30个开源网站做出了超过850项的启发式评估。结果显示，在问题检测上，模型表现出中度的内部一致性，但在严重性评估方面差异较大，显示出需要人工监督的必要性。可行性及局限性被重点分析，强调了改善自动化用户体验（UX）评估的方法。


<details>
  <summary>Details</summary>
Motivation: 现代接口的可用性评估至关重要，但传统的专家启发式评估耗时且主观，该文旨在确定LLM是否能提供更高效且一致的启发式评估。

Method: 使用OpenAI的GPT-4o对30个开源网站进行了三次独立评估，应用了Jakob Nielsen的十项可用性启发法。评估模型的一致性通过Cohen's Kappa和Krippendorff's Alpha进行量化。

Result: 模型在问题检测上具有中等一致性（平均Cohen's Kappa为0.50，完全一致率84%），但在严重性判断上变异较大（加权Cohen's Kappa为0.63，完全一致率56%，Krippendorff's Alpha接近零）。

Conclusion: LLM在可用性问题的早期检测中具备可行性，但严重性评估需要人工监督以保证准确性。该研究为提升自动化UX评估一致性提供了基础。

Abstract: Usability evaluations are essential for ensuring that modern interfaces meet user needs, yet traditional heuristic evaluations by human experts can be time-consuming and subjective, especially early in development. This paper investigates whether large language models (LLMs) can provide reliable and consistent heuristic assessments at the development stage. By applying Jakob Nielsen's ten usability heuristics to thirty open-source websites, we generated over 850 heuristic evaluations in three independent evaluations per site using a pipeline of OpenAI's GPT-4o. For issue detection, the model demonstrated moderate consistency, with an average pairwise Cohen's Kappa of 0.50 and an exact agreement of 84%. Severity judgments showed more variability: weighted Cohen's Kappa averaged 0.63, but exact agreement was just 56%, and Krippendorff's Alpha was near zero. These results suggest that while GPT-4o can produce internally consistent evaluations, especially for identifying the presence of usability issues, its ability to judge severity varies and requires human oversight in practice. Our findings highlight the feasibility and limitations of using LLMs for early-stage, automated usability testing, and offer a foundation for improving consistency in automated User Experience (UX) evaluation. To the best of our knowledge, our work provides one of the first quantitative inter-rater reliability analyses of automated heuristic evaluation and highlights methods for improving model consistency.

</details>


### [49] [Polynomiogram: An Integrated Framework for Root Visualization and Generative Art](https://arxiv.org/abs/2512.04263)
*Hoang Duc Nguyen,Anh Van Pham,Hien D. Nguyen*

Main category: cs.SE

TL;DR: 介绍Polynomiogram框架，这是一个综合计算平台，用于探索、可视化和生成来自多项式根系统的艺术。


<details>
  <summary>Details</summary>
Motivation: 多项式根系统在科学和艺术领域具有重要意义。

Method: 两个独立参数采样方案，并通过生成函数映射到多项式系数。平台集成了两种互补的数值引擎：用于快速大规模计算的NumPy伴随矩阵求解器和用于高精度科学验证的MPSolve。

Result: 该方法被应用于三次多项式系统，分析其分岔结构。被验证可用于科学探索和艺术生成，例如生成木槿花形状和个性化生成艺术品。

Conclusion: Polynomiogram展示了其在科学分析和个性化生成艺术中的潜力。

Abstract: This work presents the Polynomiogram framework, an integrated computational platform for exploring, visualizing, and generating art from polynomial root systems. The main innovation is a flexible sampling scheme in which two independent parameters are drawn from user defined domains and mapped to the polynomial coefficients through a generating function. This design allows the same mathematical foundation to support both scientific investigation and generative algorithmic art. The framework integrates two complementary numerical engines: NumPy companion matrix solver for fast, large scale computation and MPSolve for high precision, scientifically rigorous validation. This dual architecture enables efficient visualization for creative use and accurate computation for research and education. Numerical accuracy was verified using classical ensembles, including the Kac and Lucas polynomials. The method was applied to the cubic polynomial system to analyze its bifurcation structure, demonstrating its value as both a scientific tool for exploring root phenomena and an educational aid for visualizing fundamental concepts in algebra and dynamical systems. Beyond analysis, the Polynomiogram also demonstrated its potential as a tool for personalized generative art. Examples include the use of the platform to generate a natural form resembling a hibiscus flower and to create personalized artwork expressing gratitude toward advances in artificial intelligence and large language models through a tribute composition.

</details>


### [50] [Quantitative Analysis of Technical Debt and Pattern Violation in Large Language Model Architectures](https://arxiv.org/abs/2512.04273)
*Tyler Slater*

Main category: cs.SE

TL;DR: 随着大模型变成自主系统架构师，其对于长期软件可维护性的影响仍未有量化。本研究呈现了第一个测量AI生成微服务的架构腐蚀和技术债务积累的实证框架。


<details>
  <summary>Details</summary>
Motivation: 随着大模型变成自主系统架构师，其对于长期软件可维护性的影响仍未有量化。

Method: 比较试验，三个先进模型（GPT-5.1, Claude 4.5 Sonnet, and Llama 3 8B）进行对照试验。运用抽象语法树（AST）分析。

Result: 专有模型架构符合度高（GPT-5.1违规率为0%），而开源模型（Llama 3）违规率高达80%。开源模型表现出“实现懒惰”，生成代码逻辑行数少，导致技术债务积累较快。

Conclusion: 运用小的开源模型进行系统框架构建会加速结构性技术债务的积累，除非有自动架构检查工具。

Abstract: As Large Language Models (LLMs) transition from code completion tools to autonomous system architects, their impact on long-term software maintainability remains unquantified. While existing research benchmarks functional correctness (pass@k), this study presents the first empirical framework to measure "Architectural Erosion" and the accumulation of Technical Debt in AI-synthesized microservices. We conducted a comparative pilot study of three state-of-the-art models (GPT-5.1, Claude 4.5 Sonnet, and Llama 3 8B) by prompting them to implement a standardized Book Lending Microservice under strict Hexagonal Architecture constraints. Utilizing Abstract Syntax Tree (AST) parsing, we find that while proprietary models achieve high architectural conformance (0% violation rate for GPT-5.1), open-weights models exhibit critical divergence. Specifically, Llama 3 demonstrated an 80% Architectural Violation Rate, frequently bypassing interface adapters to create illegal circular dependencies between Domain and Infrastructure layers. Furthermore, we identified a phenomenon of "Implementation Laziness," where open-weights models generated 60% fewer Logical Lines of Code (LLOC) than their proprietary counterparts, effectively omitting complex business logic to satisfy token constraints. These findings suggest that without automated architectural linting, utilizing smaller open-weights models for system scaffolding accelerates the accumulation of structural technical debt.

</details>


### [51] [MANTRA: a Framework for Multi-stage Adaptive Noise TReAtment During Training](https://arxiv.org/abs/2512.04319)
*Zixiao Zhao,Fatemeh H. Fard,Jie JW Wu*

Main category: cs.SE

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The reliable application of deep learning models to software engineering tasks hinges on high-quality training data. Yet, large-scale repositories inevitably introduce noisy or mislabeled examples that degrade both accuracy and robustness. While Noise Label Learning (NLL) has been extensively studied in other fields, there are a few works that investigate NLL in Software Engineering (SE) and Large Language Models (LLMs) for SE tasks. In this work, we propose MANTRA, a Multi-stage Adaptive Noise TReAtment framework that embeds noise diagnosis and mitigation directly into the fine-tuning process of code-Pretrained Language Models (PTM) and code-LLMs. We first investigate the effect of noise at varying levels on convergence and loss trajectories of the models. Then we apply an adaptive dropout strategy guided by per-sample loss dynamics and Gaussian Mixture Model clustering to exclude persistently noisy points while preserving clean data. Applying to code summarization and commit intent classification, our experiments reveal that some LLMs are more sensitive to noise than others. However, with MANTRA, the performance of all models in both tasks is improved. MANTRA enables researchers and practitioners to reduce the impact of errors introduced by the dataset in training, saves time in data cleaning and processing, while maximizing the effect of fine-tuning.

</details>


### [52] [Targeted Testing of Compiler Optimizations via Grammar-Level Composition Styles](https://arxiv.org/abs/2512.04344)
*Zitong Zhou,Ben Limpanukorn,Hong Jin Kang,Jiyuan Wang,Yaoxuan Wu,Akos Kiss,Renata Hodovan,Miryung Kim*

Main category: cs.SE

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Ensuring the correctness of compiler optimizations is critical, but existing fuzzers struggle to test optimizations effectively. First, most fuzzers use optimization pipelines (heuristics-based, fixed sequences of passes) as their harness. The phase-ordering problem can enable or preempt transformations, so pipelines inevitably miss optimization interactions; moreover, many optimizations are not scheduled, even at aggressive levels. Second, optimizations typically fire only when inputs satisfy specific structural relationships, which existing generators and mutations struggle to produce. We propose targeted fuzzing of individual optimizations to complement pipeline-based testing. Our key idea is to exploit composition styles - structural relations over program constructs (adjacency, nesting, repetition, ordering) - that optimizations look for. We build a general-purpose, grammar-based mutational fuzzer, TargetFuzz, that (i) mines composition styles from an optimization-relevant corpus, then (ii) rebuilds them inside different contexts offered by a larger, generic corpus via synthesized mutations to test variations of optimization logic. TargetFuzz is adaptable to a new programming language by lightweight, grammar-based, construct annotations - and it automatically synthesizes mutators and crossovers to rebuild composition styles. No need for hand-coded generators or language-specific mutators, which is particularly useful for modular frameworks such as MLIR, whose dialect-based, rapidly evolving ecosystem makes optimizations difficult to fuzz. Our evaluation on LLVM and MLIR shows that TargetFuzz improves coverage by 8% and 11% and triggers optimizations 2.8$\times$ and 2.6$\times$, compared to baseline fuzzers under the targeted fuzzing mode. We show that targeted fuzzing is complementary: it effectively tests all 37 sampled LLVM optimizations, while pipeline-fuzzing missed 12.

</details>


### [53] [Automating Complex Document Workflows via Stepwise and Rollback-Enabled Operation Orchestration](https://arxiv.org/abs/2512.04445)
*Yanbin Zhang,Hanhui Ye,Yue Bai,Qiming Zhang,Liao Xiang,Wu Mianzhi,Renjun Hu*

Main category: cs.SE

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Workflow automation promises substantial productivity gains in everyday document-related tasks. While prior agentic systems can execute isolated instructions, they struggle with automating multi-step, session-level workflows due to limited control over the operational process. To this end, we introduce AutoDW, a novel execution framework that enables stepwise, rollback-enabled operation orchestration. AutoDW incrementally plans API actions conditioned on user instructions, intent-filtered API candidates, and the evolving states of the document. It further employs robust rollback mechanisms at both the argument and API levels, enabling dynamic correction and fault tolerance. These designs together ensure that the execution trajectory of AutoDW remains aligned with user intent and document context across long-horizon workflows. To assess its effectiveness, we construct a comprehensive benchmark of 250 sessions and 1,708 human-annotated instructions, reflecting realistic document processing scenarios with interdependent instructions. AutoDW achieves 90% and 62% completion rates on instruction- and session-level tasks, respectively, outperforming strong baselines by 40% and 76%. Moreover, AutoDW also remains robust for the decision of backbone LLMs and on tasks with varying difficulty. Code and data will be open-sourced. Code: https://github.com/YJett/AutoDW

</details>


### [54] [LLM-SrcLog: Towards Proactive and Unified Log Template Extraction via Large Language Models](https://arxiv.org/abs/2512.04474)
*Jiaqi Sun,Wei Li,Heng Zhang,Chutong Ding,Shiyou Qian,Jian Cao,Guangtao Xue*

Main category: cs.SE

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Log parsing transforms raw logs into structured templates containing constants and variables. It underpins anomaly detection, failure diagnosis, and other AIOps tasks. Current parsers are mostly reactive and log-centric. They only infer templates from logs, mostly overlooking the source code. This restricts their capacity to grasp dynamic log structures or adjust to evolving systems. Moreover, per-log LLM inference is too costly for practical deployment. In this paper, we propose LLM-SrcLog, a proactive and unified framework for log template parsing. It extracts templates directly from source code prior to deployment and supplements them with data-driven parsing for logs without available code. LLM-SrcLog integrates a cross-function static code analyzer to reconstruct meaningful logging contexts, an LLM-based white-box template extractor with post-processing to distinguish constants from variables, and a black-box template extractor that incorporates data-driven clustering for remaining unmatched logs. Experiments on two public benchmarks (Hadoop and Zookeeper) and a large-scale industrial system (Sunfire-Compute) show that, compared to two LLM-based baselines, LLM-SrcLog improves average F1-score by 2-17% and 8-35%. Meanwhile, its online parsing latency is comparable to data-driven methods and about 1,000 times faster than per-log LLM parsing. LLM-SrcLog achieves a near-ideal balance between speed and accuracy. Finally, we further validate the effectiveness of LLM-SrcLog through practical case studies in a real-world production environment.

</details>


### [55] [Completion by Comprehension: Guiding Code Generation with Multi-Granularity Understanding](https://arxiv.org/abs/2512.04538)
*Xinkui Zhao,Rongkai Liu,Yifan Zhang,Chen Zhi,Lufei Zhang,Guanjie Cheng,Yueshen Xu,Shuiguang Deng,Jianwei Yin*

Main category: cs.SE

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: As code completion task from function-level to repository-level, leveraging contextual information from large-scale codebases becomes a core challenge. However, existing retrieval-augmented generation (RAG) methods typically treat code as plain natural language, relying primarily on shallow semantic matching while overlooking structural semantics and code-specific dependencies. This limits their ability to capture control flow and underlying intent, ultimately constraining the quality of generated code. Therefore, we propose CoCo, a novel framework that enables code Completion by Comprehension of multi-granularity context from large-scale code repositories. CoCo employs static code analysis to extract structured context at the function, file, and project levels, capturing execution logic and semantic dependencies. It then adopts an graph-based multi-granularity context selection mechanism to filter out redundant information and remove noise. Consequently, the information is converted into natural language in a consistent manner, thereby functioning as explicit contextual prompts to guide subsequent code completion. Additionally, a structure-aware code re-ranker mechanism ensures alignment at both semantic and structural levels. Extensive experiments on CrossCodeEval and RepoEval benchmarks demonstrate that CoCo consistently surpasses state-of-the-art baselines, achieving up to 20.2% gains in EM. Moreover, the framework is model-agnostic and can be seamlessly integrated into existing methods, leading to significant performance.

</details>


### [56] [Cross-Task Benchmarking and Evaluation of General-Purpose and Code-Specific Large Language Models](https://arxiv.org/abs/2512.04673)
*Gunjan Das,Paheli Bhattacharya,Rishabh Gupta*

Main category: cs.SE

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Large Language Models (LLMs) have revolutionized both general natural language processing and domain-specific applications such as code synthesis, legal reasoning, and finance. However, while prior studies have explored individual model capabilities, a systematic cross-domain comparison that unifies linguistic, reasoning, and code understanding abilities remains underexplored. In this work, we present a comprehensive evaluation of five general-purpose and three code-specific state-of-the-art LLMs across six diverse benchmarks encompassing linguistic competence, mathematical reasoning, and trustworthiness. Additionally, we analyze model behavior on the CoNaLa dataset for code explanation, comparing natural language and code-specialized LLMs. Our findings reveal that models optimized for code (e.g., CodeLLaMA variants) exhibit strong reasoning and syntactic precision, that even for non-coding tasks can show measurable performance gains, in contrast to general-purpose models like Mistral-7B and Llama-3-8B.

</details>


### [57] [Generative AI for Self-Adaptive Systems: State of the Art and Research Roadmap](https://arxiv.org/abs/2512.04680)
*Jialong Li,Mingyue Zhang,Nianyu Li,Danny Weyns,Zhi Jin,Kenji Tei*

Main category: cs.SE

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Self-adaptive systems (SASs) are designed to handle changes and uncertainties through a feedback loop with four core functionalities: monitoring, analyzing, planning, and execution. Recently, generative artificial intelligence (GenAI), especially the area of large language models, has shown impressive performance in data comprehension and logical reasoning. These capabilities are highly aligned with the functionalities required in SASs, suggesting a strong potential to employ GenAI to enhance SASs. However, the specific benefits and challenges of employing GenAI in SASs remain unclear. Yet, providing a comprehensive understanding of these benefits and challenges is complex due to several reasons: limited publications in the SAS field, the technological and application diversity within SASs, and the rapid evolution of GenAI technologies. To that end, this paper aims to provide researchers and practitioners a comprehensive snapshot that outlines the potential benefits and challenges of employing GenAI's within SAS. Specifically, we gather, filter, and analyze literature from four distinct research fields and organize them into two main categories to potential benefits: (i) enhancements to the autonomy of SASs centered around the specific functions of the MAPE-K feedback loop, and (ii) improvements in the interaction between humans and SASs within human-on-the-loop settings. From our study, we outline a research roadmap that highlights the challenges of integrating GenAI into SASs. The roadmap starts with outlining key research challenges that need to be tackled to exploit the potential for applying GenAI in the field of SAS. The roadmap concludes with a practical reflection, elaborating on current shortcomings of GenAI and proposing possible mitigation strategies.

</details>


### [58] [POLARIS: Is Multi-Agentic Reasoning the Next Wave in Engineering Self-Adaptive Systems?](https://arxiv.org/abs/2512.04702)
*Divyansh Pandey,Vyakhya Gupta,Prakhar Singhal,Karthik Vaidhyanathan*

Main category: cs.SE

TL;DR: POLARIS是一个三层多智能体自适应框架，旨在解决不确定性并实现超越传统反应式适应的主动调整。


<details>
  <summary>Details</summary>
Motivation: 现代软件生态系统日益增长的规模、复杂性、互联性和自主性带来了前所未有的不确定性，挑战了传统自我适应的基础。现有基于规则控制器或孤立学习组件的方法难以推广到新的情境或跨分布式子系统进行协调响应。

Method: POLARIS框架包括：(1)低延迟的Adapter层，用于监控和安全执行；(2)透明的Reasoning层，使用工具感知和可解释智能体生成并验证计划；(3)Meta层，记录经验并进行元学习以优化适应策略。

Result: 通过在两个自我适应范例SWIM和SWITCH上的初步评估，POLARIS始终优于当前最先进的基线方法。

Conclusion: POLARIS不仅展示了从Self-Adaptation 2.0向Self-Adaptation 3.0的转变，即系统不仅能从环境中学习，还能推理并进化自身的适应过程，持续改进以应对新挑战。

Abstract: The growing scale, complexity, interconnectivity, and autonomy of modern software ecosystems introduce unprecedented uncertainty, challenging the foundations of traditional self-adaptation. Existing approaches, typically rule-driven controllers or isolated learning components, struggle to generalize to novel contexts or coordinate responses across distributed subsystems, leaving them ill-equipped for emergent unknown unknowns. Recent discussions on Self-Adaptation 2.0 emphasize an equal partnership between AI and adaptive systems, merging learning-driven intelligence with adaptive control for predictive and proactive behavior. Building on this foundation, we introduce POLARIS, a three-layer multi-agentic self-adaptation framework that advances beyond reactive adaptation. POLARIS integrates: (1) a low-latency Adapter layer for monitoring and safe execution, (2) a transparent Reasoning layer that generates and verifies plans using tool-aware, explainable agents, and (3) a Meta layer that records experiences and meta-learns improved adaptation policies over time. Through shared knowledge and predictive models, POLARIS handles uncertainty, learns from past actions, and evolves its strategies, enabling systems that anticipate change and maintain resilient, goal-directed behavior. Preliminary evaluation on two self-adaptive exemplars, SWIM and SWITCH, shows that POLARIS consistently outperforms state-of-the-art baselines. We argue this marks a shift toward Self-Adaptation 3.0, akin to Software 3.0: a paradigm where systems not only learn from their environment but also reason about and evolve their own adaptation processes, continuously improving to meet novel challenges.

</details>


### [59] [Configuration Defects in Kubernetes](https://arxiv.org/abs/2512.05062)
*Yue Zhang,Uchswas Paul,Marcelo d'Amorim,Akond Rahman*

Main category: cs.SE

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Kubernetes is a tool that facilitates rapid deployment of software. Unfortunately, configuring Kubernetes is prone to errors. Configuration defects are not uncommon and can result in serious consequences. This paper reports an empirical study about configuration defects in Kubernetes with the goal of helping practitioners detect and prevent these defects. We study 719 defects that we extract from 2,260 Kubernetes configuration scripts using open source repositories. Using qualitative analysis, we identify 15 categories of defects. We find 8 publicly available static analysis tools to be capable of detecting 8 of the 15 defect categories. We find that the highest precision and recall of those tools are for defects related to data fields. We develop a linter to detect two categories of defects that cause serious consequences, which none of the studied tools are able to detect. Our linter revealed 26 previously-unknown defects that have been confirmed by practitioners, 19 of which have already been fixed. We conclude our paper by providing recommendations on how defect detection and repair techniques can be used for Kubernetes configuration scripts. The datasets and source code used for the paper are publicly available online.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [60] [Towards Contextual Sensitive Data Detection](https://arxiv.org/abs/2512.04120)
*Liang Telkamp,Madelon Hulsebos*

Main category: cs.CR

TL;DR: 本文提出了两种基于数据上下文的敏感数据检测新机制，旨在扩展和优化传统仅关注隐私的敏感数据定义。实验证明其能大幅降低误报并提升准确性。


<details>
  <summary>Details</summary>
Motivation: 开放数据门户的普及要求加强对敏感数据的保护，尤其是在发布和交换数据集之前。现存的敏感数据定义和方法过于聚焦于隐私和风险，需要扩展定义，考虑数据的具体上下文。

Method: 1) 类型上下文化 (Type contextualization)：检测数据值的语义类型，并考虑其在数据集或文档中的整体上下文。2) 领域上下文化 (Domain contextualization)：根据数据敏感性规范文档中的相关规则，确定给定数据集的敏感性。

Result: 1) 类型上下文化显著减少误报，召回率达到94%，而商业工具为63%。2) 领域上下文化可有效检测非标准领域（如人道主义数据）的敏感性。数据专家的评估表明基于上下文的LLM解释在手动数据审核中提供一致性的指导。

Conclusion: 本文引入的两个上下文化机制在敏感数据检测方面表现出色。特别是在非标准领域和开放数据审核中具有广泛的应用前景。项目代码和数据公开在 https://github.com/trl-lab/sensitive-data-detection。

Abstract: The emergence of open data portals necessitates more attention to protecting sensitive data before datasets get published and exchanged. While an abundance of methods for suppressing sensitive data exist, the conceptualization of sensitive data and methods to detect it, focus particularly on personal data that, if disclosed, may be harmful or violate privacy. We observe the need for refining and broadening our definitions of sensitive data, and argue that the sensitivity of data depends on its context. Based on this definition, we introduce two mechanisms for contextual sensitive data detection that con- sider the broader context of a dataset at hand. First, we introduce type contextualization, which first detects the semantic type of particular data values, then considers the overall context of the data values within the dataset or document. Second, we introduce domain contextualization which determines sensitivity of a given dataset in the broader context based on the retrieval of relevant rules from documents that specify data sensitivity (e.g., data topic and geographic origin). Experiments with these mechanisms, assisted by large language models (LLMs), confirm that: 1) type-contextualization significantly reduces the number of false positives for type-based sensitive data detection and reaches a recall of 94% compared to 63% with commercial tools, and 2) domain-contextualization leveraging sensitivity rule retrieval is effective for context-grounded sensitive data detection in non-standard data domains such as humanitarian datasets. Evaluation with humanitarian data experts also reveals that context-grounded LLM explanations provide useful guidance in manual data auditing processes, improving consistency. We open-source mechanisms and annotated datasets for contextual sensitive data detection at https://github.com/trl-lab/sensitive-data-detection.

</details>


### [61] [Primitive Vector Cipher(PVC): A Hybrid Encryption Scheme based on the Vector Computational Diffie-Hellman (V-CDH) Problem](https://arxiv.org/abs/2512.04237)
*Gülçin ÇİVİ BİLİR*

Main category: cs.CR

TL;DR: PVC是一种新型混合加密方案，结合了基于矩阵的密码学和先进的Diffie-Hellman密钥交换，其安全性基于V-CDH问题的硬度。两层设计采用HKDF通过DH认证共享原始向量对明文进行掩码，并以每块偏移量对密码块进行随机化。这种方法消除了确定性重复，并提供了对线性和已知明文攻击的高抵抗力。PVC的块结构支持大规模并行和优秀的线性扩展性。形式化安全分析显示，在V-CDH假设下具备INDCPA安全性，结合STS协议可提升至IND-CCA安全保证。


<details>
  <summary>Details</summary>
Motivation: 设计一种具备高安全性和高性能的混合加密系统。

Method: 将基于矩阵的密码学和Diffie-Hellman密钥交换结合，利用HKDF掩码和随机化技术。

Result: 实现INDCPA安全性，通过STS协议集成后达到IND-CCA安全保证，支持大规模并行和线性扩展。

Conclusion: PVC在V-CDH问题基础上，展示了强安全性与出色的扩展性，适用于抵御复杂密码攻击的场景。

Abstract: This work introduces the Primitive Vector Cipher (PVC), a novel hybrid encryption scheme integrating matrix-based cryptography with advanced Diffie-Hellman key exchange. PVC's security is grounded on the established hardness of the Vector Computational Diffie- Hellman (V-CDH) problem. The two-layered design uses HKDF to mask plaintext via a DH-authenticated shared primitive vector and randomize cipher blocks with a per-block offset. This approach eliminates deterministic repetitions and provides strong resistance against linear and known-plaintext attacks. PVC's block-wise structure allows for massive parallelism and excellent linear scaling. Security is formally analyzed, demonstrating INDCPA security under V-CDH. STS protocol integration elevates security toward IND-CCA guarantees.

</details>


### [62] [Hey GPT-OSS, Looks Like You Got It - Now Walk Me Through It! An Assessment of the Reasoning Language Models Chain of Thought Mechanism for Digital Forensics](https://arxiv.org/abs/2512.04254)
*Gaëtan Michelet,Janine Schneider,Aruna Withanage,Frank Breitinger*

Main category: cs.CR

TL;DR: 本文首次研究了推理语言模型在数字取证中的应用，评估了其在结果可解释性方面的潜力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然被广泛用于数字取证，但存在着结果可解释性差的问题，影响其实用性和法律可用性。

Method: 文章测试了四个用例，结合了新的量化指标和定性分析来评估推理组件在结果可解释性方面的支持能力。

Result: 在中等推理水平下，推理组件能较好地帮助解释和验证模型输出，但这种支持通常是有限的，而高级推理水平并未提高回答质量。

Conclusion: 推理组件在提升大型语言模型于数字取证中的应用潜力中具有积极作用，但整体效果还存在一定的局限性。

Abstract: The use of large language models in digital forensics has been widely explored. Beyond identifying potential applications, research has also focused on optimizing model performance for forensic tasks through fine-tuning. However, limited result explainability reduces their operational and legal usability. Recently, a new class of reasoning language models has emerged, designed to handle logic-based tasks through an `internal reasoning' mechanism. Yet, users typically see only the final answer, not the underlying reasoning. One of these reasoning models is gpt-oss, which can be deployed locally, providing full access to its underlying reasoning process. This article presents the first investigation into the potential of reasoning language models for digital forensics. Four test use cases are examined to assess the usability of the reasoning component in supporting result explainability. The evaluation combines a new quantitative metric with qualitative analysis. Findings show that the reasoning component aids in explaining and validating language model outputs in digital forensics at medium reasoning levels, but this support is often limited, and higher reasoning levels do not enhance response quality.

</details>


### [63] [WildCode: An Empirical Analysis of Code Generated by ChatGPT](https://arxiv.org/abs/2512.04259)
*Kobra Khanmohammadi,Pooria Roy,Raphael Khoury,Abdelwahab Hamou-Lhadj,Wilfried Patrick Konan*

Main category: cs.CR

TL;DR: 该论文讨论了在现实世界中ChatGPT生成的代码的安全性和正确性。


<details>
  <summary>Details</summary>
Motivation: 随着LLM模型越来越多地用于代码生成，其生成的代码质量和安全性存在不确定性。

Method: 对ChatGPT生成的真实代码进行了大规模实证分析，评估其正确性和安全性，并探讨用户请求代码的意图。

Result: 研究发现，LLM生成的代码在安全性方面常常不足，而且用户对生成代码的安全性缺乏关注。

Conclusion: 该研究证实了早期使用合成查询的研究，并强调了AI生成代码在安全性方面的缺陷。

Abstract: LLM models are increasingly used to generate code, but the quality and security of this code are often uncertain. Several recent studies have raised alarm bells, indicating that such AI-generated code may be particularly vulnerable to cyberattacks. However, most of these studies rely on code that is generated specifically for the study, which raises questions about the realism of such experiments. In this study, we perform a large-scale empirical analysis of real-life code generated by ChatGPT. We evaluate code generated by ChatGPT both with respect to correctness and security and delve into the intentions of users who request code from the model. Our research confirms previous studies that used synthetic queries and yielded evidence that LLM-generated code is often inadequate with respect to security. We also find that users exhibit little curiosity about the security features of the code they ask LLMs to generate, as evidenced by their lack of queries on this topic.

</details>


### [64] [Breaking Isolation: A New Perspective on Hypervisor Exploitation via Cross-Domain Attacks](https://arxiv.org/abs/2512.04260)
*Gaoning Pan,Yiming Tao,Qinying Wang,Chunming Wu,Mingde Hu,Yizhi Ren,Shouling Ji*

Main category: cs.CR

TL;DR: 该论文提出了一种针对虚拟化环境的新型漏洞利用技术，通过利用现代虚拟化环境中的弱内存隔离特性，实现了跨域攻击（CDA）。


<details>
  <summary>Details</summary>
Motivation: 针对现有虚拟化环境中的内存安全漏洞利用技术存在的不足，特别是在高度受限结构和地址随机化（ASLR）的环境下。

Method: 提出并系统化地描述了跨域攻击（CDA），通过识别和利用guest内存中的可用数据区域，自动匹配、合成触发输入并组装完整的攻击链。

Result: 在QEMU和VirtualBox的15个真实漏洞上验证了CDA的广泛适用性和高效性。

Conclusion: CDA是一种有效的方法，能够在虚拟化环境中克服传统漏洞利用的困难。

Abstract: Hypervisors are under threat by critical memory safety vulnerabilities, with pointer corruption being one of the most prevalent and severe forms. Existing exploitation frameworks depend on identifying highly-constrained structures in the host machine and accurately determining their runtime addresses, which is ineffective in hypervisor environments where such structures are rare and further obfuscated by Address Space Layout Randomization (ASLR). We instead observe that modern virtualization environments exhibit weak memory isolation -- guest memory is fully attacker-controlled yet accessible from the host, providing a reliable primitive for exploitation. Based on this observation, we present the first systematic characterization and taxonomy of Cross-Domain Attacks (CDA), a class of exploitation techniques that enable capability escalation through guest memory reuse. To automate this process, we develop a system that identifies cross-domain gadgets, matches them with corrupted pointers, synthesizes triggering inputs, and assembles complete exploit chains. Our evaluation on 15 real-world vulnerabilities across QEMU and VirtualBox shows that CDA is widely applicable and effective.

</details>


### [65] [One Detector Fits All: Robust and Adaptive Detection of Malicious Packages from PyPI to Enterprises](https://arxiv.org/abs/2512.04338)
*Biagio Montaruli,Luca Compagna,Serena Elisa Ponta,Davide Balzarotti*

Main category: cs.CR

TL;DR: 本文介绍了一种能抵御对抗性源代码转换并适应不同假阳性率需求的供应链攻击鲁棒检测器。


<details>
  <summary>Details</summary>
Motivation: 当前方法未能有效应对对抗性源代码变换及不同组织/角色对假阳性率（FPR）不同要求的挑战，特别是在恶意Python包供应链攻击检测中。

Method: 作者提出了一种细粒度代码混淆生成对抗软件包的新方法，并结合对抗训练以提升检测器鲁棒性。此外还通过两个案例研究展示了该检测器在生产环境中的适应能力。

Result: 对抗性训练使检测器鲁棒性提高了2.5倍，在80天内针对122,398个PyPI包的测试中，能发现10%更多经过混淆的恶意包，但稍微降低了非混淆包的性能。两个使用案例研究证明了该检测器的灵活性和有效性（对91,949个包的分析，37天内每天检出2.48个恶意包并有2.18个假阳性；对企业1,596个包的分析，每天仅产生1.24个假阳性）。共发现了346个恶意包并向社区报告。

Conclusion: 该检测器可无缝集成到公共代码仓库和企业生态系统中，在不同的假阳性率要求下表现优秀，为Python软件供应链安全提供了有力工具。

Abstract: The rise of supply chain attacks via malicious Python packages demands robust detection solutions. Current approaches, however, overlook two critical challenges: robustness against adversarial source code transformations and adaptability to the varying false positive rate (FPR) requirements of different actors, from repository maintainers (requiring low FPR) to enterprise security teams (higher FPR tolerance).
  We introduce a robust detector capable of seamless integration into both public repositories like PyPI and enterprise ecosystems. To ensure robustness, we propose a novel methodology for generating adversarial packages using fine-grained code obfuscation. Combining these with adversarial training (AT) enhances detector robustness by 2.5x. We comprehensively evaluate AT effectiveness by testing our detector against 122,398 packages collected daily from PyPI over 80 days, showing that AT needs careful application: it makes the detector more robust to obfuscations and allows finding 10% more obfuscated packages, but slightly decreases performance on non-obfuscated packages.
  We demonstrate production adaptability of our detector via two case studies: (i) one for PyPI maintainers (tuned at 0.1% FPR) and (ii) one for enterprise teams (tuned at 10% FPR). In the former, we analyze 91,949 packages collected from PyPI over 37 days, achieving a daily detection rate of 2.48 malicious packages with only 2.18 false positives. In the latter, we analyze 1,596 packages adopted by a multinational software company, obtaining only 1.24 false positives daily. These results show that our detector can be seamlessly integrated into both public repositories like PyPI and enterprise ecosystems, ensuring a very low time budget of a few minutes to review the false positives.
  Overall, we uncovered 346 malicious packages, now reported to the community.

</details>


### [66] [AutoGuard: A Self-Healing Proactive Security Layer for DevSecOps Pipelines Using Reinforcement Learning](https://arxiv.org/abs/2512.04368)
*Praveen Anugula,Avdhesh Kumar Bhardwaj,Navin Chhibber,Rohit Tewari,Sunil Khemka,Piyush Ranjan*

Main category: cs.CR

TL;DR: AutoGuard是一个基于RL的自动修复安全框架，旨在通过持续观察和动态学习策略，保护DevSecOps环境免受安全威胁。


<details>
  <summary>Details</summary>
Motivation: 现有方法如基于规则的入侵检测和静态漏洞扫描在面对系统变化时显得不足，导致响应时间变长，并且难以应对新兴攻击手段。

Method: 引入AutoGuard，一个基于强化学习（RL）的自动修复安全框架，持续观察管道活动并动态学习以即时应对潜在威胁。

Result: 在模拟的CI/CD环境中测试显示，AutoGuard成功将威胁检测准确率提高了22%，将事件恢复的平均时间（MTTR）缩短了38%，相较于传统方法整体事件抵御能力增强。

Conclusion: AutoGuard能够实时提升防护、检测和响应能力显著改进了DevSecOps环境中的安全有效性。

Abstract: Contemporary DevSecOps pipelines have to deal with the evolution of security in an ever-continuously integrated and deployed environment. Existing methods,such as rule-based intrusion detection and static vulnerability scanning, are inadequate and unreceptive to changes in the system, causing longer response times and organization needs exposure to emerging attack vectors. In light of the previous constraints, we introduce AutoGuard to the DevSecOps ecosystem, a reinforcement learning (RL)-powered self-healing security framework built to pre-emptively protect DevSecOps environments. AutoGuard is a self-securing security environment that continuously observes pipeline activities for potential anomalies while preemptively remediating the environment. The model observes and reacts based on a policy that is continually learned dynamically over time. The RL agent improves each action over time through reward-based learning aimed at improving the agent's ability to prevent, detect and respond to a security incident in real-time. Testing using simulated ContinuousIntegration / Continuous Deployment (CI/CD) environments showed AutoGuard to successfully improve threat detection accuracy by 22%, reduce mean time torecovery (MTTR) for incidents by 38% and increase overall resilience to incidents as compared to traditional methods.
  Keywords- DevSecOps, Reinforcement Learning, Self- Healing Security, Continuous Integration, Automated Threat Mitigation

</details>


### [67] [ReFuzz: Reusing Tests for Processor Fuzzing with Contextual Bandits](https://arxiv.org/abs/2512.04436)
*Chen Chen,Zaiyan Xu,Mohamadreza Rostami,David Liu,Dileep Kalathil,Ahmad-Reza Sadeghi,Jeyavijayan,Rajendran*

Main category: cs.CR

TL;DR: 本文提出了一种名为ReFuzz的适应性模糊测试框架，能复用先前处理器的有效测试用例检测新处理器漏洞，并在实验中发现多个新型漏洞和错误。


<details>
  <summary>Details</summary>
Motivation: 当前的处理器模糊测试框架不能复用先前处理器的测试用例来检测新处理器的漏洞，导致效率不高。

Method: 提出一种名为ReFuzz的框架，利用上下文强盗算法复用先前处理器的有效测试用例，通过智能变异，发现新处理器的漏洞。

Result: 发现了三个新安全漏洞和两个新功能性错误，同时在检测效率和代码覆盖速率等方面均有显著提升。

Conclusion: ReFuzz通过复用和变异有效的测试用例，显著提升了处理器漏洞检测的效率和覆盖率，同时发现了多个新型漏洞和错误。

Abstract: Processor designs rely on iterative modifications and reuse well-established designs. However, this reuse of prior designs also leads to similar vulnerabilities across multiple processors. As processors grow increasingly complex with iterative modifications, efficiently detecting vulnerabilities from modern processors is critical. Inspired by software fuzzing, hardware fuzzing has recently demonstrated its effectiveness in detecting processor vulnerabilities. Yet, to our best knowledge, existing processor fuzzers fuzz each design individually, lacking the capability to understand known vulnerabilities in prior processors to fine-tune fuzzing to identify similar or new variants of vulnerabilities.
  To address this gap, we present ReFuzz, an adaptive fuzzing framework that leverages contextual bandit to reuse highly effective tests from prior processors to fuzz a processor-under-test (PUT) within a given ISA. By intelligently mutating tests that trigger vulnerabilities in prior processors, ReFuzz effectively detects similar and new variants of vulnerabilities in PUTs. ReFuzz uncovered three new security vulnerabilities and two new functional bugs. ReFuzz detected one vulnerability by reusing a test that triggers a known vulnerability in a prior processor. One functional bug exists across three processors that share design modules. The second bug has two variants. Additionally, ReFuzz reuses highly effective tests to enhance efficiency in coverage, achieving an average 511.23x coverage speedup and up to 9.33% more total coverage, compared to existing fuzzers.

</details>


### [68] [A Light-Weight Large Language Model File Format for Highly-Secure Model Distribution](https://arxiv.org/abs/2512.04580)
*Huifeng Zhu,Shijie Li,Qinfeng Li,Yier Jin*

Main category: cs.CR

TL;DR: CryptoTensors是一种安全且与格式兼容的文件结构，专为保密的大型语言模型分布设计。


<details>
  <summary>Details</summary>
Motivation: 为了提升LLMs在特定领域应用（如医疗、法律和金融）的性能，使用敏感数据进行私有定制或微调。但如何保护模型权重并在部署和分发过程中保持机密性至关重要。现有模型格式和部署框架对机密性、访问控制及与可信硬件的安全集成支持甚少。

Method: 引入了CryptoTensors文件格式，扩展了广泛使用的Safetensors格式。采用张量级加密和嵌入式访问控制策略，支持惰性加载和部分反序列化。提供透明解密和自动密钥管理，支持灵活授权和安全模型执行，并在性能方面进行了基准测试和验证。

Result: CryptoTensors在保持兼容性的前提下，提供了轻量、高效且开发人员友好的方式，用于在实际场景中保护LLM权重。

Conclusion: CryptoTensors被证明是一种简单、高效且易于推广的解决方案，能广泛部署以保护大型语言模型权重。

Abstract: To enhance the performance of large language models (LLMs) in various domain-specific applications, sensitive data such as healthcare, law, and finance are being used to privately customize or fine-tune these models. Such privately adapted LLMs are regarded as either personal privacy assets or corporate intellectual property. Therefore, protecting model weights and maintaining strict confidentiality during deployment and distribution have become critically important. However, existing model formats and deployment frameworks provide little to no built-in support for confidentiality, access control, or secure integration with trusted hardware. Current methods for securing model deployment either rely on computationally expensive cryptographic techniques or tightly controlled private infrastructure. Although these approaches can be effective in specific scenarios, they are difficult and costly for widespread deployment.
  In this paper, we introduce CryptoTensors, a secure and format-compatible file structure for confidential LLM distribution. Built as an extension to the widely adopted Safetensors format, CryptoTensors incorporates tensor-level encryption and embedded access control policies, while preserving critical features such as lazy loading and partial deserialization. It enables transparent decryption and automated key management, supporting flexible licensing and secure model execution with minimal overhead. We implement a proof-of-concept library, benchmark its performance across serialization and runtime scenarios, and validate its compatibility with existing inference frameworks, including Hugging Face Transformers and vLLM. Our results highlight CryptoTensors as a light-weight, efficient, and developer-friendly solution for safeguarding LLM weights in real-world and widespread deployments.

</details>


### [69] [PBFuzz: Agentic Directed Fuzzing for PoV Generation](https://arxiv.org/abs/2512.04611)
*Haochen Zeng,Andrew Bao,Jiajun Cheng,Chengyu Song*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Proof-of-Vulnerability (PoV) input generation is a critical task in software security and supports downstream applications such as path generation and validation. Generating a PoV input requires solving two sets of constraints: (1) reachability constraints for reaching vulnerable code locations, and (2) triggering constraints for activating the target vulnerability. Existing approaches, including directed greybox fuzzing and LLM-assisted fuzzing, struggle to efficiently satisfy these constraints. This work presents an agentic method that mimics human experts. Human analysts iteratively study code to extract semantic reachability and triggering constraints, form hypotheses about PoV triggering strategies, encode them as test inputs, and refine their understanding using debugging feedback. We automate this process with an agentic directed fuzzing framework called PBFuzz. PBFuzz tackles four challenges in agentic PoV generation: autonomous code reasoning for semantic constraint extraction, custom program-analysis tools for targeted inference, persistent memory to avoid hypothesis drift, and property-based testing for efficient constraint solving while preserving input structure. Experiments on the Magma benchmark show strong results. PBFuzz triggered 57 vulnerabilities, surpassing all baselines, and uniquely triggered 17 vulnerabilities not exposed by existing fuzzers. PBFuzz achieved this within a 30-minute budget per target, while conventional approaches use 24 hours. Median time-to-exposure was 339 seconds for PBFuzz versus 8680 seconds for AFL++ with CmpLog, giving a 25.6x efficiency improvement with an API cost of 1.83 USD per vulnerability.

</details>


### [70] [Cryptanalysis of Gleeok-128](https://arxiv.org/abs/2512.04675)
*Siwei Chen,Peipei Xie,Shengyuan Xu,Xiutao Feng,Zejun Xiang,Xiangyong Zeng*

Main category: cs.CR

TL;DR: Gleeok是一种低延迟的伪随机函数（PRF）家族，包括三个并行SPN结构，通过异或（XOR）形成最终输出。Gleeok-128和Gleeok-256分别具有128位和256位块大小，都使用256位密钥。Gleeok的多分支结构给安全分析和密钥恢复攻击带来重大挑战。


<details>
  <summary>Details</summary>
Motivation: 论文旨在对Gleeok-128进行深入第三方密码分析，并针对多分支结构提出了两个阶段MILP方法分析分支和全密码差分线性（DL），并结合专用积分框架进行密钥恢复攻击。

Method: 提出了一种两阶段MILP框架来构造分支和全密码DL差分器。此外，设计了专用于多分支结构的积分密钥恢复框架。还包括改进的代数次数界定，进一步推导出积分差分器。

Result: 分析结果显示：
1. 7, 7, 8, 和4轮DL差分器（Branch 1, Branch 2, Branch 3, 和Gleeok-128）
2. 9, 9, 7, 和7轮积分差分器
3. 7和8轮密钥恢复攻击（非全码与全码设置）
4. 检测到Branch 3的12轮以上线性安全缺陷
5. 优化线性层参数显著提高线性安全性

Conclusion: 该研究提高了对Gleeok-128的理解，并为多分支对称设计提供了通用分析方法。

Abstract: Gleeok is a family of low latency keyed pseudorandom functions (PRFs) consisting of three parallel SPN based permutations whose outputs are XORed to form the final value. Both Gleeok-128 and Gleeok-256 use a 256 bit key, with block sizes of 128 and 256 bits, respectively. Owing to its multi branch structure, evaluating security margins and mounting effective key recovery attacks present nontrivial challenges. This paper provides the first comprehensive third party cryptanalysis of Gleeok-128. We introduce a two stage MILP based framework for constructing branch wise and full cipher differential linear (DL) distinguishers, together with an integral based key recovery framework tailored to multi branch designs. Our DL analysis yields 7, 7, 8, and 4 round distinguishers for Branch 1, Branch 2, Branch 3, and Gleeok-128, respectively, with squared correlations approximately 2 to the power minus 88.12, 2 to the power minus 88.12, 2 to the power minus 38.73, and 2 to the power minus 49.04, outperforming those in the design document except for the full PRF case. By tightening algebraic degree bounds, we further derive 9, 9, and 7 round integral distinguishers for the three branches and a 7 round distinguisher for the full PRF, extending the designers results by 3, 3, and 2 rounds and by 2 rounds, respectively. These integral properties enable 7 round and 8 round key recovery attacks in the non full codebook and full codebook settings. In addition, we identify a flaw in the original linear security evaluation of Branch 3, showing that it can be distinguished over all 12 rounds with data complexity about 2 to the power 48. We also propose optimized linear layer parameters that significantly improve linear resistance without sacrificing diffusion. Our results advance the understanding of Gleeok-128 and provide general methods for analyzing multi branch symmetric designs.

</details>


### [71] [SoK: a Comprehensive Causality Analysis Framework for Large Language Model Security](https://arxiv.org/abs/2512.04841)
*Wei Zhao,Zhe Li,Jun Sun*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Large Language Models (LLMs) exhibit remarkable capabilities but remain vulnerable to adversarial manipulations such as jailbreaking, where crafted prompts bypass safety mechanisms. Understanding the causal factors behind such vulnerabilities is essential for building reliable defenses.
  In this work, we introduce a unified causality analysis framework that systematically supports all levels of causal investigation in LLMs, ranging from token-level, neuron-level, and layer-level interventions to representation-level analysis. The framework enables consistent experimentation and comparison across diverse causality-based attack and defense methods. Accompanying this implementation, we provide the first comprehensive survey of causality-driven jailbreak studies and empirically evaluate the framework on multiple open-weight models and safety-critical benchmarks including jailbreaks, hallucination detection, backdoor identification, and fairness evaluation. Our results reveal that: (1) targeted interventions on causally critical components can reliably modify safety behavior; (2) safety-related mechanisms are highly localized (i.e., concentrated in early-to-middle layers with only 1--2\% of neurons exhibiting causal influence); and (3) causal features extracted from our framework achieve over 95\% detection accuracy across multiple threat types.
  By bridging theoretical causality analysis and practical model safety, our framework establishes a reproducible foundation for research on causality-based attacks, interpretability, and robust attack detection and mitigation in LLMs. Code is available at https://github.com/Amadeuszhao/SOK_Casuality.

</details>


### [72] [A Novel Trust-Based DDoS Cyberattack Detection Model for Smart Business Environments](https://arxiv.org/abs/2512.04855)
*Oghenetejiri Okporokpo,Funminiyi Olajide,Nemitari Ajienka,Xiaoqi Ma*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: As the frequency and complexity of Distributed Denial-of-Service (DDoS) attacks continue to increase, the level of threats posed to Smart Internet of Things (SIoT) business environments have also increased. These environments generally have several interconnected SIoT systems and devices that are integral to daily operations, usually depending on cloud infrastructure and real-time data analytics, which require continuous availability and secure data exchange. Conventional detection mechanisms, while useful in static or traditional network environments, often are inadequate in responding to the needs of these dynamic and diverse SIoT networks. In this paper, we introduce a novel trust-based DDoS detection model tailored to meet the unique requirements of smart business environments. The proposed model incorporates a trust evaluation engine that continuously monitors node behaviour, calculating trust scores based on packet delivery ratio, response time, and anomaly detection. These trust metrics are then aggregated by a central trust-based repository that uses inherent trust values to identify traffic patterns indicative of DDoS attacks. By integrating both trust scores and central trust-based outputs, the trust calculation is enhanced, ensuring that threats are accurately identified and addressed in real-time. The model demonstrated a significant improvement in detection accuracy, and a low false-positive rate with enhanced scalability and adaptability under TCP SYN, Ping Flood, and UDP Flood attacks. The results show that a trust-based approach provides an effective, lightweight alternative for securing resource-constrained business IoT environments.

</details>


### [73] [Logic-Driven Cybersecurity: A Novel Framework for System Log Anomaly Detection using Answer Set Programming](https://arxiv.org/abs/2512.04908)
*Fang Li,Fei Zuo,Gopal Gupta*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: This study explores the application of Answer Set Programming (ASP) for detecting anomalies in system logs, addressing the challenges posed by evolving cyber threats. We propose a novel framework that leverages ASP's declarative nature and logical reasoning capabilities to encode complex security rules as logical predicates. Our ASP-based system was applied to a real-world Linux system log dataset, demonstrating its effectiveness in identifying various anomalies such as potential brute-force attacks, privilege escalations, frequent network connections from specific IPs, and various system-level issues. Key findings highlight ASP's strengths in handling structured log data, rule flexibility, and event correlation. The approach shows promise in providing explainable alerts from real-world data. This research contributes to computer forensics by demonstrating a logic-based paradigm for log analysis on a practical dataset, opening avenues for more nuanced and adaptive cyber intelligence systems.

</details>


### [74] [Opacity problems in multi-energy timed automata](https://arxiv.org/abs/2512.04950)
*Étienne André,Lydia Bakiri*

Main category: cs.CR

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Cyber-physical systems can be subject to information leakage; in the presence of continuous variables such as time and energy, these leaks can be subtle to detect. We study here the verification of opacity problems over systems with observation over both timing and energy information. We introduce guarded multi-energy timed automata as an extension of timed automata with multiple energy variables and guards over such variables. Despite undecidability of this general formalism, we establish positive results over a number of subclasses, notably when the attacker observes the final energy and/or the execution time, but also when they have access to the value of the energy variables every time unit.

</details>


### [75] [Personalizing Agent Privacy Decisions via Logical Entailment](https://arxiv.org/abs/2512.05065)
*James Flemings,Ren Yi,Octavian Suciu,Kassem Fawaz,Murali Annavaram,Marco Gruteser*

Main category: cs.CR

TL;DR: 提出ARIEL框架，结合语言模型和基于规则的逻辑，实现个性化数据共享决策。


<details>
  <summary>Details</summary>
Motivation: 个性化数据共享决策需求强烈，但通用隐私规范无法有效满足，模型推理可靠性较低。

Method: ARIEL框架：将个性化数据共享视为蕴含问题，结合模型及规则逻辑进行结构化推理。

Result: ARIEL相比ICL减少39.1%的F1得分误差，显著提升用户数据共享请求判断准确性。

Conclusion: 结合大型语言模型与严格逻辑蕴含的策略能高效实现个性化的隐私判决。

Abstract: Personal language model-based agents are becoming more widespread for completing tasks on behalf of users; however, this raises serious privacy questions regarding whether these models will appropriately disclose user data. While prior work has evaluated language models on data-sharing scenarios based on general privacy norms, we focus on personalizing language models' privacy decisions, grounding their judgments directly in prior user privacy decisions. Our findings suggest that general privacy norms are insufficient for effective personalization of privacy decisions. Furthermore, we find that eliciting privacy judgments from the model through In-context Learning (ICL) is unreliable to due misalignment with the user's prior privacy judgments and opaque reasoning traces, which make it difficult for the user to interpret the reasoning behind the model's decisions. To address these limitations, we propose ARIEL (Agentic Reasoning with Individualized Entailment Logic), a framework that jointly leverages a language model and rule-based logic for structured data-sharing reasoning. ARIEL is based on formulating personalization of data sharing as an entailment, whether a prior user judgment on a data-sharing request implies the same judgment for an incoming request. Our experimental evaluations on advanced models and publicly-available datasets demonstrate that ARIEL can reduce the F1 score error by $\textbf{39.1%}$ over language model-based reasoning (ICL), demonstrating that ARIEL is effective at correctly judging requests where the user would approve data sharing. Overall, our findings suggest that combining LLMs with strict logical entailment is a highly effective strategy for enabling personalized privacy judgments for agents.

</details>
